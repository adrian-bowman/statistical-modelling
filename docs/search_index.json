[["index.html", "Statistical Modelling A conceptual, visual and practical introduction Preface: what’s the problem?", " Statistical Modelling A conceptual, visual and practical introduction Adrian Bowman The University of Glasgow adrian.bowman@glasgow.ac.uk 20 January, 2026 Preface: what’s the problem? The world abounds with data - but scientific and investigative work does not begin with data. It is true that data which is already available may give ideas and suggest hypotheses, but a serious project will start by thinking carefully about well defined objectives. In other words, scientific work begins with a clearly stated problem. Statistical modelling refers to the process by which we collect and use data to gain insight into the problem we have defined and to lead us towards a conclusion. In an interesting paper on scientific and statistical methods, MacKay and Oldford (2000) structured the process in the following broad steps, referred to by the acronym PPDAC. Some of the key questions which are likely to arise in each step have been highlighted. Problem What are the key questions we would like to address? What is the context in which these questions are framed? Plan How should our experiment be designed? What data should be collected and how much? Data How should the data be checked for validity and consistency? What methods of visual exploration should we use? Analysis How should an appropriate model be constructed? What does analysis of our model tell us about our problem? Conclusion How should we report to others on our conclusions? What limitations and caveats should we highlight? This broad view of statistical modelling sets the agenda for the book. There are many textbooks and resources available which discuss statistical modelling - why another one? This book aims takes a particular approach. The focus is on conceptual understanding of the main ideas behind statistical thinking and modelling. Some technical details are provided for those who are interested but engagement with this material is optional. There is a strong theme of visual communication of both data and the concepts behind statistical models. The approach is also practical with extensive reference to the widely used statistical computing environment R as a means of engaging with the concepts and implementing the methods discussed. In particular, there is extensive use of real datasets. The aim is to discuss interesting and scientifically important questions, where possible with the data used in published papers. As data become increasingly ‘open’, datsets are read from publicly available sources wherever possible. The target audience is those who need statistical methods to understand data. A good example is PhD students who are well motivated to analyse their experimental data. The scientific contexts of the examples come from a wide range of application areas, including the life sciences, the social sciences and topics of general interest. The aim is to provide examples which are both interesting and accessible, without the need for detailed technical knowledge of a particular area. Throughout the book there are frequent references to the widely used statistical computing system R. It is possible to read this book without using R but it is primarily intended that the reader will use this powerful system to engage with the examples and exercises and with the whole process of statistical modelling. A description of how to install R, and the popular system RStudio which helpfully manages some aspects of the environment, is available immediately after this preface. A gentle introduction to R is provided in Chapter 2. Acknowledgements Please note that this is a work in progress. Please forgive some rough edges in the presentation here and there. References MacKay, R Jock, and R Wayne Oldford. 2000. “Scientific Method, Statistical Method and the Speed of Light.” Statistical Science, 254–78. https://www.jstor.org/stable/2676665. "],["getting-started-with-r.html", "Getting started with R", " Getting started with R The R system can be downloaded for all major computing platforms at: https://cran.r-project.org RStudio has become very popular as a ‘front-end’ for R which helpfully manages some aspects of the environment. It is recommended that you use this. It is freely available at: https://posit.co/download/rstudio-desktop/ A note to Mac users If you are using R on an Apple computer which is running the macOS operating system, you need the XQuartz system to be installed. If this is not present, it can be obtained from https://www.xquartz.org. A note to Windows users Ther are many packages available to extend the range of tools providing in R. Installation of packages from the CRAN archive (https://cran.r-project.org) is very straightforward and quite a few packages will be used throughout this book. Occasionally the need arises to install a package from ‘source’ rather than from the convenient form provided by CRAN. For computers running the Microsoft Windows operating system, some additional tools, referred to as Rtools, may be needed to allow that. Details on how to install these tools are at https://cran.r-project.org/bin/windows/Rtools/. Installing the rpanel package One of the features of R is the availability of a large number of add-on packages which considerably extend the tools available. One of these, rpanel, is particularly associated with this book. It provides some of the datasets used as examples and it has some tools intended to help in communicating the statistical concepts to be covered. It would therefore be helpful to install this package at the very beginning. It would also be helpful to install the development version so that the contents are up-to-date. This can be done by typing the following instructions into the console window. R may respond with some messages as it tracks progress in obeying these instructions. install.packages(&quot;devtools&quot;) devtools::install_github(&quot;adrian-bowman/rpanel/rpanel&quot;) Learning R A gentle introduction to R is provided in Chapter 2. Following this, in the early chapters of the book there are quite a few comments in the text on the R code used to analyse the data. In later chapters, very detailed comments are sometimes inserted as comments in the displayed code, in order not to distract attention from the narrative of the data and its analysis. The material in this book is intended to introduce you to R and what it can do. However, if you are new to R then, as you become more comfortable with the system, it is worth knowing that there is a great deal of useful information at the R project web pages at https://www.r-project.org. In particular, this includes: links to an enormous list of contributed packages at https://cran.r-project.org/web/packages/index.html; manuals at https://cran.r-project.org/manuals.html; an introduction to the syntax of R at https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf. Some reassurance This book provides code which shows how each dataset can be analysed. It is quite likely that you will at times see this and wonder how on earth you could possibly have thought of this yourself! Please be reassured. Code is very often the product of a lot of trial and effort. The final code presented here was often preceded by considerable a number of false starts and substantial editing. So you will often find yourself going through the same process as you experiment with the examples, attempt the exercises or address your own datasets. We all do! "],["where-data.html", "1 Where do data come from?", " 1 Where do data come from? Data can take many forms. Sometimes we are dealing with simple numbers, such as blood pressure measurements or age. Some data may consist of categories, such as country of origin or presence/absence of a characteristic of interest. Modern data can be much more sophisticated, arising in the form of images, temperature curves, three-dimensional surfaces or networks. This chapter focuses on relatively simple data forms but uses these to discuss some important principles of how data are collected and what they are able to tell us. "],["examples-data.html", "1.1 Some examples of data", " 1.1 Some examples of data 1.1.1 CO2 and global warming One of the world’s most pressing issue is climate change as a result of global warming. The levels of CO2 in the atmosphere are clearly implicated in this as a result of the ‘greenhouse’ effect. Accurate monitoring of CO2 began in the late 1950’s through the pioneering work of Charles David Keeling at Mauna Loa Observatory in Hawaii and measurements have been made continuously since then, providing an invaluable record of change. This ‘time series’ is displayed at monthly resolution in the left hand panel below. This time series makes it clear that there has been a steady rise in atmospheric CO2 over the entire period of monitoring. The implications of this become startling when we compare the series to data which indicate the levels of CO2 which correspond to earlier historical times. This can be done by measuring the CO2 content of air trapped in ice cores. Careful analysis of the ice layers allow the identification of the time scale, which stretches back over hundreds of thousands of years. The CO2 measurements from ice cores has been superimposed on the plot of the Mauna Loa data in the right hand panel above. This makes it clear that the modern measurements of CO2 are ‘off the scale’ of the historical levels. In order to assuage any concerns about the comparability between the modern atmospheric and historical ice core CO2 measurements, the plots below focus on the period where these overlap. The left hand plot highlights the sudden dramatic rise in CO2 as the industrial revolution gathered pace. The right hand panel zooms in on the years when both types of measurement are available, indicating the very strong level of agreement between the two. We will look at corresponding temperature changes later, but these data already paint a stark picture of the nature and size of the challenge we face in addressing climate change. The Intergovernmental Panel on Climate Change synthesises our scientific understanding of the process and continues to report on the current situation, with urgent calls to action. This example highlights that, in some settings, there is a need for an appropriate control to enable informative comparisons. Controls can sometimes be difficult to identify. In this case, considerable effort has been undertaken to ensure the validity of comparisons with the control data. The example also highlights the relevance of our scientific understanding of the context in which the experiment takes place. The physics of greenhouse gasses is well understood and this allows us to strengthen the interpretation of what we see in the data. 1.1.2 The first tuberculosis trial One of the very earliest systematic evaluations of medical treatment, marking a significant step in what we now term ‘clinical trials’, was a study on the effects of Streptomycin on pulmonary tuberculosis by Marshall et al. (1948). The statistician Austin Bradford Hill, whose picture is below, introduced very important methodology in this study. In his review of the development of clinical trials, Bhatt (2010) writes “This trial was a model of meticulousness in design and implementation, with systematic enrolment criteria and data collection compared with the ad hoc nature of other contemporary research. A key advantage of Dr Hill’s randomization scheme over alternation procedure was “allocation concealment” at the time patients were enrolled in the trial. Another significant feature of the trial was the use of objective measures such as interpretation of x-rays by experts who were blinded to the patient’s treatment assignment.” Figure 1.1: Sir Austin Bradford Hill, Wellcome Collection, CC BY. The headline results reported in the scientific paper are shown in the table below. Radiological assessment Streptomycin group Control group Considerable improvement 28 4 Moderate or slight improvement 10 13 No material change 2 3 Moderate or slight deterioration 5 12 Considerable deterioration 6 6 Deaths 4 14 Total 55 52 The beneficial effects of the treatment are clear and we can have confidence in the conclusion because of the careful conduct of the trial. Key features of the design include the presence of a control group and the use of randomisation. 1.1.3 The birds and the bees: how to tell the sex of a herring gull Herring gulls are found across the coastal regions of North-Western Europe. When studying the behaviour of these birds, it is useful to be able to identify sex. With this species, this is not possible by visual examination of the obvious anatomical features as the appropriate organs are internal. It would therefore be very useful to be able to identify the sex of a bird by taking simple measurements of some kind, on the assumption that the sexes are lijkely to differn in size, as happens with many animal species. The correct identification of the sex of a herring gull has to be carried out by dissection. The most suitable source of data for this purpose is therefore birds which have been found dead or have been culled for other reasons. Measurements from a sample of 100 male and 100 female birds, kindly provided by Prof. Pat Monaghan from thte University of Glasgow, are available for investigation. Length measurements, based on the distance between two of the yellow landmarks in the picture above, could be useful in distinguishing between the sexes. as males and females tend to have different sizes in many species. There is an interactive application in R which can help in thinking this through. A gentle introduction to R is provided in Chapter 2 but, if you have followed the guidance at the start of the book on installing R and the add-on package rpanel, then type in the following instructions into the console window to launch the app. library(rpanel) rp.gulls() Consider which pairs of landmarks might provide a suitable length measurement. Suitable criteria are: reproducible, by yourself and others; valid, in what they aim to measure; informative, as they are likely to be different for male and females; well calibrated, as they target a feature of interest; practical, as measurements can be made reasonably easily. Click on your selected pairs of landmarks and some feedback will be given. If you are able to identify some suitable measurements, checkboxes and buttons will appear to allow you to see some plots of the data, separated out by sex. 1.1.4 Tracking the covid pandemic When the covid19 pandemic began there was immediate and urgent effort to track its progress. That happened in many ways, most immediately in the numbers of deaths and hospital admissions. In the UK, when tests became available, the number of positive results was also regularly reported. The symptoms data collected by the app from the ZOE Health Study provided another source of information. Although these sources provided useful information, the most reliable estimates of covid infection levels came from a large survey conducted by the Office of National Statistics (ONS), in partnership with the University of Oxford, the University of Manchester, the UK Health Security Agency (UKHSA) and the Wellcome Trust. Swab and blood samples were provided by thousands of people from across the UK who had been selected and random and who had agreed to participate by providing repeated samples over an extended time period. Statistical modeling was also undertaken to ensure that the results represented the population as a whole. The ONS provided extensive information about the data, including details of the methods and study design. Scientific publication of the methods appeared in a Lancet paper. This is an example where very considerable effort was made to ensure that what we observe is an accurate representation of what is happening. The plot above uses the survey data to track the pandemic in terms of the percentage of people in the UK who tested positive. The virulence of the infection changed over time so deaths and hospital admissions show rather different patterns. We will look at data on those later. An account of the covid-19 tracking project is given in an article published by the Royal Statistical Society in its Statistics Under Pressure series. The hugely valuable nature of the information provided by the ONS survey is discussed in a Conversation article. 1.1.5 More complex data objects The examples above mostly involve measurements of a single quantity of interest, such as CO\\(_2\\), the level of health improvement or the proportion of people with covid19. The herring gulls example might involve several different measurements on each bird. However, data can be much more complex. Some examples are: high-resolution images captured from a video camera monitoring wildlife movement; free-form text entered into the search box of a web browser; an extensive set of responses recorded from an individual in a survey, including later questions which are conditional on the responses to earlier questions; a network describing the interactions of one individual with others in a group; the life history of a hospital patient. As the complexity of data increases, so the models required for analysis may also need to increase in sophistication. Nonetheless, some basic principles and concepts still apply and it is the aim of this book to discuss these. While the focus is on relatively straightforward types of data, the ideas will provide helpful building blocks for more complex situations. References Bhatt, Arun. 2010. “Evolution of Clinical Research: A History Before and Beyond James Lind.” Perspectives in Clinical Research 1 (1): 6. Marshall, Geoffrey, J. W. S. Blacklock, C. Cameron, N. B. Capon, R. Cruickshank andJ. H. Gaddum, F. R. G. Heaf, A. Bradford Hill, et al. 1948. “Streptomycin Treatment of Pulmonary Tuberculosis.” British Medical Journal, 769–82. "],["some-broad-issues.html", "1.2 Some broad issues", " 1.2 Some broad issues It is worthwhile reflecting on some of the broad issues which have already arisen in our review of these examples of data. To structure our reflections it will be helpful to recall the PPDAC framework discussed in the Preface, displayed below in graphical form. In this section we will focus on the Problem, Plan and Data stages. 1.2.1 What’s the problem? It is very important to define the objectives of an experiment clearly. Failure to do this will make subsequent analysis and interpretation very difficult. Pilot studies are a perfectly valid preliminary step but a clear objective is still needed. ‘Fishing expeditions’ are of rather limited use. Our objectives will be informed by previous work so a review of the scientific literature is a wise start. Scientific knowledge of the subject domain will be very helpful not only in deciding what problem to tackle but also in informing later decisions in the planning process and in due course in interpreting the results of our analysis. There are some general aspects of this process which are worth highlighting. The first is the distinction between an observational study where we simply observe and record variables of interest. We do not intervene or influence the situation in any way other by recording what we see. In an observational study we can only identify association between variables. A simple of example is the association between social or economic characteristics and political voting patterns. In contrast, a designed experiment involves the specification of `treatments’ which are assigned by the experimenter. Rather than simply observe, we intervene. We are then interested in the way the treatments affect the outcome. If a designed experiment is conducted well it allows us to identify causal relationships between variables. A simple example is a clinical trial, such as the tuberculosis trial described in Section 1.1, where we compare the effects of different treatments on the recovery of patients suffering from a particular medical condition. A second general issue is whether our aim is to understand the processes at work in our scientific context or whether we simply want to predict an outcome or design a system which will classify future observations into different groups. The classification of herring gulls into male and female groups discussed in Section 1.1 is an example of the latter. This is a case where we may not be primarily interested in which variables are involved in our model, simply in how successful our model is in prediction or classification. This may change our attitude to building a model, when the time comes, but it is also helpful to be aware of the distinction between these two aims from the start of the planning process. 1.2.2 What’s the plan? Acquiring data which are appropriate, informative and unbiase requires careful thought. The ONS covid-19 survey was described in Section 1. Take a look at the methodology guide for this survey. It is very extensive, covering all the major issues which had to be considered. One of the exercises at the end of this chapter ask you to consider how you might design a simple experiment to investigate the operation of short term memory, known to psychologists as “working memory”. How is a list of items recalled from memory? This is an experiment which can be caried out in a classroom or small group setting. It might surprise you just how extensive the list of detailed arrangements needs to be. One of the important tasks is to identify which measurements should be taken. The list should include not only those which are mentioned in the definition of the problem being tackled but also those which we know or suspect from previous work or our scientific understanding are likely to influence the process we are studying. This will help us to consider an appropriate experimental design - a topic we will revisit in a later chapter. 1.2.3 Where do the data come from? The diagram at the start of this section includes two headings, “Real world” and “Model world”. The question our experiment aims to tackle is about what is going on “out there in the wild”, even when the “wild” refers to a laboratory setting. If we consider all the observations we might ever make then we can view this as the population we are studying. The process of collecting a particular dataset then delivers a sample. Our modelling process will use this sample to try to understand what is going on in the population. It is then crucially important to obtain a sample which properly represents the population and does not suffer from serious bias. If we make serious mistakes at this stage, it is unlikely that we will be able to retrieve the situation. A good mechanism for avoiding bias is to use randomisation. This applies in two ways. The first is to the process of identifying which items from the population are recruited into our sample, ideally ensuring that all potential items have an equal chance of appearing. The second applies to the allocation of any treatments to sampled items. The tuberculosis trial outlined in Section 1.1 is one of the first occasions where this was used. It is now a standard component of clinical trials worldwide. 1.2.4 Who do you trust? We live in a world where ‘fake news’ has become a commonly used term. Not everyone is careful in the way data are collected, analysed and interpreted. Sadly, data are sometimes used selectively to support a conclusion already adopted. It is very important that this is countered and that the analysis of data is conducted in an honest and professional manner. The UK Statistics Authority is the body which oversees the production of official statistics in the UK. Its code of practice is based on the principles of: trustworthiness, confidence in the people and organisations that produce statistics and data; quality, data and methods that produce assured statistics; value, statistics that support society’s needs for information. The full code is well worth reading. Although it is couched in terms of official statistics, providing information for the public and for government, the principles it describes are very important. A Declaration on Professional Ethics is also provided by the International Statistical Institute. In addition to the obligation on all who collect and analyse data to act with professionalism and integrity, specific ethical considerations arise in the planning and design of experiments, particularly those involving humans. Indeed, the protocol for any experiment involving humans must be approved by an appropriate ethical committee before it can be put into practice. A simple example is in a clinical trial to compare two treatments where the size of the sample must be considered very carefully. The sample must be large enough to enable a clear conclusion about the treatments to be reached, but if it is too large then some patients may end up being given a treatment which might clearly have been shown by a smaller trial to be inferior. That would not be ethical. We will often find ourselves using data from other sources as part of our investigations. Indeed, many different sources are used in this book. The issue of trustworthiness, raised in the UK Statistics Authority code of conduct, again arises. What sources can we trust as reliable? Some guidance comes from the source organisations stated aims and code of conduct. Reputation also matters. Open documentation is important, so that the details of the data collection process can be reviewed. Accountability matters too, so that there is a mechanism for query and complaint if the need arises. The RSS has provided a document, Sound or suspicious? Ten tips to be statistically savvy which offers advice on how to assess claims that are made. "],["what-could-possibly-go-wrong.html", "1.3 What could possibly go wrong?", " 1.3 What could possibly go wrong? It is very instructive to think about studies which went wrong. 1.3.1 US presidential elections, 1936 and 1948 The Literary Digest was a magazine which surveyed 10 million people, beginning with its own readers, who they planned to vote for in the 1936 US presidential election. A massive 2.4 million people responded, leading to the prediction of a clear win for the candidate Alf Landon. In fact, Franklin Delano Roosevelt had a landslide victory. What went wrong? For another occasion when things went wrong in the prediction of the outcome of a US presidential election, see the article in the Los Angeles Times about the 1948 election. "],["further-reading.html", "1.4 Further reading", " 1.4 Further reading The Royal Statistical Society has a very helpful Guide to UK official statistics on climate change. The CEDA Archive, maintained by the National Environmental Research Council (NERC) in the UK, contains a very large collection of environmental data from atmospheric and earth observation research. If data are to be collected through a survey, the nature and construction of the questions are very important, to avoid bias or leading the respondent. The Pew Research Centre, a high profile independent organisation, provides a helpful discussion of writing survey questions. Copernicus: European environmental data. https://surfobs.climate.copernicus.eu/dataaccess/index.php Diggle and Chetwynd (2011) Rosling (2018) The Tiger That Isn’t. Andrew Dilnot &amp; Michael Blastland. How to lie with Statistics. Darrell Huff. Damned Lies and Statistics. Joel BEst. More Damned Lies and Statistics. Joel Best. Innumeracy. John Allen Paulos. Reckoning with Risk. Gerd Gegerenzer. (Some people object to technical errors?) Dicing with Death. Stephen Senn. Risk. John Adams. Britain in Numbers. Simon Briscoe. Why Do Buses Come in Threes. Rob Eastaway. How Long is a Piece of String. Rob Eastaway. How to Take a Penalty. Rob Eastaway. References Diggle, Peter J, and Amanda Chetwynd. 2011. Statistics and Scientific Method: An Introduction for Students and Researchers. Oxford University Press. Rosling, Hans. 2018. Factfulness. London: Hodder &amp; Stoughton. "],["exercises.html", "1.5 Exercises", " 1.5 Exercises 1.5.1 An investigation of short-term memory Consider how you might design a simple experiment to investigate the operation of short term memory, known to psychologists as “working memory”? How is a list of items recalled from memory? This is an experiment which can be caried out in a classroom or small group setting. It might surprise you just how extensive the list of detailed arrangements needs to be. Once you have spent some time considering this, you may like tyo consult Bowman (1994) which describes some of the issues which arose in a classroom setting. 1.5.2 A survey of dental health Imagine you have been commissioned to conduct a survey of the dental health of five year old children in England. Write down some of the things on which you will need to make decisions and sketch out some possible answers. This should include how a suitable sample of children will be selected, what measurements will be made, how this will be done, and another other issues which you think are relevant. In fact, a survey of exactly this type is regularly conducted under the National Dental Epidemiology Programme (NDEP) for England. Once you have spent some time considering the issues, you can see the detail of what was done in the documents available here for the 2016-2017 survey. The protocol document describes the planning of the survey in considerable detail. The results of a further survey in 2022, using the same protocol, are also available. 1.5.3 Hearings aid and dementia A Lancet paper studied the association between hearing loss and dementia, in particular examining the role of hearing aids. The interpretation of the findings were: In people with hearing loss, hearing aid use is associated with a risk of dementia of a similar level to that of people without hearing loss. With the postulation that up to 8% of dementia cases could be prevented with proper hearing loss management, our findings highlight the urgent need to take measures to address hearing loss to improve cognitive decline. Are there other possible interpretations? You may wish to look at this article published by the British Geriatrics Society which discusses the issue. The article also provides a link to a further scientific paper for the technical detail. References Bowman, A. W. 1994. “Teaching by Design.” Teaching Statistics 16: 2–4. https://doi.org/10.1111/j.1467-9639.1994.tb00670.x. "],["R-introduction.html", "2 A gentle introduction to R", " 2 A gentle introduction to R This chapter assumes no previous experience of R. Those who do have some exposure may still find the material useful in rehearsing some of the key concepts, as these are a little different from some other computing environments. If you haven’t yet installed R, guidance for this is given in the section entitled Getting started with R at the very beginning of the book. "],["r-fuss.html", "2.1 R: what’s all the fuss about?", " 2.1 R: what’s all the fuss about? 2.1.1 History In the 1970’s, a research project led by John Chambers in Bell Labs produced an experimental statistical computing environment called S. This was designed around high level structures for handling data. It became very popular and began to be used in teaching, although its origins as a research project meant that the standard methods users expect needed to be added. A commercial version was created to support this and to provide the facilities that package users expect. In the 1990’s, Ross Ihaka and Rob Gentleman from the University of Auckland started to create a system which was ‘not unlike’ S in syntax but which was available as open source software. Over the years this became so popular that it has become the standard computational environment in Statistics and is widely used in many other areas. Why did this happen? There are several reason and these are outlined in the subsection below. The power of the system, its cost (nil), access to state-of-the art modelling tools and a very well organised mechanism for sharing add-on contributed packages are some of the reasons. R has become a research sharing mechanism, where methods described in scientific papers can quickly be made available for others to use. To give one example from genetics, the popular Bioconductor project provides tools for the analysis and comprehension of high-throughput genomic data. The articles below indicate how big an impact R has made. Vance, Ashlee (2009). “Data Analysts Are Mesmerized by the Power of Program R: [Business/Financial Desk]”. The New York Times. https://www.nytimes.com/2009/01/07/technology/business-computing/07program.html Vance, Ashlee (2009). “R You Ready for R?”. The New York Times. http://bits.blogs.nytimes.com/2009/01/08/r-you-ready-for-r/ Muenchen, Robert (2017). “The Popularity of Data Science Software”. http://r4stats.com/articles/popularity/ 2.1.2 Why should I be interested? There are many reasons why R may be very helpful for you. Access to very powerful statistical and visualisation tools. Easy access to an enormous library of user-contributed packages which provide state-of-the-art specialist tools for a wide variety of statistical topics and application areas. Relatively simple production of publication quality graphics. A full and very flexible programming environment in which to develop your own functions and tools. Good facilities for connecting with other computing languages (C++ etc.) and to other software systems (Matlab, OpenGL, etc.). Powerful tools to assist in producing reproducible research, including the creation of documents which integrate code and text into a self-generating report. It is freely available under a GNU licence. It operates in a consistent manner across all major computing platforms (Windows, Mac, Unix, Linux). It is supported by a very large community of users. The simplest form of operation is based on scripts containing written instructions. This approach has many advantages. It allows detailed, fine control of exactly what happens. It promotes logical thinking about the data analysis process as a whole. It allows iteration and refinement of the modelling process as scripts are improved by editing. It creates a record of the analysis which can be revisited at any time, without starting again from scratch. The script defines the analysis in a reproducible manner. 2.1.3 What’s the down side? For the reasons outlined above, R is not a ‘point-and-click’ system. However, this necessarily means that the syntax has to be learned by the user. All systems have an element of syntax but R requires more effort to learn than most. This is a consequence of its power and flexibility. So there is a learning curve to climb. If your statistical analysis requires only simple techniques then R may not be for you. "],["taste-of-r.html", "2.2 A taste of R", " 2.2 A taste of R Example: Mass and speed in quadrupedal rodents In an investigation of the relationship between mass (kg) and speed (km/hr) in mammals, Garland (1983) collected information from published articles on these two variables for a large number of different species. These measurements were recorded for a variety of four-footed rodents. (The common names of the species are taken from Corbet &amp; Hill, 1986.) Notice that the measurements are not all recorded to the same level of accuracy since the results have been collated from the work of a number of different scientists. Is it true that the bigger you are the faster you can run? We will use this simple example to explore how R operates. This will include the idea of a script where R instructions can be typed and, when required, copied into the Console window for R to execute. The majority of datasets in this book are read directly from external sources and Section 2.4 below shows how this can be done conveniently. Some datasets, such as the rodent one described above, are not available externally and these are provided within the add-on package rpanel. If you haven’t yet installed this package, follow the instructions in the Getting started with R page which immediately precedes Chapter 1. The first thing to do is to set up a new script. If you are using RStudio, this can be done from the menu: File &gt; New file &gt; R Script. (The detailed wording of the menu items may change with the particular computing system you are using.) Now type the following text into this script window. data(rodent, package = &#39;rpanel&#39;) It is strongly recommended that all R instructions are placed in a script, to build up a record of your analysis which can be edited and re-executed as needed. As discussed in Section 2.1, this provides a very convenient and reproducible workflow. Instructions can easily be copied and pasted from the script window into the console window. (This can be done efficiently by placing the cursor on the line of interest, or highlighting a block of code, and then using a keystroke combination such as Ctrl + Enter or command + Enter, although this may depend on your particular computer system.) Copy and paste the data instruction now. In R, data is a function designed to make a dataset available, in this case the rodent dataset from the rpanel package. It is helpful to think of R as a collection of functions. A function is something which accepts input of some kind and creates an output or action of some kind. The inputs are specified through the arguments of the function. Here the first argument is the name of the dataset. Functions often have many arguments. In this case, the second argument has been ‘named’ to identify that it refers to the ‘package’ argument, out of all the other arguments available. (If the package argument had been the second one in the definition of the function, we could have omitted naming it.) There is no automatic response from R but the rodent dataset is now available in the object rodent. This has rows for each rodent and columns for each variable. It is referred to as a dataframe. We can see what is inside any object by typing its name in the console. (Alternatively, the object can be inspected by clicking on its name in the RStudio Environment window.) rodent ## Mass Speed ## North American Porcupine 9.000 3.2 ## Woodchuck 4.000 16.0 ## Long-clawed ground squirrel 0.600 36.0 ## Long-tailed souslik 0.600 20.0 ## Eastern grey squirrel 0.550 27.0 ## European souslik 0.500 18.0 ## European red squirrel and Persian squirrel 0.400 20.0 ## Belding&#39;s ground squirrel 0.300 13.0 ## Rat 0.250 9.7 ## American red squirrel 0.220 15.0 ## Golden hamster 0.110 9.0 ## Eastern American chipmunk 0.100 17.0 ## Chisel-toothed kangaroo rat 0.056 21.0 ## Meadow vole 0.050 11.0 ## Least chipmunk 0.045 16.0 ## Merriman&#39;s kangaroo rat 0.035 32.0 ## Fawn hopping mouse 0.035 14.0 ## Pine mouse 0.030 6.8 ## Deer mouse 0.030 9.1 ## White footed mouse 0.025 11.0 ## Woodland jumping mouse 0.025 8.6 ## North American meadow jumping mouse 0.018 8.9 ## House mouse 0.016 13.0 To explore the data, plots always provide a good starting point. The plot function is very powerful and adaptable. Here we give it two variables and a scatterplot is produced. Notice that we refer to variables inside the dataset by &lt;dataframe name&gt;$&lt;variable name&gt;. What does this plot tell us? plot(rodent$Mass, rodent$Speed) A noticeable feature is that the data are ‘bunched up’ at one end and stretched out at the other, making any underlying pattern difficult to see. It would be worthwhile considering a change of scale by applying the log transformation to both axes. The log transformation is often effective in dealing with data which are skewed in this manner but there is also a more reasoned justification in this example. Mass is proportional to volume, which is the product of three dimensions - length, breadth and height (when adopting a rather drastic simplification of rodent shape!). On the other hand speed refers to the distance travelled in unit time and so it is one-dimensional. When the log transformation is applied to a product it creates quantities which add together on the log scale. With some further thought on the mathematical relationships involved, there is a strong aregument for the appropritaeness of the log transformation. (You are invited to consider this further in one of the Exercises at the end of the chapter.) The plot below shows this to be very effective at putting the data onto scales where the variation is more consistent. Notice that we do not need to create new variables containing the log transformed data. We can simply apply the log function inside our instruction. Note that this refers to logarithm to the base \\(e\\), the mathematical constant with value 2.7182…, and not to the base 10 (for which the function is log10). plot(log(rodent$Mass), log(rodent$Speed)) model1 &lt;- lm(log(Speed) ~ log(Mass), data = rodent) abline(model1, col = &#39;red&#39;) The instructions also fit a simple linear trend to the data. This is an example of a linear model which we will discuss in detail in Chapter 10. The lm function accepts a formula of the form &lt;response variable&gt; ~ &lt;explanatory variable&gt;. We can also conveniently specify the dataframe we are working with in the data argument. The syntax of this instruction also makes an assignment by placing the result of the lm function into a new object with a name of our own choosing. The backwards arrow &lt;- is how we make assignments in R. Objects can contain numbers, text, vectors of data, matrices, dataframes and many other things, some of them rather sophisticated. This is one of the more sophisticated cases as model1 now contains a fitted linear model. The fitted model has been passed to the abline which is designed to draw straight lines. What should it do? This illustrates that R is an object-oriented system. The abline function recognises that the object which has been passed to it is a linear model with a single ‘explanatory’ variable. There is a method for that kind of object which locates the slope and intercept of the fitted line and draws that on the plot, with the colour set by the col argument. The red line shows the effect of fitting a simple linear model. This is almost entirely flat and so provides no evidence of any relationship between Mass and Speed. Are you content that this model provides a good description of the data? Inspection of the plot raises concern about the observation in the lower right corner. This has the largest mass but the slowest speed. What is this rodent? It is the porcupine, whose spines provide an excellent defence mechanism as an alternative to running fast to escape a predator. We should not throw away data because they appear to be inconvenient. However, where there is a good biological motivation, as here, we can investigate the effect of fitting models without particular observations. The subset argument of the lm function allows us to do that easily. The porcupine is the first observation in the dataframe and the minus sign indicates that it should omitted. The green line shows the effect of omitting the procupine and this does suggest an underlying relationship between mass and speed for the other rodents. plot(log(rodent$Mass), log(rodent$Speed)) model2 &lt;- lm(log(Speed) ~ log(Mass), data = rodent, subset = -1) abline(model2, col = &#39;green&#39;) This simple example has proceeded slowly to enable detailed comments on the syntax and construction of the R code. The sequence of steps we have taken also illustate that building a suitable model involves careful thought, critical inspection, and the use of our understanding of the context. At this stage, we should save our script in a file. This defines our analysis. It allows us to reload the script when we need to revisit or refine the analysis. It is also easy to send the file to others to show precisely what we have done. This provides an excellent basis for reproducible research. It is worthwhile spending a few moments to add comments so that we can remind ourselves about the thought process we have gone through. Lines whose first non-space character is # will be ognored when the R code is executed. # Make the rodent data available from the rpanel package data(rodent, package = &#39;rpanel&#39;) # Plot Speed against Mass plot(rodent$Mass, rodent$Speed) # Use the log transformation to provide more effective scales plot(log(rodent$Mass), log(rodent$Speed)) # Fit a simple straight line model and display this on the plot. model1 &lt;- lm(log(Speed) ~ log(Mass), data = rodent) abline(model1, col = &#39;red&#39;) # Fit the model without the porcupine which is biologically different plot(log(rodent$Mass), log(rodent$Speed)) model2 &lt;- lm(log(Speed) ~ log(Mass), data = rodent, subset = -1) abline(model2, col = &#39;green&#39;) This can be taken further with R markdown which is a very powerful system for combining text and R code in a single document. When this document is compiled the R code runs to perform the analysis and generate graphs and other forms of output. "],["a-summary-of-key-concepts-in-r.html", "2.3 A summary of key concepts in R", " 2.3 A summary of key concepts in R It is useful to take stock of some key ideas. Everything in R is an object. To expand on this, examples of objects include: single values; the type could be integer, numeric, character, logical or date; a set of values, all of the same type, referred to as a vector; a vector can be created directly by using the c function to collect values together, for example as c(1, 5, 3, 8, 6); a matrix of values, all of the same type, arranged in row and column format; an array of values, all of the same type, arranged with multiple dimensions; a dataset, called in R a dataframe; a list which brings together different types of object into a single object; a dataframe is an example of a list object; the individual components can be accessed by using the name of the list object and the name of the component, separated by a $ sign; a function. R can be thought of as a large collection of functions which operate on objects and produce some kind of output, perhaps a plot or a new object. The standard form of a statement in R is result &lt;- &lt;functionname&gt;(&lt;arguments&gt;). (Here the &lt;...&gt; is a placeholder for an actual function name or a set of arguments.) The round brackets are an essential part of the syntax as they delimit the input arguments. Here are some other things which it is useful to know when writing R instructions. Help! The number of functions available even in base R can be rather overwhelming but you can achieve a lot by becoming familiar with a relatively small set. If you know the name of the function you would like to use, say lm, then the instruction ?lm will show a help file with all the details of how the function can be used. Examples are also usually given. You may find that the help information is sometimes complex. This is the downside of the power and flexibility of the functions available, as the help files have to document all the different uses to which they may be put, and all the different arguments available to control the details of the operation. More help! A full-blooded introduction to R as an environment and a programming language is available from the RStudio menu Help &gt; R Help. This brings up in the Help pane links to a large collection of resources, including An Introduction to R. Cheatsheets. A large collection of helpful summaries of different topics are available from the RStudio menu: Help &gt; Cheatsheets &gt; Browse Cheatsheets .... Many refer to the functions available in particular R packages. If you scroll down to the Contributed section you will find one on Base R. Specifying function arguments. Functions often have multiple arguments and each of these has a name given in the definition of the function. When the function is called, the arguments can be named. When they are not named, they can be identified by their position in the list of arguments. For example, the code for the rodent example used the lm function in the form: lm(log(Speed) ~ log(Mass), data = rodent, subset = -1) The help file for lm shows that the first three arguments are formula, data and subset. The call above names the data and subset arguments but the formula has been left unnamed. It is the first argument, so its meaning is clear. We could have omitted the names of the other two arguments because, as it happens, these are the second and third arguments in the function definition. However, the statement is much more readable if we give the names. If we wish to set one of the arguments further down the list then we need to use the name, otherwise there is ambiguity over which argument we intend to use. Square bracket notation. It is very helpful to be able to specify a subset of an object. Square brackets provide a convenient mechanism to do this. For example, the speed the porcupine can be expressed as rodent$Speed[1]. The speeds of all the other rodents are available as rodent$Speed[-1] where a negative sign indicates that this case should be removed. For dataframes, square brackets need row and column information so, for example, rodent[1, ] gives the data for the porcupine while rodent[-1, ] provides a dataframe with the porcupine removed. Logical expressions. These are very useful, particularly in specifying a subset of a vector or dataframe. For example, we wished to exclude the porcupine when we fitted our second linear model to the rodent data. It was convenient to use the subset argument of the lm function but we could instead have defined an indicator vector as: ind &lt;- (rownames(rodent) != &#39;North American Porcupine&#39;) (The brackets around the logical expression make it clear that the order of operations is to perform the test first and then assign the result to ind.) A test for equality is performed by == while != corresponds to ‘not equal’. If you inspect the contents of ind you will see a vector of logical values, one for each rodent, which are TRUE in every case except for the porcupine, where it is FALSE. We could now fit the linear model by using our logical variable in conjunction with the square bracket notation as lm(log(Speed)[ind] ~ log(Mass)[ind], data = rodent) or as lm(log(Speed) ~ log(Mass), data = rodent[ind, ]). A further possibility in this case would be to use the subset argument as lm(log(Speed) ~ log(Mass), data = rodent, subset = ind). One advantage of this over the use of the indicator -1 is that the code is more readable - in the definition of ind we can see what’s happening. Finally, although we haven’t needed it in this example, logical variables and expressions can be combined using the &amp; (‘and’), &amp;&amp; (‘conditional and’, where the second expression is examined only if the first is TRUE), | (‘or’) and ! (‘not’) operators. Again, for vectors this is done element by element. Packages. The ability to access the additional facilities provided by the very large number of packages available for R is one of its strengths. The rpanel package has already appeared but many more will be used throughout this book. Remember that when the library function is used to load a package this assumes that the package has already been installed. You may need to do that first by calling the install.packages function. The intention of this chapter is simply to provide a head start in learning R in order that it may be used to engage seriously with the business of statistical modelling. The concepts discussed here should allow us to do many useful things. Many other facilities and ways of doing things will appear along the way. Learning by doing is a good way of proceeding. For those who would prefer a more systematic and thorough coverage of R, the resources mentioned under Help! above and the further resources listed near the end of this chapter will prove very helpful. "],["reading-data.html", "2.4 Reading data into R", " 2.4 Reading data into R 2.4.1 Accessing the data used in this book There are many examples of datasets throughout this book and, wherever possible, these are read from their original source. This is a good principle as it avoids the possibility of mistakes by copying and redistributing the data but, more importantly, it encourages the data to be seen in their original context. Also, some of the datasets are regularly updated so accessing these from source ensures that the most recent data are used. Many of the datasets are therefore accessed through web addresses. Some of these are rather long and some need to be downloaded locally before they can be read into R. The rp.datalink function is therefore provided in the rpanel package to help. library(rpanel) rp.datalink() ## name type ## 1 Ben_Nevis .csv ## 2 centenarians .txt ## 3 children_services .zip .... A list of the datasets available is provided by calling the function with no arguments. The output above shows the first few lines of the result. The remote file addresses are truncated but these can be viewed in full by assigning the output of the rp.datalink function to an object and inspecting it. When an individual dataset is needed, it can be accessed by calling the function with the name of the dataset as an argument. path &lt;- rp.datalink(&#39;centenarians&#39;) centenarians &lt;- read.table(path, header = TRUE) The object which the rp.datalink function creates is simply a piece of text which gives the location of the dataset. As some R functions cannot read directly from remote locations, in this case the dataset is downloaded to a local temporary file and it is this file name which is returned. It is often convenient to be able to work with datasets offline so an option to download datasets to a convenient local directory is also provided. The second ‘action’ argument of the function allows a local directory to be specified and files to be downloaded there. For the download action, the name 'all' can be given, in which case all the datasets will be downloaded. rp.datalink(&#39;~/Desktop/temp&#39;, &#39;set local directory&#39;) rp.datalink(&#39;centenarians&#39;, &#39;download&#39;) 2.4.2 Functions for reading data One of the first things to learn is how to read your own data into R. Some useful functions from base R are listed below. read.table: this takes a file with plain text information and reads it in row/column format into a dataframe. If the first line of the file contains the names of the variables in the different columns, then use the additional argument header = TRUE. There are many other arguments to control details. This function can handle a filename which is a remote web address. read.csv: this is very convenient when data have been exported from Excel or similar systems in ‘comma separated’ format. Fine control of separation characters and other features is available, if required. load: this is a very convenient function for placing into your R workspace objects of any type which have previously been saved, by yourself or others, using the save function. This offers considerable flexibility because it can handle lists and other objects which are not in standard row/column format. Similar functions are available for exporting data and information from R. The analogues of some of the functions above are write.table and write.csv. Several packages which provide additional facilities for reading data are available. For example, the readr package provides read_table, read_csv and several other functions, while the readxl package provides the read_excel function to read Excel files directly. A helpful summary is provided in a cheatsheet. Sometimes data are made available in the form of a ‘zip’ file, which may contain one or more individual files. The code below gives an example of reading data on the UK parliamentary boundaries of 2024, available on the ONS web site. The unzip function unpacks the files into a specified folder from which the individual files can be read. (This particular dataset will be used in a later case study.) path &lt;- rp.datalink(&quot;UK_parliamentary_boundaries_2024&quot;) unzip(path, exdir = &#39;~/Desktop/temp&#39;) An example of reading an Excel file is given in Section 2.4.3 below. The CRAN website provides extensive details on import/export issues, including facilities for reading data from other software systems. The rio package also provides a helpful interface to various packages for reading different kinds of data files. 2.4.3 An example: reading an Excel spreadsheet The ONS holds information on marriage, cohabitation and cohort analysis. The code below uses the rp.datalink function to provide the path to the Excel file holding historical data and then the read_excel function from the readxl package to read the data. The further arguments of read_excel locate the particular sheet of interest, skip some lines with header information, stop at the appropriate row and rename the columns as we wish. The read_excel functions actually delivers the data in a form known as a tibble. This is a slightly enhanced form of dataframe whose useful properties we will encounter later. library(readxl) path &lt;- rp.datalink(&#39;married_men&#39;) married_men &lt;- read_excel(path, sheet = &quot;Table 2&quot;, skip = 9, n_max = 96, col_names = c(&quot;Year&quot;, as.character(17:50))) This dataset essentially describes a surface over year and age so it is interesting to plot it in this form. The rgl package provides powerful facilities for three-dimensional plotting by giving access to the OpenGL system. In the code below, the grid points on the x and y axes and the matrix of values of surface heights are passed as the principal arguments to the persp3d function. The heights are given by omitting the first column of the dataframe d and scaling down by 1000 to create proportions. library(rgl) persp3d(married_men$Year, 17:50, as.matrix(married_men[ , -1]) / 1000, col = &quot;lightgreen&quot;, xlab = &quot;Year&quot;, ylab = &quot;Age&quot;, zlab = &quot;Proportion married&quot;) The dramatic post-war change in marriage patterns is very clear. With the web version of this book, you should be able to rotate the plot using the mouse controls. "],["a-further-example-the-scottish-independence-referendum.html", "2.5 A further example: the Scottish independence referendum", " 2.5 A further example: the Scottish independence referendum Example: The Scottish Referendum on Independence The results of the Scottish referendum on independence were of enormous interest to the UK. A research paper from the House of Commons Library reported the voting patterns separately by Council area, together with a variety of social and demographic characteristics. The data are available in the Scottish_referendum dataframe in the rpanel package. The information in this file is: Council the Scottish council name Voted.no the percentage of people who voted no Turnout the percentage of people who voted Population the number of people eligible to vote Unemployment.rate the percentage unemployed Scottish.identity.only the percentage who identify themselves as Scottish only Aged.16 the percentage who are 16 years of age Aged.over.50 the percentage who are over 50 years of age Aged.over.65 the percentage who are over 65 years of age We can use this information to explore the demographic and geographic patterns of voting. The dataset is available in the Scottish_referendum dataframe in the rpanel package. Here we also use the head function to inspect the first few rows. head(Scottish_referendum) ## Council Yes No Rejected Total Electorate ## 1 Aberdeen City 59390 84094 180 143664 175751 ## 2 Aberdeenshire 71337 108606 102 180045 206490 ## 3 Angus 35044 45192 66 80302 93656 ## 4 Argyll and Bute 26324 37143 49 63516 72014 ## 5 Clackmannanshire 16350 19036 24 35410 39974 ## 6 Dumfries and Galloway 36614 70039 122 106775 122052 ## Scottish_only_identity Born_in_Scotland Unemployed Age_65_or_over ## 1 54.7 75.0 7.6 14.8 ## 2 61.3 80.5 5.6 17.2 ## 3 66.8 85.9 9.4 21.5 ## 4 57.4 76.1 10.3 23.4 ## 5 67.0 86.4 14.9 17.6 ## 6 59.6 77.1 11.4 23.3 R can act as a simple calculator. For example, consider the data recording the proportion of people voting ‘no’ and the proportion of people who voted. We can identify the number of people who voted ‘no’ in each region, and then confirm the percentage of those who voted ‘no’ across the country, by Scottish_referendum$Vote &lt;- Scottish_referendum$Yes + Scottish_referendum$No + Scottish_referendum$Rejected Scottish_referendum$Turnout &lt;- 100 * Scottish_referendum$Vote / Scottish_referendum$Electorate Scottish_referendum$Yes_percent &lt;- 100 * Scottish_referendum$Yes / Scottish_referendum$Vote 100 * sum(Scottish_referendum$No) / sum(Scottish_referendum$Vote) ## [1] 55.25078 The first instruction multiplies each population by the corresponding turnout proportion to find the number of people who voted in each region. Notice that the operation is performed for each element of Scottish_referendum$Population and the corresponding element of Scottish_referendum$Turnout. The resulting vector of numbers is stored in a new component of the Scottish_referendum dataframe, with the variable name Vote. The second instruction calculates a weighted average of the percentages voting ‘no’, using the number of people voting in each region as the weights. This creates the overall percentage who voted ‘no’. It would be interesting to produce a scatterplot to explore the relationship between the percentage of people who voted ‘no’ and the unemployment rate of the Council regions. The plot function can do this for us. There seems to be quite a strong relationship here. plot(Scottish_referendum$Unemployed, Scottish_referendum$Yes_percent) It might be more helpful to plot the Council names instead of simple points. Here we add the argument type = \"n\" to the plot function to stop any points being plotted and then use the text function to plot the Council names instead. plot(Scottish_referendum$Unemployed, Scottish_referendum$Yes_percent, type = &quot;n&quot;) text(Scottish_referendum$Unemployed, Scottish_referendum$Yes_percent, Scottish_referendum$Council) "],["further-reading-1.html", "2.6 Further reading", " 2.6 Further reading Data Science for Psychologists. https://bookdown.org/hneth/ds4psy/ The first appearance of R in the scientific literature: Ihaka and Gentleman (1996). References Ihaka, Ross, and Robert Gentleman. 1996. “R: A Language for Data Analysis and Graphics.” Journal of Computational and Graphical Statistics 5 (3): 299–314. https://doi.org/10.1080/10618600.1996.10474713. "],["exercises-1.html", "2.7 Exercises", " 2.7 Exercises These exercises are intended to reinforce the introductory material on R functions and expressions. They require relatively simple R code, although examination of the help files for relevant functions may be required in order to identify useful arguments and their options. 2.7.1 Scottish Referendum data Here are some further things to explore. It would be interesting to relate the turnout to the unemployment rate. This can be done simply by replacing Voted.no with Turnout in the code. Try this. What do you see? Remember that you can use plot and text together to see the Council names. There are several other variables whose relationships with turnout and with the ‘no’ vote it would be interesting to explore. Two of these are Scottish.identity.only and Aged.over.50. Spend some time creating plots with these two variables on the horizontal axis and with Voted.no and Turnout on the vertical axis. Any comments? 2.7.2 Aircraft designs In a study of how aircraft technology develop during the 20th century, Saviotti and Bowman (1984) collated some simple physical characteristics of each design. These included: total engine power (kW); wing span (m); length (m); maximum take-off weight (kg); maximum speed (km/h); range (km). The year (`Yr) of the first appearance of each design is also provided. Produce some plots to investigate the patterns of change in these individual characteristics over time. The data are available in the aircraft dataframe in the sm package. (Remember that if this package is not already installed then the install.packages instruction needs to be given to do that.) 2.7.3 Centenarians What are the population age trends in Scotland? Data from the National Records of Scotland are available in the dataset centenarians, which can be access in the usual way through the rp.datalink function. The variables are: Region the administrative region of Scotland Males the total number of males of all ages in 2014 Males90 the number of males over 90 years of age in 2014 Males100 the number of males over 100 years of age in 2014 Females the total number of females of all ages in 2014 Females90 the number of females over 90 years of age in 2014 Females100 the number of females over 100 years of age in 2014 Males90y2004 the number of males over 90 years of age in 2004 Females90y2004 the number of females over 90 years of age in 2004 Explore the data in any way you see fit, in order to highlight recent trends. Here are some questions which might guide you. Is there a consistent pattern in the numbers of males and females over 90 years of age in 2014? Is the ratio of males to females similar across the regions? Repeat this for those over 100 years of age. Repeat this for those over 90 years of age using the data from 2004. 2.7.4 Justification of the log transformation for the rodent data This exercise is for those who are comfortable in handling mathematical formulae. In the discussion of the rodent dataset, a log transformation was used to very good effect. If you are comfortable in handling mathematical formulae, consider a simple model which relates speed (S) to length (L), breadth (B) and height (H) through the relationship which assumes speed to be proportional to a power of volume: \\[ S = \\alpha (L B H)^\\beta \\] If \\(\\beta = 1\\) then we have simple proportionality but using the parameter \\(\\beta\\) allows other relationships to be described. What happens when you apply a log transformation? Consider in particular what happens in the realistic scenario when length, breadth and height all increase by the same proportion, becoming \\(\\gamma L\\), \\(\\gamma B\\) and \\(\\gamma H\\), for example when \\(\\gamma = 1.1\\) for a 10% increase. What happens to S when the log transformation is used? References Saviotti, P P, and A W Bowman. 1984. “Indicators of Output of Technology.” In Proceedings of the ICSSR/SSRC Workshop on Science and Technology in the 1980’s, edited by M Gibbons et al., 117–47. Brighton: Harvester Press. "],["data-wrangling.html", "3 Data wrangling", " 3 Data wrangling The organisation and restructuring of datasets is a very important step in preparing for later analysis. This is sometimes called data wrangling and some powerful tools for this are outlined here. "],["reorganising-data.html", "3.1 Reorganising data", " 3.1 Reorganising data It is rarely the case that the form in which data are collected is immediately suitable for analysis. There are often things we need to do to put the data into a more suitable structure. Here is an example. Data on the number of deaths related to covid-19 are available from the National Records of Scotland. Background information is available from the detailed notes by selecting the About tab. The data can be conveniently accessed through the rp.datalink function. The function str gives a useful indication of the type of data present. path &lt;- rp.datalink(&#39;covid19_deaths_scotland&#39;) covid_deaths &lt;- read.csv(path) str(covid_deaths) ## &#39;data.frame&#39;: 10510 obs. of 9 variables: ## $ FeatureCode : chr &quot;S92000003&quot; &quot;S92000003&quot; &quot;S92000003&quot; &quot;S92000003&quot; ... ## $ DateCode : chr &quot;w/c 2020-08-17&quot; &quot;w/c 2020-11-02&quot; &quot;w/c 2020-09-14&quot; &quot;w/c 2020-08-10&quot; ... ## $ Measurement : chr &quot;Count&quot; &quot;Count&quot; &quot;Count&quot; &quot;Count&quot; ... ## $ Units : chr &quot;Deaths&quot; &quot;Deaths&quot; &quot;Deaths&quot; &quot;Deaths&quot; ... ## $ Value : num 0 9 0 1 32359 ... ## $ Sex : chr &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; ... ## $ Age : chr &quot;45-64 years&quot; &quot;45-64 years&quot; &quot;45-64 years&quot; &quot;45-64 years&quot; ... ## $ CauseOfDeath : chr &quot;COVID-19 related&quot; &quot;COVID-19 related&quot; &quot;COVID-19 related&quot; &quot;COVID-19 related&quot; ... ## $ LocationOfDeath: chr &quot;All&quot; &quot;All&quot; &quot;All&quot; &quot;All&quot; ... Like all real datasets, we need to think carefully about the structure of the data, the way it has been coded and many other detailed aspects. As our understanding grows and our thinking develops so too the R script we use will evolve to represent our exploration and analysis. The code below is the product of a lot of experimentation - and many mistakes! For example, it took a while to understand the meaning of the FeatureCode variable. The setting shown below gives the data for the whole of Scotland, but more serious use of the data should confirm and document that. As ever, some filtering and recoding of the data may be useful. The code below uses the subset function to pull out the data for Scotland as a whole, for cause of death related to covid 19, for deaths in all settings. Summary numbers for the years 2020 and 2021 are also removed. It is also helpful to indicate that the information in DateCode is a date, as R has special facilities for dates. The sub function removes the “w/c” text at the start of each date code and the uses the as.Date function to tell R that this is a date. covid_deaths &lt;- subset(covid_deaths, FeatureCode == &quot;S92000003&quot; &amp; CauseOfDeath == &quot;COVID-19 related&quot; &amp; LocationOfDeath == &quot;All&quot; &amp; !(DateCode %in% c(&quot;2020&quot;, &quot;2021&quot;))) covid_deaths$DateCode &lt;- sub(&quot;w/c &quot;, &quot;&quot;, covid_deaths$DateCode) covid_deaths$Date &lt;- as.Date(covid_deaths$DateCode) In order to plot the deaths over time, we need to ensure that we have the dates in the correct order. The order function returns the set of indices which will do this. We can then use the square bracket notation to put the dates and numbers of deaths into the correct order. The type = 'l' argument in the plot function joins these points by lines. sbst &lt;- subset(covid_deaths, Age == &quot;All&quot; &amp; Sex == &#39;All&#39;) ind &lt;- order(sbst$Date) plot(sbst$Date[ind], sbst$Value[ind], type = &#39;l&#39;, xlab = &#39;Date&#39;, ylab = &#39;Number of deaths&#39;) This shows the overall course of the pandemic in the UK. An exercise at the end of Chapter 4 on Data Visualisation chapter will invite you to explore this at more detailed levels of age and sex. "],["dplyr.html", "3.2 Making life easier: the dplyr package", " 3.2 Making life easier: the dplyr package In the previous section, standard R functions were used to reorganise data. In principle, most things can be achieved by this route but the code can become complex to handle very detailed manipulations. As often happens, we can take advantage of the careful work of others by using packages which will help. The dplyr package is an excellent example. This provides powerful tools to manipulate data using high-level functions which take care of the details for us. Brief examples of these tools are described in this section, using the data on covid19 deaths in Scotland, discussed in the previous section. Remember that packages need to be installed on your computer before they can be used, as was done for the rpanel package at the start of the book. The code below illustrates this for the new packages we need, but it will be assumed for other package use later in the book. Installation needs to be done only once, with packages made available for a particular R session through the library function. install.packages(&#39;dplyr&#39;) We begin by reading the data again from source. path &lt;- rp.datalink(&#39;covid19_deaths_scotland&#39;) covid_deaths &lt;- read.csv(path) The earlier operations can be performed by employing the filter function, rather than subset, and the mutate function to amend variables or create new ones. There is a small advantage here because dplyr functions operate on dataframes so the mutate operations know where to find existing variables and create new ones. library(dplyr) covid_deaths &lt;- filter(covid_deaths, FeatureCode == &quot;S92000003&quot; &amp; CauseOfDeath == &quot;COVID-19 related&quot; &amp; LocationOfDeath == &quot;All&quot; &amp; !(DateCode %in% c(&quot;2020&quot;, &quot;2021&quot;))) covid_deaths &lt;- mutate(covid_deaths, DateCode = sub(&quot;w/c &quot;, &quot;&quot;, DateCode)) covid_deaths &lt;- mutate(covid_deaths, Date = as.Date(DateCode)) Another option is to make the code more streamlined and readable by using the pipe operator %&gt;%. (This is provided by the magrittr package, which will be loaded by dplyr.) The results of the instruction preceding the %&gt;% symbol are passed as the first argument of the instruction which follows. The filter and mutate functions expect a dataframe as their first argument and what the receive is the result of the previous operation. This allows multiple instructions to be combined in a natural order, without the need to use repeated &lt;- assignments. covid_deaths &lt;- read.csv(path) %&gt;% filter(FeatureCode == &quot;S92000003&quot; &amp; CauseOfDeath == &quot;COVID-19 related&quot; &amp; LocationOfDeath == &quot;All&quot; &amp; !(DateCode %in% c(&quot;2020&quot;, &quot;2021&quot;))) %&gt;% mutate(DateCode = sub(&quot;w/c &quot;, &quot;&quot;, DateCode)) %&gt;% mutate(Date = as.Date(DateCode)) The advantages of dplyr in this example are modest but the benefits of these tools become apparent as the complexities of data wrangling operations increase. Some further examples are given in the following section. A very useful resource is the summary sheet available from the RStudio menu Help &gt; Cheat Sheets &gt; Data Transformation with dplyr and on the web in html and pdf forms. This summarises the wide range of operations which dplyr supports. "],["collating-data-from-different-sources.html", "3.3 Collating data from different sources", " 3.3 Collating data from different sources When the data we need are spread across different sources, collation into a single dataset can be rather messy. The tools of the dplyr package are particularly helpful here. This section also makes use of the readxl package to read Excel spreadsheets. Example: Giving in the Church of England The Church of England is a large organisation whose 12,500 parishes, organised into 42 regions called dioceses, cover the whole of England. This gives a good context within which to explore patterns in charitable giving and Pickering (1985) examined this using data from the 1980’s. Prosperity (expressed in employment rate) and attachment (the proportion of the population associated with the church) were identified as important covariates associated with the variations in giving per church member across the country. Interestingly, giving per church member went down as attachment increased. Is this relationship still evident? The Data Services unit publishes a large amount of information about the composition and activity of the church so this provides an opportunity to examine whether these effects remain. However, the information required is spread across multiple files covering attendance, deprivation and giving. You may like to follow these links to look at the form and content of these spreadsheets. The first task is to collate the information into a single dataset containing the appropriate data. We will focus on 2019 as this is pre-pandemic. The English Index of Multiple Deprivation (IMD) will be used as a measure of regional prosperity. It is appropriate to exclude Europe, the Channel Islands and the Isle of Man, for which some information is not available. These are also regions with unusual characteristics. library(readxl) path.attend &lt;- rp.datalink(&#39;cofe_attendance_2019&#39;) path.giving &lt;- rp.datalink(&#39;cofe_giving_2019&#39;) path.depriv &lt;- rp.datalink(&#39;cofe_deprivation_2019&#39;) d.elect &lt;- read_excel(path.attend, sheet = 5, range = &#39;B4:I47&#39;) %&gt;% select(Diocese = 1, Elect = 3, Worship = 6) The instructions above make use of the read_excel function to read an Excel spreadsheet. This includes helpful arguments to specify which sheet and which block of cells we would like to read from. The select function from the dplyr package then allows us to select out three columns of interest while simultaneously renaming them. The pipe operator discussed in the previous section allows these operation to be combined neatly. Notice that dplyr functions create tibbles. These are dataframes with some extra features. We can apply this idea when reading the spreadsheets about attendance, giving and deprivation. The deprivation information is actually supplied at a finer level (parish) so the group_by and summarise functions allow us to aggregate to the level (diocese) which matches the information from the other spreadsheets. d.attend &lt;- read_excel(path.attend, sheet = 6, range = &#39;B4:D47&#39;) %&gt;% select(Diocese = 1, Attend = 3) d.giving &lt;- read_excel(path.giving, sheet = 3, range = &#39;B8:BS49&#39;) %&gt;% select(Diocese = 1, Giving = 19, Givers = 59) d.depriv &lt;- read_excel(path.depriv, sheet = 2, range = &#39;A1:AK12408&#39;) %&gt;% select(Diocese = 11, population = 13, IMD = 37) %&gt;% group_by(Diocese) %&gt;% summarise(IMD = sum(IMD * population, na.rm = TRUE) / sum(population, na.rm = TRUE), population = sum(population, na.rm = TRUE)) Before joining these different sets of information togethjer, it would be wise to check that there are no inconsistencies in the way the information has been coded. The anti_join function is very useful for this. Here is identifies some small differences in the names of the dioceses. anti_join(d.elect, d.attend, by = &#39;Diocese&#39;) ## # A tibble: 0 × 3 ## # ℹ 3 variables: Diocese &lt;chr&gt;, Elect &lt;dbl&gt;, Worship &lt;dbl&gt; anti_join(d.giving, d.attend, by = &#39;Diocese&#39;) ## # A tibble: 3 × 3 ## Diocese Giving Givers ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 St.Albans 12579567 15283 ## 2 St.Edmundsbury &amp; Ipswich 4614832 8542 ## 3 Sodor and Man 447514 623 anti_join(d.depriv, d.attend, by = &#39;Diocese&#39;) ## # A tibble: 2 × 3 ## Diocese IMD population ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 St.Albans 14.7 1927579. ## 2 St.Edmundsbury &amp; Ipswich 17.1 679678. That can be fixed by using the mutate and recode functions. d.attend &lt;- mutate(d.attend, Diocese = recode(Diocese, &#39;St. Albans&#39; = &#39;St.Albans&#39;, &#39;St. Edms &amp; Ipswich&#39; = &#39;St.Edmundsbury &amp; Ipswich&#39;)) d.elect &lt;- mutate(d.elect, Diocese = recode(Diocese, &#39;St. Albans&#39; = &#39;St.Albans&#39;, &#39;St. Edms &amp; Ipswich&#39; = &#39;St.Edmundsbury &amp; Ipswich&#39;)) d.giving &lt;- mutate(d.giving, Diocese = recode(Diocese, &#39;Sodor and Man&#39; = &#39;Sodor &amp; Man&#39;)) Now we are in a position to join everything together using the full_join function, with a further adjustment to express attachment on a proportional scale and giving on a ‘per member’ scale. cofe_2019 &lt;- d.attend %&gt;% full_join(d.elect, by = &#39;Diocese&#39;) %&gt;% full_join(d.depriv, by = &#39;Diocese&#39;) %&gt;% full_join(d.giving, by = &#39;Diocese&#39;) %&gt;% mutate(Attachment = Attend / population, Giving_per_member = Giving / Elect) Now that the data have been collated into a single dataframe, we are ready to produce some plots which will address the question we began with. There is a lot of variation here. We will revisit this example in Chapter 10 when we discuss the topic of linear models. plot(Giving_per_member ~ Attachment, data = cofe_2019) plot(Giving_per_member ~ IMD, data = cofe_2019) This example further illustrates the power of the tools provided by the dplyr package. These manipulations could be performed through standard R functions but the code is likely to be much longer and more complex. References Pickering, JF. 1985. “Giving in the Church of England: An Econometric Analysis.” Applied Economics 17 (4): 619–32. "],["tidy-data.html", "3.4 Tidy data", " 3.4 Tidy data The idea of tidy data is that we have a dataframe in row/column format where each row corresponds to a ‘case’ and each column corresponds to a unique ‘variable’. Many functions expect this format but datasets are not always organised in this way. For example, sometimes we have data where the same variable is measured at different times. The marriage data introduced in Section 2.4.3 is a case in point. (The code to read this dataset is repeated here for convenience.) Each row contains the values for the cohort born in a particular year. library(readxl) path &lt;- rp.datalink(&#39;married_men&#39;) married_men &lt;- read_excel(path, sheet = &quot;Table 2&quot;, skip = 9, n_max = 96, col_names = c(&quot;Year&quot;, as.character(17:50))) head(married_men) ## # A tibble: 6 × 35 ## Year `17` `18` `19` `20` `21` `22` `23` `24` `25` `26` `27` `28` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1900 0 1 8 24 61 135 214 302 396 483 558 627 ## 2 1901 0 2 8 25 57 119 191 276 365 446 527 596 ## 3 1902 0 1 6 19 44 101 169 248 330 419 500 573 ## 4 1903 0 1 5 16 38 92 158 231 319 404 488 562 ## 5 1904 0 1 4 14 36 89 151 232 319 409 494 568 ## 6 1905 0 1 3 13 34 82 146 223 311 401 488 561 ## # ℹ 22 more variables: `29` &lt;dbl&gt;, `30` &lt;dbl&gt;, `31` &lt;dbl&gt;, `32` &lt;dbl&gt;, ## # `33` &lt;dbl&gt;, `34` &lt;dbl&gt;, `35` &lt;dbl&gt;, `36` &lt;dbl&gt;, `37` &lt;dbl&gt;, `38` &lt;dbl&gt;, ## # `39` &lt;dbl&gt;, `40` &lt;dbl&gt;, `41` &lt;dbl&gt;, `42` &lt;dbl&gt;, `43` &lt;dbl&gt;, `44` &lt;dbl&gt;, ## # `45` &lt;dbl&gt;, `46` &lt;dbl&gt;, `47` &lt;dbl&gt;, `48` &lt;dbl&gt;, `49` &lt;dbl&gt;, `50` &lt;dbl&gt; This was a convenient structure for the three-dimensional surface function persp3d used earlier but many other functions expect the values of interest to be in a single variable, with other variables indexing other relevant information, such as Year and Age in this case. It is perfectly possible to use basic R instructions to restructure the data, but the very detailed manipulations required can be messy, increasing the possibility of coding errors. The tidyr package is designed to help with this by providing high-level functions to restructure datasets and take care of the detailed manipulations required. For example, we can use the pivot_longer function to change the format of the marriage data in a single instruction. The cols argument identifies where the values of interest are located. There is some flexibility in how those are defined with, for example, !Year providing an alternative expression as ‘all the columns apart from Year’. The values_to argument gives the name of the long column which will be created for the data in the specified cols. The names_to argument provides a name for the new column which will be created from the existing column names to index the new long column of values. library(tidyr) married_men_tidy &lt;- pivot_longer(married_men, cols = 2:35, values_to = &quot;Share&quot;, names_to = &quot;Age&quot;) head(married_men_tidy) ## # A tibble: 6 × 3 ## Year Age Share ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1900 17 0 ## 2 1900 18 1 ## 3 1900 19 8 ## 4 1900 20 24 ## 5 1900 21 61 ## 6 1900 22 135 You are invited use the data in this form in an exercise in the Data Visualisation chapter. The concept of ‘tidy data’ gives its name to a whole ecosystem of packages known as the tidyverse. This includes the dplyr and tidyr packages referred to in this chapter. It also includes the ggplot2 package which forms the basis of the data visualisation methods discussed in Chapter 4. It is often convenient to load the entire ecosystem through the instruction library(tyidyverse). "],["further-reading-2.html", "3.5 Further reading", " 3.5 Further reading A key reference on data wrangling is the R for Data Science book (Wickham, Çetinkaya-Rundel, and Grolemund 2023). This is also available in web form. References Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\". "],["exercises-2.html", "3.6 Exercises", " 3.6 Exercises 3.6.1 UN demography data The United Nations World Population Prospects web page provides a great deal of demographic information on countries in the world. This exercise involves downloading a file from the UN website and pulling out information on life expectancy and fertility. A second stage involves collation with some further information in a second file. The first dataset is in an Excel spreadsheet whose location can be identified by rp.datalink('UN_demography'). The initial tasks are: use the read_excel function from the readxl package to read the data; filter the data to retain only cases where the Type variable takes the value ‘Country/Area’; select only the variables LocID, Location, Time, TPopulation1Jan, TFR and LEx, renaming the last four variables as Year, Population, Fertility and Life_Expectancy respectively; divide the Population variable by 1000. The filter, select and mutate functions from the dplyr package will be useful in doing this. With the second dataset, use the read_excel function from the readxl package to read the Excel spreadsheet labelled “Overall”, skipping the first 16 lines as these do not contain data; select only the variables LocID and GeoregName, renaming the second variable as Continent respectively; use the distinct function from the dplyr package to remove any duplicate rows; shorten some of the Continent names to allow neater visualisations. This last step requires the case_match function from the dplyr package. As this is its first appearance, the instruction is provided below using dfrm as the name of the dataframe. mutate(dfrm, Continent = case_match(Continent, &quot;Latin America and the Caribbean&quot; ~ &quot;LA and Car.&quot;, &quot;Northern America&quot; ~ &quot;N. America&quot;, .default = Continent)) Now use the left_join function from the dplyr package to collate the two datasets together, using the LocID variable to match the cases. (Remember to use the anti_join function first, to ensure that there are no mismatches in the coding of LocID in the two datasets.) "],["data-visualisation.html", "4 Data Visualisation", " 4 Data Visualisation Graphs and summaries provide some of the most powerful tools available for dealing with data. A well chosen graph or summary can sometimes allow us to reach conclusions immediately, without the need for further, more complex analysis. When careful anaysis is required, graphs can assist in many other ways, for example by providing initial insight, checking assumptions and reporting results. "],["thousand-words.html", "4.1 Graphics: a picture is worth a thousand words", " 4.1 Graphics: a picture is worth a thousand words You may recognise the image displayed below. On this occasion the R code has not been displayed as the graphs are the basis of an exercise at the end of this chapter. The data are available at https://climate.nasa.gov/vital-signs/global-temperature/. Here is a historical example from the early development of graphics. Florence Nightingale used graphics to excellent effect in portraying the different causes of mortality in the Crimean War. This had a significant effect on the subsequent management of disease. There is plenty of scope for the imaginative construction of graphics. In the example below, geographer James Cheshire and designer Oliver Uberti have created a graphic which shows the number of migrants that acquired British nationality after spending a little while in the UK. Good use of colour, size, and the visual connection with the stamps used in passports combine to make the information readily accessible in an intuitive manner. This form of infographics can be very powerful, although there is sometimes a danger that the beauty of the graphic takes precedence over communication of the content. The focus here will be on what we might call statistical graphics - plots with standard forms of construction which aim to communicate as clearly and effectively as possible some aspects of what the data can tell us. There is particularly strong motivation for the good use of graphs. A picture is worth a thousand words. Most people are able to absorb information and identify pattern much more easily from a picture than from lists of numbers. Graphs are a good way to check data for unusual or problematic observations. If we go on to carry out a more formal analysis of the data, it is very useful to have some impression of what the answer is likely to be, as a check on our calculations. A graph can often communicate this. Graphs can often suggest the kind of model which will be appropriate for the data. They help us to think through the problem. Particular types of graph have been developed to allow us to check the assumptions which a model makes. A well chosen graph can also be the best way to communicate our results and conclusions. A map is a very sophisticated graphical device. We have simply become used to seeing and interpreting them. The code below uses the leaflet package to create an interactive map. (You will also need to issue the library(tidyverse) instruction, if you have not already done that in the current session, in order to use the ‘pipes’.) There are multiple layers of information here, with points, symbols, text, lines, filled areas, colours and other features. That helpfully prepares the ground for the system to be introduced in the next section. library(leaflet) leaflet() %&gt;% addTiles() %&gt;% addMarkers(lng = -4.29, lat = 55.872, popup = &quot;The University of Glasgow&quot;) "],["ggplot-graphics.html", "4.2 ggplot graphics", " 4.2 ggplot graphics The native graphics functions in R are very useful for a variety of tasks. However, there is also a set of more general, powerful and systematic graphical tools known as the ggplot system, available in the ggplot2 package. This uses some attractive graphic design principles to create a style of plots which have become extremely popular. It also provides a framework and set of tools which allow complex plots to be built. Near the end of the previous section a map was displayed and this provides a useful analogy. Our familiarity with maps masks the fact that they encode a large amount of information in a sophisticated manner. There are different symbols (points, text, other symbols), lines, filled areas, colours and shading, with informative keys and legends. These are built up in layers to create the final product. ggplot operates in a somewhat similar manner, constructing the final plot in successive layers. 4.2.1 The basics Hans Rosling became famous for the talks he gave which challenged our views of the world, using publicly available data on health, society and the environment. You might like to view his highly entertaining TED talk. In honour of his highly influential work, the next illustration will reproduce the example given in this talk, but using updated data. An exercise at the end of Chapter 3 involves the collation of data from various UN websites into a demog dataframe. In case you haven’t tried this exercise yet, this can be done for us conveniently through the rp.wrangle function from the rpanel package. To keep things simple at the moment, we will filter out the dat for the year 1950, using the filter function from the dplyr package discussed in Chapter 3. This package and the ggplot2 package we are about to use can be made available through the single library(tidyverse) instruction. demog &lt;- rp.wrangle(&quot;UN_demography&quot;) library(tidyverse) demog50 &lt;- filter(demog, Year == 1950) The starting point is a call to the ggplot function. This specifies which dataframe we are using and the aes function within it sets the global aesthetics for the plot by specifying the variables to be used on the x and y axes. The left hand panel below shows the effect of this. The axes and labels are drawn but nothing is yet entered in the plot. In the second expression below, the geom_point function is added. This specifies the geometry of the plot, in this case asking for points to be plotted. This is a very simple example of the manner in which the syntax of ggplot allows us to add multiple layers to build up the plot as we would like it. ggplot(demog50, aes(Fertility, Life_Expectancy)) ggplot(demog50, aes(Fertility, Life_Expectancy)) + geom_point() We can usefully add the country names by extending the aesthetics to include a label and changing the geometry of the plot to text. Further settings of the aesthetics allow us to use colours to identify the continents and the size of the text to indicate the population. Notice that the legends for colour and size are added automatically. ggplot(demog50, aes(Fertility, Life_Expectancy, label = Country, col = Continent, size = Population)) + geom_text() The names of small countries are rather difficult to read. Perhaps it may be better to revert to points but to use the geom_text_repel function from the ggrepel package to add names where there is space. library(ggrepel) ggplot(demog50, aes(Fertility, Life_Expectancy, label = Country, col = Continent, size = Population)) + geom_point() + geom_text_repel(size = 3) So, in ggplot we can set up the basic structure of the graph in aesthetics defined by the ggplot function and then a whole variety of different layers can be built on top of that. Complex plots can be created by adding multiple layers which give huge flexibility in the plot produced. 4.2.2 Facets The challenge in constructing a good visualisation is to present as much information as is needed in a manner which is as clear as possible. What other devices might we use? The demography data covers many years. it would be interesting to compare life expectancy and gdp in 1950 and 2020, seventy years apart. Indeed it was Hans Rosling’s aim to challenge our understanding of how things have changed over time. One way to do this would be to create a new dataframe containing these years only. That is done below through the filter function from the dplyr package. Note the use of the | symbol representing the or logical operator. To compare the two years, we can use facets. This creates multiple (in this case two) plots with the same axes, with a different group of data in each facet. The common axes allow the different groups to be compared quite effectively. demog_compare &lt;- filter(demog, Year == 1950 | Year == 2020) ggplot(demog_compare, aes(Fertility, Life_Expectancy, label = Country, col = Continent, size = Population)) + geom_point() + geom_text_repel(size = 3) + facet_wrap(~ Year) 4.2.3 Going deeper The aim of this section has been to provide a gentle introduction to the ggplot2 package and give a broad overview of its approach. There are many other facilities available and, as the popularity of this approach to graphics has rapidly increased, numerous additional packages to extend the types of visualisations which can be produced. No attempt to give systematic coverage of the facilities and tools available will be attempted here. Examples of other facilities will be introduced as required in later examples. Graphics are better integrated with other parts of the analysis process. A very useful resource is the cheat sheet available from the RStudio menu Help &gt; Cheat Sheets &gt; Data Visualization with ggplot2 and on the web in html and pdf forms. This summarises the basic form of ggplot constructions, common aesthetic values, the geometries available for different data structures, the statistical functions available to restructure data and the numerous tools for controlling scales, co-ordinate systems and many detailed aspects of the visualisation. Colour choice is a particularly interesting issue and Zeileis, Hornik, and Murrell (2009) give a very interesting discussion of this. A significant advantage of ggplot is that the system generally makes very good default choices which represent good practice, while allowing many other options. Wickham (2009) provides definitive description of ggplot2 A third edition of the book is under development and the authors have generously made this available in web form. There are many other texts on data visualisation, many of them making use of ggplot2, and some of these are mentioned in the Further Reading section below. References Wickham, Hadley. 2009. Ggplot2: Elegant Graphics for Data Analysis. New York: Springer-Verlag. https://r-pkgs.org. Zeileis, Achim, Kurt Hornik, and Paul Murrell. 2009. “Escaping RGBland: Selecting Colors for Statistical Graphics.” Computational Statistics &amp; Data Analysis 53 (9): 3259–70. "],["what-are-we-looking-for.html", "4.3 What are we looking for?", " 4.3 What are we looking for? In the Introduction to this book, the importance of keeping a clear focus on the problem we are tackling was emphasised. This remains true when we explore the data graphically. We should be guided by the questions we are asking. However, visualisation can also be helpful in basic data screening and sanity checks. This section will illustrate both of these approaches. It will also briefly cover some of the standard methods of visualisations for simple forms of data. The focus will be on simple simple but commonly occurring problems: what is the pattern of variation in single groups of data?; how do different groups compare?; what is the relationship between different measurements? At the same time, we need to be aware of the different scales of measurement with which we may be dealing. These include: continuous scales where, in principle, any value can be observed along a continuous numerical interval (although rounding to a practical level of accuracy may influence the recorded value); discrete scales where the values correspond to integers, usually describing a count; categorical scales where the recorded value simply indicates the group, out of a finite number of groups, into which the observation falls, for example a blood group; categorical scales are ordinal when the groups have a natural and meaningful ordering and nominal when they do not. We will begin with the description of an interesting historical dataset which will be used to illustrate the issues and methods. Example: Longevity and paternal age at birth The biological process of ageing is the focus of a great deal of current research. In fact, this topic has a very long history and one of the earliest studies came from a surprising source. Alexander Graham Bell was a Scottish-born scientist and inventor who moved to North America where he played a crucial role in the development of the telephone and became a co-founder of the American Telephone and Telegraph Company (AT&amp;T). His scientific interests were much wider than this and included some early work on heredity. As part of this, he organised meticulous records on the descendants of one of the early European settlers in the ‘New World’. This was William Hyde, who died in 1681 and whose descendants are documented in the “Genealogy of the Hyde Family,” by Reuben H. Walworth, LL. D. (1864). A. G. Bell arranged for relevant dates to be extracted from which, among other things, the age of each father at a child’s birth and the subsequent age at death of the child could be calculated. He reported his findings on the dataset in general, and on the relationship between father’s age and child’s longevity in particular, in Bell (1918). The report on the analysis by Bell gives summary data and interpretation, with the raw data listed in Bell’s journal, the Beinn Bhreagh Recorder. This is held by the US Library of Congress who kindly gave access for the data to be transcribed. The dataset is provided in the Hyde dataframe in the rpanel package. The datafile contains three columns. The first two give the age of the father and the age of the child. To provide a compact data representation, the third column gives the frequency of this particular age combination. Standard plotting functions generally expect each row to refer to an individual. We can easily expand the dataframe into this form, using the rep function. The code below creates an indicator variables ind which repeats each row number, as dictated by its frequency. The dataframe is then overwritten by its expanded form. head(Hyde) ## Father Child Frequency ## 1 17 10 1 ## 2 20 14 1 ## 3 21 0 2 ## 4 21 1 1 ## 5 21 4 1 ## 6 21 6 1 ind &lt;- rep(1:nrow(Hyde), Hyde$Frequency) Hyde &lt;- Hyde[ind, 1:2] head(Hyde) ## Father Child ## 1 17 10 ## 2 20 14 ## 3 21 0 ## 3.1 21 0 ## 4 21 1 ## 5 21 4 4.3.1 Continuous scales The key question which motivated the collection of the data is whether the age of the father is linked, on average, to the age of the child. Before exploring that question, it might be useful to inspect the pattern of ages for fathers and children separately. Are these ages on a continuous or a discrete scale? The ages of the fathers are mostly between 20 and 60 while the ages of the children are mostly between 0 and 100. Although the ages are rounded to a whole number of years, the resolution on the axes is fairly high, with a large number of distinct, evenly-spaced values. It would therefore be reasonable to treat this as a continuous scale. Measurements on any continuous scale can only be recorded to some specific degree of accuracy. For data on a continuous scale, a histogram, where we display the frequency of data in grouped form, is an obvious method of data visualisation. The two upper panels in the figure below show the observations along the years scale with some random jittering and the use of transparency to reduce the problem of overplotting and allow individual observations to be seen. Construction of a histogram involves splitting the axis into intervals or bins, as illustrated by the grid of blue lines, and recording the frequency of observations in each bin. The number of intervals can be altered to allow us to view the underlying pattern in the distribution at different scales. A histogram can be produced by instructions of the form ggplot(Hyde, aes(Father)) + geom_histogram() although the panels below have been further annotated to illustrate the method of construction. A striking feature of these histograms is the very high level of childhood mortality. The bottom row of the figure above shows another common form of display for samples of data on a continuous scale, namely the boxplot. This is based on a summary of each set of data. The median of a sample is the value which has half of the observations below and half of the observations above. (If the number of data points in the sample is odd this will be the middle observation. If the number of data points is even, it is defined as the average of the middle two observations.) Reapplying this thinking to the lower and upper half of the dataset produces the lower and upper quartiles as the middle points of each half. The median and quartiles give a useful summary of the location and spread (distance of the quartiles from the median) of the sample of data. Two further summary numbers are usually added. These could be the extremes (largest and smallest observations) or other values indicating where the ‘tails’ of the sample lie. (There are various algorithms for this latter approach but the details are not crucial here. This allows a small number of unusually large or small data points to be identified separately.) This summary is turned into graphical form through a box whose edges lie at the quartiles, with a central line marking the median and a horizontal line marking the ‘extent’ of the data points. Boxplots can be produced by instructions of the form ggplot(Hyde, aes(Father)) + geom_boxplot() In the panels above the layer scale_y_discrete() has been added to remove the vertical scale which, for a single group of data is not relevant. Useful as this is, a summary based on such a small number of sample characteristics cannot capture much detail. In this case the skewness of the distribution of father’s age is expressed through the median being positioned closer to the lower quartile than the upper quartile, but the clustering of observations due to child mortality is completely missed. It is the relationship between father’s age and child’s longevity which is of interest, so a scatterplot of these two variables in the left hand plot below gives an initial impression. At any father’s age, there is huge variation in the child’s longevity so there is little indication of any underlying relationship. However, one unusual feature is apparent, with a vertical gap at father’s age 36 in an otherwise dense plot. Review of the transcription of the data brought to light that a group of observations has been mistakenly tagged with father’s age 35 when this should have been 36. Correction of this transcription error produces the right hand scatterplot, where the anomalous gap has disappeared. This is a simple example of another of the useful roles of data visualisation, namely data screening. ggplot(Hyde, aes(Father, Child)) + geom_point() The high rate of child mortality is again apparent, now in the cluster of observations near the horizontal axis. In addition to the normal process of adult ageing there is a heightened challenge, relative to modern standards, to reach maturity. It may be helpful to separate these two effects by examining any relationship between father’s age and: the proportion of children who survive childhood; longevity for those who survive childhood; age of death for those who do not survive childhood. The definition of ‘surviving childhood’ needs some thought. For current exploratory purposes, a suitable criterion would be to reach age 10 as this broadly corresponds to a ‘thinning’ of the data at this point on the vertical scale. In order to explore whether there is evidence of a link between father’s age and child’s longevity, Alexander Graham Bell reported the mean child longevity for different groups of father’s age. We will do the same, but apply this to to those who survived childhood rather than to the whole dataset. We begin by adding Father_age_group to the Hyde dataset, filtering out the cases where childhood was survived. The fact that the dplyr package works with the enhanced form of dataframe called a tibble now helps, as this allows the group_by function to summarise child longevity by sample mean and sample standard deviation across the father’s age groups. The mean of the father’s ages in each group is also calculated. The mean of a group of observations can be thought of as its ‘centre of gravity’ if each observation corresponds to a weight resting at that point on the (weightless) axis. It therefore summarises location of the sample. Mean values are important quantities which will often be of interest in later analyses, so it will be helpful to give a precise definition in terms of a formula. If \\(n\\) observations is a sample are denoted by \\(x_1, x_2, \\ldots, x_n\\), the the sample mean, denoted as \\(\\bar{x}\\), is defined as \\[ \\bar{x} = \\left(\\sum_{i=1}^n x_i \\right) / n = (x_1 + x_2 + \\ldots + x_n) / n , \\] where the \\(\\sum\\) symbol is a neat way of showing that we add oall the terms together. The sample standard deviation measures how far the observations are from the sample mean, on average. Use of the squared distance \\((x_i - \\bar{x})^2\\) for each observation allows us to focus on distance without worrying about direction. So the sample standard deviation is defined as \\[ s = \\sqrt{\\left( \\sum_{i=1}^n (x_i - \\bar{x})^2 \\right) / (n - 1)} \\ \\ . \\] The use of the square-root helpfully takes us back to the original scale of measurement. (Notice that we average by dividing by \\(n - 1\\) rather than \\(n\\) as technical arguments show this to be more effective.) This summary of the spread of the sample has a helpful interpretation. If we start at the sample mean and travel one standard deviation away in either direction then, informally speaking, this takes us halfway into the spread of the sample. So travelling two standard deviations from the sample mean generally covers most of the observations. This rule of thumb doesn’t work well if the sample has strong skewness but it is a useful guide. It will be explored further in (samples and populations section). Hyde &lt;- mutate(Hyde, Father_age_group = cut(Father, breaks = c(0, 25, 35, 45, 100))) Hyde_10 &lt;- filter(Hyde, Child &gt;= 10) Hyde_10_means &lt;- Hyde_10 %&gt;% group_by(Father_age_group) %&gt;% summarise(mean = mean(Child), sd = sd(Child), age = mean(Father)) Hyde_10_means ## # A tibble: 4 × 4 ## Father_age_group mean sd age ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (0,25] 51.1 22.0 23.6 ## 2 (25,35] 48.2 22.3 30.5 ## 3 (35,45] 46.8 22.0 39.8 ## 4 (45,100] 46.2 22.8 49.7 We can now look for a trend in child longevity against father’s age by adding sample means to the scatterplot, for those cases where the child survived at least until age 10. This shows a small but steady decline in the mean child longevity as father’s age increases. This simple but effective form of insight, employed by Alexander Graham Bell on the full dataset, is regarded as the first indication of a link between the age of the father and the longevity of the child. The effect is small, so this is about understanding the underlying connection between these variables. It is certainly not feasible to predict the longevity of individual children from the father’s age! At this stage, we are still dealing with descriptions of the data. We will have to defer the business of weighing evidence for change more carefully until we have the inferential tools introduced in Chapter 6. # Notice that the second instance of geom_point replaces the default `data` # with the Hyde_10_means dataset and the appropriate `aes`. filter(Hyde, Child &gt;= 10) %&gt;% ggplot(aes(Father, Child)) + geom_point() + geom_point(aes(age, mean), data = Hyde_10_means, size = 5, col = &quot;red&quot;) + geom_vline(xintercept = c(25, 35, 45), col = &quot;red&quot;, linetype = 2) We will revisit this dataset in later chapters but, for the moment, we can complete the informal investigation of trend by using medians rather than means. In fact, the simplest way to do this is to display boxplots. This tells a similar story. filter(Hyde, Child &gt;= 10) %&gt;% ggplot(aes(Father_age_group, Child)) + geom_boxplot() Summary For groups of data on a single variable on a continuous scale, the histogram is a very useful method of visualisation. Features to look out for include: unimodal or multimodal shape; symmetry or skewness; outliers (unusual observations); rounding (where observations cluster at particular values, possibly indicating a simplification in the recorded values; we did not see this in the Hyde data.). Boxplots are based on summaries which can be helpful when comparing location, scale and skewness across multiple groups but features such as multimodality are hidden. Dotcharts and stripcharts (not discussed here) are useful displays when the samples sizes are small. Helpful numerical summaries of samples of measurements on a continuous scale are: sample means and standard deviations; sample medians and quartiles (and indeed other percentiles). The presence of a relationship between two measurements on continuous scales can be investigated by exploring whether the mean or median of one variable changes with movement in the other variable. This can also be quantified through the concept of correlation which is discussed in 4.4 below. 4.3.2 Discrete scales Ages are on a continuous scale although they are recorded to some level of accuracy. When we look at infant mortality this is less reasonable. We should view the data on a discrete scale. A barplot is then the natural display. Mean values make sense because a discrete scale is numerically meaningful. There doesn’t seem to be an interpretable systematic shift with Father’s age. Hyde_10 &lt;- filter(Hyde, Child &lt; 10) Hyde_means_10 &lt;- Hyde_10 %&gt;% group_by(Father_age_group) %&gt;% summarise(mean = mean(Child)) ggplot(Hyde_10, aes(Child)) + geom_bar() + facet_wrap(~ Father_age_group) + geom_vline(aes(xintercept = mean), Hyde_means_10, col = &quot;red&quot;) + scale_x_continuous(&quot;Infant age&quot;, breaks = 0:9) # The &#39;scale_x_continuous&#39; function allows the axis ticks and label to be specified. Perhaps it would be more effective to consider simply whether each child survivied childhood by living to double figures. As we are dealing with counts and the number of categories is small for both survival and father’s age group, an interesting alternative display is a mosaicplot. This uses blocks whose areas are proportional to the count in each cell of the cross-classification. Here the width of each column of blocks represents the differences in the numbers of observations in the Father’s age categories, while the positions of the breaks between the vertical blocks correspond to the proportions surviving childhood. This is a very neat way of representing the information in the table in an interpretable manner. Hyde &lt;- mutate(Hyde, Survived = Child &gt;= 10) tbl &lt;- table(Father = Hyde$Father_age_group, Hyde$Survived) mosaicplot(tbl, main = &quot;&quot;) 4.3.3 Categorical data When we have more than two factors or grouping variables a contingency table is a useful way of presenting the data. The table of father and child ages shown above, for children who died before 10 years, is an example. As a further example, we can consider whether the incidence of infant mortality, irrespective of particular age of death, changes with paternal age. This is the missing part of our analysis, which so far has only considered the patterns within the infant and adult death groups. The mosaicplot below suggests that the proportion of infant deaths may be rising slightly with father’s age. Hyde &lt;- mutate(Hyde, infant_mortality = factor(Child &lt; 10)) tbl &lt;- table(&quot;Father&#39;s age&quot; = Hyde$Father_age_group, &quot;Infant mortality&quot; = Hyde$infant_mortality) mosaicplot(tbl, main = &quot;&quot;) The ggmosaic package allows mosaicplots to be produced within the ggplot2 framework. References Bell, Alexander Graham. 1918. The Duration of Life and Conditions Associated with Longevity. Genealogical record office. "],["gaining-an-overview-of-datasets.html", "4.4 Gaining an overview of datasets", " 4.4 Gaining an overview of datasets Before starting serious analysis of any dataset, it is helpful to ‘screen’ the data by checking some basic features. For example, we might check the scales of measurement used, view the broad patterns of variation exhibited by individual variables, look out for unusual observations and so on. This is a kind of ‘sanity check’ which can highlight issues we may need to consider. As an example, we will use data on herring gulls, as discussed in Section 1.1. (If you haven’t yet tried the data collection activity there, do go back and have a go.) Several measurements are available on each gull and the aim is to investigate whether these measurements can be used to classify birds as male or femal, as sex cannot easily be determined by inspection of the bird’s anatomy. The dataset is hidden in the rpanel package, but we can retrieve it through the system.file command. Note the use of the stringsAsFactors argument of read.table which will ensure that any character variables are interpreted as factors. To improve the layout of results, a rather long variable name has also been slightly shortened. path &lt;- system.file(&quot;extdata/gulls.txt&quot;, package = &quot;rpanel&quot;) gulls &lt;- read.table(path, stringsAsFactors = TRUE) %&gt;% rename(Head.Length = Head.and.Bill.Length) How should we inspect this dataset? A first step might be to produce helpful summaries of each individual variable. The function summary is very useful as it adapts its output to the type of object it is passed, in this case a dataframe. summary(gulls) ## Weight Wing.Length Bill.Depth Head.Length Sex ## Min. : 705.0 Min. :395.0 Min. :15.70 Min. :103.0 Female:210 ## 1st Qu.: 860.0 1st Qu.:408.0 1st Qu.:17.50 1st Qu.:115.0 Male :153 ## Median : 920.0 Median :417.0 Median :18.20 Median :119.0 ## Mean : 948.1 Mean :418.6 Mean :18.41 Mean :119.6 ## 3rd Qu.:1042.5 3rd Qu.:429.0 3rd Qu.:19.30 3rd Qu.:125.0 ## Max. :1250.0 Max. :457.0 Max. :22.50 Max. :136.0 This provides a useful description of each variable, with quantiles for those on continuous scales and tabulations for factors. However, even at this stage we should be guided by the aim of the analysis, which in this case is to assess the possibility of identifying the sex of a bird from its measurements. The skimr package provides some flexibility and fits neatly with the tidyverse, so we can insert the dplyr function group_by before it in a pipeline to create separate summaries for males and females. Here we improve the layout by omitting n_missing and complete_rate from the output, as there are no missing data. The small histograms provide a neat graphical summary of the shape of the variability in each variable. library(skimr) gulls %&gt;% group_by(Sex) %&gt;% skim() %&gt;% select(-c(n_missing, complete_rate)) ## ── Data Summary ──────────────────────── ## Values ## Name Piped data ## Number of rows 363 ## Number of columns 5 ## _______________________ ## Column type frequency: ## numeric 4 ## ________________________ ## Group variables Sex ## ## ── Variable type: numeric ────────────────────────────────────────────────────── ## skim_variable Sex mean sd p0 p25 p50 p75 p100 hist ## 1 Weight Female 872. 71.5 705 830 868. 900 1195 ▂▇▂▁▁ ## 2 Weight Male 1053. 83.5 740 1005 1055 1110 1250 ▁▁▇▇▂ ## 3 Wing.Length Female 410. 8.15 395 404 410. 415 435 ▅▇▆▃▁ ## 4 Wing.Length Male 430. 8.68 395 426 430 436 457 ▁▁▇▅▁ ## 5 Bill.Depth Female 17.6 0.701 15.7 17.1 17.6 18.1 19.8 ▁▅▇▃▁ ## 6 Bill.Depth Male 19.5 0.804 17.4 19 19.5 20 22.5 ▂▇▇▁▁ ## 7 Head.Length Female 115. 3.64 103 113 115 118. 128 ▁▃▇▂▁ ## 8 Head.Length Male 126. 3.94 110 124 126 128 136 ▁▂▆▇▂ We are also interested in maximising the use of graphical exploration. Boxplots for each variable, separated by Sex, provide a good overall visual summary. Facets provide a mechanism for this, if the data are first rearranged to place everything in a single column with a further column to identify the variable names. The argument scales = \"free\" allows the scales of each facet to be different. gulls %&gt;% pivot_longer(cols = !Sex, names_to = &quot;variable&quot;) %&gt;% ggplot(aes(value, Sex)) + geom_boxplot() + facet_grid(. ~ variable, scales = &#39;free&#39;) It is also of interest to explore the relationships between these different variables. The plot below shows the relationship between Weight and Head.Length for males. As we would expect, heavier birds tend to have longer heads simply because ‘bigger’ birds tend to have larger measurements across the board. How should we quantify this relationship? To keep the notation general, we will name the measurements simply as \\(x\\) and \\(y\\). We are interested in the extent to which the measurements increase (or decrease) together across the dataset. From this perspective, we are not interested in the location of the scatterplot (quantified by the sample means, \\(\\bar{x}\\) and \\(\\bar{y}\\)) or the scales of measurement (quantified by the sample standard deviations, \\(s_x\\) and \\(s_y\\)) so it is helpful to remove these effects by standardising each variable as \\[ x&#39; = \\frac{x - \\bar{x}}{s_x} \\hspace{3em} \\mbox{and} \\hspace{3em} y&#39; = \\frac{y - \\bar{y}}{s_y} . \\] The right hand plot below confirms that the shape of the relationship is the same on these standardised scales. gullsM &lt;- filter(gulls, Sex == &#39;Male&#39;) ggplot(gullsM, aes(Weight, Head.Length)) + geom_point() gullsM &lt;- mutate(gullsM, `x&#39;` = (Weight - mean(Weight)) / sd(Weight), `y&#39;` = (Head.Length - mean(Head.Length)) / sd(Head.Length)) ggplot(gullsM, aes(`x&#39;`, `y&#39;`)) + geom_point() + geom_vline(xintercept = 0, linetype = &#39;dashed&#39;) + geom_hline(yintercept = 0, linetype = &#39;dashed&#39;) It turns out that a useful way to quantify the extent to which \\(x\\) and \\(y\\) ‘go together’ is to find the average value of \\(x&#39; y&#39;\\). If \\(x&#39;\\) and \\(y&#39;\\) tend to fall into the top-right and bottom-left quadrants then the product will give a strong positive value, while a top-left and botoom-right pattern will produce a negative value and a completely random pattern is likely to produce a value close to \\(0\\). As a result of the standardised scales, it turns out that the maximum and minimum values of the product, on average, are \\(1\\) and \\(-1\\). So, the degree of association between the variables \\(x\\) and \\(y\\) can be measured through the correlation coefficient defined as \\[ \\frac{1}{n-1} \\sum_{i = 1}^n \\frac{(x_i - \\bar{x})}{s_x} \\frac{(y_i - \\bar{y})}{s_y} \\] In this example the sample correlation coefficient is cor(gullsM$Weight, gullsM$Head.Length) = 0.462. Exercises at the end of the chapter invite you to explore the behaviour of the sample correlation coefficient in measuring different strengths of association. A variety of tools are now available to help in gaining an overview of the features of the dataset when there are multiple variables present. The ggpairs function in the GGally package can help with this. It provides a set of plots which incorporate a great deal of information in graphical form. As ever, there are various arguments we can use to control the details. Here colour is used to identify males and females. The stars argument is also used to suppress some information which is not relevant to us at this stage. library(GGally) ggpairs(gulls, aes(col = Sex, alpha = 0.7), upper = list(continuous = wrap(ggally_cor, stars = FALSE))) The idea is to create a set of panels which plot each variable against every other one. The type of plot in each panel adapts to the nature of the variables involved. When both variables are on continuous scales, the relationship is summarised by correlation coefficients in the upper part of the matrix. The boxplots and histograms in the margins indicate that the separation of males and females on each individual variable is strong, which offers encouragement that effective classification may be possible. These methods work well when the number of variables is small or modest. Large numbers of variables could be handled in appropriately chosen groups of variables, or by employing methods of dimension reduction, for example principal components. "],["plots-for-more-complex-data-structures.html", "4.5 Plots for more complex data structures", " 4.5 Plots for more complex data structures The types of plots discussed so far are very useful when data lie on standard scales, in one or two dimensions. The structure of data can be more complex, with the need to use graphical methods which adapt to the underlying characteristics. This section gives some examples. 4.5.1 3D data Plots of the gulls data included scatterplots of pairs of measurements, coloured by Sex, which suggested that classification of males and females from these measurements looks feasible. It is possible to explore this further in three dimensions. The rgl package provides a connection to the OpenGL system for displaying three-dimensional objects. The rp.plot3d function from the rpanel package provides a means of creating a 3d scatterplot. The mouse can be used to rotate and zoom the display. (A plot of this type is also available from the plot3d function in rgl.) Notice that the call to the rp.plot3d function is placed inside a with statement. This is a convenient way of allowing access to the variables inside the gulls dataframe, avoiding the need to use references such as gulls$Weight. gulls &lt;- mutate(gulls, clr = case_match(Sex, &#39;Male&#39; ~ &#39;green&#39;, &#39;Female&#39; ~ &#39;blue&#39;)) with(gulls, rp.plot3d(Weight, Wing.Length, Head.Length, col = clr)) rgl::rglwidget() This shows almost complete separation between males and females in these three measurements, although if you examine the plot carefully you will see three females with large values and one male with very small values. Perhaps these birds have been wrongly labelled - or perhaps they are simply very large/small for their sex. 4.5.2 Circular, spherical and compositional data Spherical data sits on the surface of a sphere. Latitude and longitude positions of locations on the surface of the earth are a simple example. library(sm) lat &lt;- rnorm(50, 10, 15) long &lt;- c(rnorm(25, 300, 15), rnorm(25, 240, 15)) par(mfrow=c(1,2)) sm.sphere(lat, long) sm.sphere(lat, long, sphim=TRUE, kappa=15) par(mfrow=c(1,1)) Compositional data consists of vectors of proportions (or percentages) which sum to 1 (or 100). The special issues in analysing this kind of data are discussed by Aitchison (1982). In the special case of three proportions a plot called a ternary diagram can be created, using the special properties of the equilateral triangle. The ggtern package allows this kind of plot to be created in the style of ggplot. Here the percentages of different chemical compositions in lava samples from Skye are displayed. library(ggtern) data(SkyeLava) ggtern(SkyeLava, aes(F, A, M)) + geom_point() + theme_showarrows() Other packages include compositions and Compositional. See Van den Boogaart and Tolosana-Delgado (2013) for more details of the theory. 4.5.3 Other data structures References Aitchison, John. 1982. “The Statistical Analysis of Compositional Data.” Journal of the Royal Statistical Society: Series B (Methodological) 44 (2): 139–60. Van den Boogaart, K Gerald, and Raimon Tolosana-Delgado. 2013. Analyzing Compositional Data with r. Vol. 122. Springer. "],["going-further.html", "4.6 Going further", " 4.6 Going further Young, Valero-Mora, and Friendly (2011) Michael Friendly and Wainer (2021) M. Friendly (n.d.) Chambers et al. (2018) Gelman, Pasarica, and Dodhia (2002) Gordon and Finch (2015) Wickham book on ggplot2 first/second edition. The third edition is available at https://ggplot2-book.org. Unwin (2018) places the emphasis on graphics for exploration, with discussion of what kind of graphics to draw for different types of data and with numerous examples of the process of exploration with different datasets. New angony Unwin book at: https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fbookdown.org%2Faatcosada%2FGmooG-book2%2F&amp;data=05%7C02%7CAdrian.Bowman%40glasgow.ac.uk%7Cf88542b57a8042bb416e08dcf2be2c74%7C6e725c29763a4f5081f22e254f0133c8%7C1%7C0%7C638652145808818739%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C0%7C%7C%7C&amp;sdata=mmYZVHa6xUKzp4ivdIi%2B3rif0Fh1UI%2BPZQdWpOEr62s%3D&amp;reserved=0 with code at: https://github.com/antonr4/GmooGkodes Rahlf (2019) focusses more on presentation graphics, including topics less commonly discussed such as layout and fonts, with much of the book devoted to detailed examples of the construction of specific graphs for specific datasets. Grant (2018) RSS Data visualisation Guide Edward Tufte book: Beautiful Evidence Exploratory Data Analysis with R: Roger Peng (web) ggplot2 essentials for great data visualisation in R. Kassambarra. https://www.datanovia.com/en/product/ggplot2-essentials-for-great-data-visualization-in-r/ Robert Grant: Data Visualization: charts, maps and interactive graphics https://flowingdata.com/2015/09/23/years-you-have-left-to-live-probably/ https://pudding.cool/2018/10/city_3d/ https://www.babynamewizard.com/voyager#prefix=&amp;sw=both&amp;exact=false https://www.visualisingdata.com https://www. nytimes.com/interactive/2018/us/2018-year-in-graphics.html Articles about graphics by Antony Unwin: https://hdsr.mitpress.mit.edu/pub/zok97i7p R Shiny! References Chambers, John M, William S Cleveland, Beat Kleiner, and Paul A Tukey. 2018. Graphical Methods for Data Analysis. Chapman; Hall/CRC. Friendly, M. n.d. “Visualising Categorical Data; SAS Institute: Carry, NC, USA, 2000.” Google Scholar. Friendly, Michael, and Howard Wainer. 2021. A History of Data Visualization and Graphic Communication. Harvard University Press. Gelman, Andrew, Cristian Pasarica, and Rahul Dodhia. 2002. “Let’s Practice What We Preach: Turning Tables into Graphs.” The American Statistician 56 (2): 121–30. Gordon, Ian, and Sue Finch. 2015. “Statistician Heal Thyself: Have We Lost the Plot?” Journal of Computational and Graphical Statistics 24 (4): 1210–29. Grant, Robert. 2018. Data Visualization: Charts, Maps, and Interactive Graphics. Crc Press. Rahlf, Thomas. 2019. Data Visualisation with r: 111 Examples. Springer Nature. Unwin, Antony. 2018. Graphical Data Analysis with r. Chapman; Hall/CRC. Young, Forrest W, Pedro M Valero-Mora, and Michael Friendly. 2011. Visual Statistics: Seeing Data with Dynamic Interactive Graphics. John Wiley &amp; Sons. "],["exercises-3.html", "4.7 Exercises", " 4.7 Exercises 4.7.1 Rodent mass and speed The rodent data were introduced in Section 2.2. Use ggplot to produce the plots which were created there. To plot the fitted regression lines, you may wish to use a geom_smooth(method = 'lm') layer. A col argument can also be passed to this layer to control the colour of the line. 4.7.2 UK covid-19 deaths The UK data on covid-19 deaths was introduced and plotted in Section 3.1. Use ggplot to create the graph. To join the observations with lines, use geom_line instead of geom_point. (Notice that ggplot automatically point the points in the correct date order.) It would be instructive to see the deaths in different Age categories. A suitable subset of the data is created by changing Age == \"All\" to Age != \"All\". Use the colour aesthetic to separate the age groups. Repeat this with Sex. Experiment with facets to display age and sex groups simultaneously. 4.7.3 Scottish referendum For a plot of the proportion voting no against the unemployment rate, we may decide that it is better simply to plot points rather than text, with the areas of the points adjusted to reflect the numbers of people who voted in the different Council regions. The code use in Section 4.2 will remind you how to do this. What does this plot tell you? 4.7.4 Centenarians in Scotland What are the population age trends in Scotland? Data from the National Records of Scotland are available in the centenarians dataset , which can be access in the usual way through the rp.datalink function. The variables are: Region the administrative region of Scotland Males the total number of males of all ages in 2014 Males90 the number of males over 90 years of age in 2014 Males100 the number of males over 100 years of age in 2014 Females the total number of females of all ages in 2014 Females90 the number of females over 90 years of age in 2014 Females100 the number of females over 100 years of age in 2014 Males90y2004 the number of males over 90 years of age in 2004 Females90y2004 the number of females over 90 years of age in 2004 Explore the data in any way you see fit, in order to highlight recent trends. Here are some questions which might guide you. Is there a consistent pattern in the numbers of males and females over 90 years of age in 2014? Is the ratio of males to females similar across the regions? Repeat this for those over 100 years of age. Repeat this for those over 90 years of age using the data from 2004. 4.7.5 Flour beetles The flour_beetles dataframe, which is provided by the rpanel package, records the number of insects which lived and died when groups were exposed to a variety of toxins at different concentrations. Some background information is available in the help file (?flour_beetles). Use ggplot to show the relationships between the proportions killed and the concentrations, using facets to separate the toxins and colour to identify the replicates. The concentration levels of the toxins are rather different, so this is a case where we would like the x-axis in each facet to fit the range of the data for that facet. Consult the scales argument of facet_wrap to see how you might do this. 4.7.6 Clyde Estuary water quality The Scottish Environment Protection Agency has monitored the water quality of the river Clyde by taking measurements of dissolved oxygen (DO) at a variety of locations on the river (Stations) over a long period (Year, Doy - day of the year). Measurements of Temperature and Salinity were also made. A dataframe which contains these measurements is available in the file indicated below. When this file is loaded, the R object clyde dataframe will be placed into the workspace. path &lt;- rp.datalink(&quot;DO_Clyde&quot;) load(path) You can then click on the object clyde in the RStudio Environment pane to see its contents. Choose a particular station and use the subset or filter functions create a dataframe which contains the data on this station only. To specify a particular value you will need the logical ‘equals’ operator. This is in the form Station == 6. Note the == with two ‘equals’ signs together. For your chosen station, plot DO against the other variables: Year, Doy (day of the year), Temperature, Salinity. Comment on the relationships you see. Try this at another station some distance away. Are the relationships similar? Now go back to the full dataframe. ggplot allows the creation of facets, as discussed in 4.2.2. This allows plots to be separated out by a grouping variable (factor). Plot DO against Doy separately for each Station. Notice that the facets argument is actually a formula. For a single variable such as Station you can create this by inserting the ~ character before Station. 4.7.7 Crabs (multivariate) The crabs data is available in R in the MASS package. The dataframe contains various size measurements of each crab shell, as well as sex and colour form (blue or orange). Look at the help file for information on the variables. There are two questions of interest. Is there evidence from the size measurements alone of a division into two forms? Can we construct a rule to predict the sex of a future crab of unknown colour form? How accurate do we expect the rule to be? (The lda function implements linear discriminant analysis.) Here are a few hints which you might find useful. You may find that size is a dominant factor in the variation present in the data. You may be able to remove this effect by standardising the measurements against one particular measurement, for example by taking ratios. Don’t forget that a transformation of the scale may be helpful. The pairs plot is useful in creating a matrix of all possible apirwise scatterplots. A ggplot variation is also available in the GGally package. (Remember that you may need to install that if it is not already on your system.) If you are familiar with the concept of principal components analysis (PCA), you may wish to use that to explore the data by creating new variables which capture as much of the variation in the data as possible. PCA is available in the function princomp. If you are familiar with the technique known as linear discriminant analysis then you may find the lda function from the MASS package useful. 4.7.8 Topographic data The quakes data, which is available in R, provides the locations in latitude, longitude and depth of seismic events in a region near Fiji. See the help file for details. Plot the data in any way you think fit in order to explore the geographical pattern of the locations of these events. An alternative is to use the rgl package which allows rotatable 3-d plots to be created. (Remember that you may need to install this by install.packages(\"rgl\").) The plot3d function should produce a rotable 3d scatterplot. 4.7.9 Global energy use the Energy Institute, which is described as ‘the chartered professional membership body for people who work across the world of energy’, publishes detailed information on energy use across the world in the form of a Statistical Review. The raw data are available, including in particular an Excel spreadsheet with an overview of World Energy Data since 1965. A summary of primary energy consumption is in Sheet 2. Use the data to provide visualisations of the trends in energy use over the range of years available in these spreadsheets. If you wish to dig deeper, a spreadsheet of Consolidated Data provides information at a more detailed level. "],["trends-and-patterns.html", "5 Trends and patterns", " 5 Trends and patterns It sometimes happens that there are trends patterns within a dataset but that these are obscured by a large degree of variability (noise). With large datasets, trends and patterns are hidden by ‘overplotting’ when every data point is displayed. The methods of this chapter aim to highlight underlying trends and patterns in a flexible manner, without imposing any assumptions on the nature of these trends and patterns apart from ‘smoothness’. "],["density-estimation.html", "5.1 Density estimation", " 5.1 Density estimation In statistical modelling, principal interest often lies in mean values or model parameters. However, there are some situations where the shape of the variation is itself the focus of attention. Here is an example on the development of aircraft technology. Example: The development of aircraft technology How did aircraft technology develop during the 20th century? Jane’s (1978) documents aircraft design for the first three quarters of the century. Saviotti and Bowman (1984) developed methods to track the evolution of these designs, using simple physical characteristics: total engine power (kW); wing span (m); length (m); maximum take-off weight (kg); maximum speed (km/h); range (km). The data are available in the aircraft dataframe in the sm package. The year (Yr) of the first appearance of each design is also provided, along with a broad categoriation (Period`) into three periods (1 - 1914-1935, before the Second World War; 2 - 1936-1955, across the Second World War and its immediate aftermath; 3 - 1956-1984, the post-war period) A brief look at the data suggests that the six measurements on each aircraft should be expressed on the log scale to reduce skewness. The code below uses the across function inside mutate to apply the log function to all the data in the specified columns. The left hand plot below shows a histogram of the Speed variable for Period 3, which corresponds to the years after the second World War. The pattern of variability shown in the histogram suggests that there may be multiple modes, although it is difficult to judge between systematic shape and the inevitable detailed variation. library(sm) library(tidyverse) aircraft_log &lt;- aircraft %&gt;% mutate(across(3:8, log), Period = factor(Period)) aircraft_log_3 &lt;- aircraft_log %&gt;% filter(Period == 3) The right hand plot above shows a histogram of the same data with minor changes in the settings: the intervals used for the histogram ‘bins’ have been specified through brks &lt;- seq(5.2, 8.2, 0.1) and then passed to the histogram geometry by geom_histogram(breaks = brks). The width of the bins is very similar and the positions have simply been moved slightly to align with neat positions on the axis. The effect of this minor change on the detailed shape of the histogram seems surprisingly large. Other additions to this plot help in understanding why. A rugplot has been added at the bottom of the histogram by adding geom_rug(sides = 'b') to the ggplot layers. The gridlines placed over the plot (code not shown) highlight the fact that each observation has an associated ‘box’ placed over the interval in which the observation lies. As the placing of these intervals (or ‘bins’) changes, the boxes shift abruptly as observations fall in or out of different bins. This is a little unsatisfactory. From another perspective, it also loses a little information in the data as we throw away the precise location of each observation and use simply the location of the bin in which it lies. One way of addressing this is to use boxes whose centres lie directly over each observation. A further modification is to replace the sharp edges of the box with a smooth shape (referred to as a kernel function) which will blend the information across the dataset to create a smooth pattern. The left hand plot below illustrates this (code not displayed) using a small sub-sample of the data so that the effects of each observation can be seen. When the kernel functions over the observations are added together, a smooth pattern emerges. This is referred to as a density estimate as it reflects the density of data across different locations on the axis. The right hand plot apples this to the full sample, through the geom_density() layer, resulting in a smooth representation of the underlying pattern. aircraft_log_3 %&gt;% ggplot(aes(Speed)) + geom_density() + xlim(4.5, 8.5) + xlab(&#39;log(Speed)&#39;) + geom_rug(sides = &#39;b&#39;) There remains the issue of choosing a bandwidth to control the smoothness of the density estimate, through the breadth of the kernel function. If this is not specified, geom_density tries to make a good choice, but it can be valuable to look at the effects of applying different levels of smoothing. This question is the early manifestation of an issue which will reappear often in later discussions of model building, namely how complex should a model be. At this stage, in the simple setting of exploring data visually, the issue is not a pressing one. Indeed it is perfectly reasonable to view the data at different levels of smoothness to inspect the patterns which emerge at different scales. Approaches to decisions on complexity will be deferred to Chapter 15.1 in the context of more extensive model building. aircraft_log_3 %&gt;% ggplot(aes(Speed)) + geom_density(aes(linetype = &quot;0.2&quot;), bw = 0.2) + geom_density(aes(linetype = &quot;0.1&quot;), bw = 0.1) + geom_density(aes(linetype = &quot;0.5&quot;), bw = 0.5) + xlim(4.5, 8.5) + xlab(&#39;log(Speed)&#39;) + geom_rug(sides = &#39;b&#39;) + scale_linetype_manual(name = &quot;bandwidth&quot;, values = c(&quot;0.2&quot; = &quot;solid&quot;, &quot;0.1&quot; = &quot;dashed&quot;, &quot;0.5&quot; = &quot;dotted&quot;)) An interactive display which allows animation across different levels of smoothing is available in sm.density function from the sm package. sm.density(aircraft_log_3$Speed, xlab = &quot;log(Speed)&quot;, panel = TRUE) One advantage of density estimates is that it is a simple matter to superimpose these to allow different groups to be compared. Here the groups for the three different time periods are plotted using colour fill (fill) and transparency (controlled by alpha which lies between 0 and 1) to enhance the comparison. There is a systematic movement to higher speeds from period 1 and 2 while in period 3 further increases may be specialising into particular designs, reflected in the multiple modes. aircraft_log %&gt;% ggplot(aes(Speed, col = Period, fill = Period)) + geom_density(alpha = 0.5) + xlab(&#39;log(Speed)&#39;) There are other ways in which density estimates can be displayed. The plots below use density ridges (Wilke 2023) and violin plots (Hintze and Nelson 1998) to display the information in innovative ways. The sina plot from the ggforce package uses jitter to separate the observations vertically, with the width of jitter controlled by the denisty estimate. This provides a display which achieves an attractive combination of raw data and underlying trend. library(ggridges) library(ggforce) ggplot(aircraft_log, aes(Speed, Period)) + geom_density_ridges() + xlab(&#39;log(Speed)&#39;) ggplot(aircraft_log, aes(Speed, Period)) + geom_violin() + xlab(&#39;log(Speed)&#39;) ggplot(aircraft_log, aes(Speed, Period)) + geom_sina() + xlab(&#39;log(Speed)&#39;) The idea of density estimation extends very naturally to a wide variety of other types of data and sample spaces. For example, in the case of two-dimensional data on continuous scales, a smooth two-dimensional kernel function is placed over each observation and these are accumulated to provide a smooth two-dimensional density estimate. The plots below show Speed and Span from period 3 of the aircraft as a scattterplot and two forms of density estimate displays. ggplot(aircraft_log_3, aes(Speed, Span)) + geom_point() + labs(x = &#39;log(Speed)&#39;, y =&#39;log(Span)&#39;) ggplot(aircraft_log_3, aes(Speed, Span)) + geom_density_2d_filled() + labs(x = &#39;log(Speed)&#39;, y =&#39;log(Span)&#39;) aircraft_log_3 %&gt;% select(Speed, Span) %&gt;% smoothScatter(bandwidth = c(0.2, 0.2), xlab = &#39;log(Speed)&#39;, ylab =&#39;log(Span)&#39;) As in the one-dimensional case, the sm.density function from the sm package allows the effect of different bandwidths to be explored dynamically. aircraft_log_3 %&gt;% select(Speed, Span) %&gt;% sm.density(panel = TRUE, panel.plot = TRUE) Finally, here is a comparison of the patterns of Speed and Span across the three periods, showing the evolution of these aspects of the designs. ggplot(aircraft_log, aes(Speed, Span)) + geom_density_2d_filled() + xlab(&#39;log(Speed)&#39;) + ylab(&#39;log(Span)&#39;) + facet_wrap(~ Period) References Hintze, Jerry L, and Ray D Nelson. 1998. “Violin Plots: A Box Plot-Density Trace Synergism.” American Statistician 52 (2): 181–84. Jane’s. 1978. Jane’s Encyclopedia of Aviation. London: Jane’s. Saviotti, P P, and A W Bowman. 1984. “Indicators of Output of Technology.” In Proceedings of the ICSSR/SSRC Workshop on Science and Technology in the 1980’s, edited by M Gibbons et al., 117–47. Brighton: Harvester Press. Wilke, Claus O. 2023. Ggridges: Ridgeline Plots in ’Ggplot2’. https://CRAN.R-project.org/package=ggridges. "],["flexible-regression.html", "5.2 Flexible regression", " 5.2 Flexible regression Example: the North Atlantic Oscillation The North Atlantic Oscillation is an important atmospheric indicator which is linked to weather patterns. It is based on the difference in atmospheric pressure at sea level between Iceland and Portugal. Historical data on Winter NAO are available from the Natioanl Center for Atmospheric Research in the USA. A description of the dataset is available. library(tidyverse) path &lt;- rp.datalink(&quot;NAO&quot;) NAO &lt;- read.table(path, header = FALSE, skip = 1) %&gt;% select(Year = 1, NAO = 2) p &lt;- ggplot(NAO, aes(Year, NAO)) + geom_point() p p + geom_smooth(method = &#39;lm&#39;, se = FALSE) p + geom_smooth(se = FALSE) The plots above show the data and then a simple linear regression line as a means of summarising trend. The third plot shows a curve which seeks to track the trend in the data over time in a more flexible manner. This is donw through kernel functions of the type used in density estimation. At any time point of interest, these are used to construct a weighted mean, where the weights diminish with the distance of each observation from the point of interest. (In fact, it turns out to be more effective to fit a straight line model with these weights and to report where this line sits at the point of interest.) The plot above was constructed by using a geom_smooth layer. This uses a locally weighted linear method known as loess whose smoothness is controlled by a span argument. The flexible curve suggests more movement in the Winter NAO over time. However, we need to investigate whether this might be simply due to random variation or whether there is real evidence of decline. Discussion of this important issue will begin in Chapter 6. (This is the reason for setting se = FALSE above, for the moment.) Example: Trawling near the Great Barrier Reef Data are available from a survey of the fauna on the sea bed lying between the coast of northern Queensland and the Great Barrier Reef. The sampling region covered a zone which was closed to commercial fishing, as well as neighbouring zones where fishing was permitted. The variables are: Zone an indicator for the closed (1) and open (0) zones Year an indicator of 1992 (0) or 1993 (1) Latitude latitude of the sampling position Longitude longitude of the sampling position Depth bottom depth Score1 catch score 1 Score2 catch score 2 Interest lies in which of the explanatory variables affects the principal catch score (Score1). The data are available in thetrawldataframe in thesm` package. A plot of Score1 against Longitude suggests a linear relationship, as a first approximation. However, there is no compelling reason that the trend should be linear anda more flexible curve highlights a different pattern. The Longitude variable is essentially a proxy for ‘distance offshore’ in this region and it may be that the mean score remains high for some distance offshore and then decreases rapidly. That has biological plausibility. library(sm) p &lt;- ggplot(trawl, aes(Longitude, Score1)) + geom_point() p + geom_smooth(method = &#39;lm&#39;, se = FALSE) p + geom_smooth(se = FALSE) You can experiment with this kind of model interactively by issing the instruction sm.regression((trawl$Longitude, trawl$Score1, panel = TRUE) This requires the rpanel and tkrplot packages to be installed. The idea of creating flexible patterns by considering the data locally extends to other settings. In particular, this can be done with two covariates simultaneously. The code below uses the sm.regression function from the sm package to create a flexible surface which gives a description of how the mean catch score change across longitude and latitude simultaneously. trawl1 &lt;- filter(trawl, Year == 1 &amp; Zone == 1) Position &lt;- cbind(trawl1$Latitude, -trawl1$Longitude) sm.regression(Position, trawl1$Score1, xlab = &quot;-Latitude&quot;, ylab = &quot;Longitude&quot;, zlab = &quot;Score&quot;) As a final example, we will revisit the rodent dataset was introduced in Section 2.2. In fact this is only part of a larger dataset on mammals, available in mammal_speed. Interestingly, the positive connection between mass and speed for rodents does not carry through for all mammals. For the heaviest mammals, speed stabilises and then tails off as mass increases. At what mass is speed heaviest, on average? A smooth curve description of the underlying relationship allows us to quantify this, without making any detailed assumptions on what the shape of this relationship should be. For the moment, we can simply inspect the plot to see that the maximum occurs at a mass around 4.5 on the log scale, and so exp(4.5) = 90 on the mass scale. library(ggplot2) path &lt;- rp.datalink(&quot;mammal_speed&quot;) load(path) ggplot(mammalSpeed, aes(log(Mass), log(Speed))) + geom_point(aes(col = Family)) + geom_smooth(se = FALSE) "],["further-reading-3.html", "5.3 Further reading", " 5.3 Further reading Bowman, A.W. and Azzalini, A. (1996). OUP: Oxford. Green, P.J. and Silverman, B.W. (1994). . Chapman &amp; Hall: London. Hastie, T. and Tibshirani, R. (1990). . Chapman &amp; Hall: London. Ruppert, D., Wand, M.P. and Carroll, R.J. (2003). . CUP: Cambridge. Wood, S. (2017) Generalized Additive Models: an introduction with R. CRC Press. "],["exercises-4.html", "5.4 Exercises", " 5.4 Exercises 5.4.1 Does conservation work? A recent paper in Science by Langhammer et al. (2024) reviewed a large number of studies of conservation projects in order to evaluate what benefit these have brought. To allow comparisons among projects which have operated over different timescales, the principal response variable was the rate of improvement in the relevant outcome. This was expressed in a standardised ‘effect size’ scale. Links are available for the data and a description of the recorded variables. Read the data. The response variables of interest is in column 11. Rename this as ‘Effect size’. Construct a suitable plot to show how Effect size varies across the different types of Intervention considered. In particular, consider plots based on density estimation (geom_violin, geom_sina). What plot would you recommend? 5.4.2 The spatial distribution of laryngeal cancer (smoothing) The lcancer dataframe in the sm package contains data which record the spatial positions of cases of laryngeal cancer in the North-West of England between 1974 and 1983, together with the positions of a number of lung cancer patients who were used as controls. The data have been adjusted to preserve anonymity. See the help file for details. It may be more convenient to divide the Easting and northing variables by 10000 to provide neater scales for plotting. Use the cbind function to create a matrix which has two columns containing the Easting and Northing values for the cases alone, and another to do the same thing for the controls. (You may need to consult the help file for cbind.) Draw a density estimate for the cases by passing the first matrix to the sm.density function, with display = \"image\". Experiment with different values of the argument h which controls the degree of smoothing. This needs to be a vector of length two as there are two variables present. The value h = rep(0.12, 2) should work reasonably well but experiment with others. Superimpose a density estimate for the controls by calling sm.density with the second matrix, with display = \"slice\" and with the additional argument add = TRUE to superimpose contours on the plot. Consider what the comparison of these two density estimates tells you. 5.4.3 Aircraft data In the lecture notes, a density estimate of log(Speed) and log(Span) from the aircraft data , for the third time period, was found to be trimodal. Produce density estimates for the first two time periods and explore whether multimodality is also present there. (Hint: you may wish to consider whether facet_wrap might help.) 5.4.4 Radiocarbon data This dataset refers to the calibration required with radiocarbon dating. The datset radioc is available in the sm package and details can be obtained by issuing the instruction ?radioc. Create a subset of the data which corresponds to Cal.age between 2000 and 3000 years. Experiment with values of the smoothing parameter h, or equivalently the degrees of freedom parameter df. (Ignore the Precision information at this stage.) What happens as \\(h\\) becomes very large (or df becomes very small)? What level of smoothing would you chose to provide a good description of the calibration curve? What level of smoothing does the crossvalidation method suggest? 5.4.5 Follicle data The follicle dataset is available in the sm package. This records information on the number of ovarian follicles counted from sectioned ovaries of women of various ages. See ?follicle for some details. A variability band for the regression curve indicates the precision in the estimation of the curve at different locations. This can be constructed simply by adding the argument display = \"se\" to the sm.regression function call. Use this to identify features of the data which cause the changes in the width of the curve. Construct a density estimate to show how the density of the age information changes. References Langhammer, Penny F, Joseph W Bull, Jake E Bicknell, Joseph L Oakley, Mary H Brown, Michael W Bruford, Stuart HM Butchart, et al. 2024. “The Positive Impact of Conservation Action.” Science 384 (6694): 453–58. "],["inference.html", "6 Inference on means", " 6 Inference on means So far we have considered how to read, organise and visualise datasets. We are now ready to discuss one of the main themes of statistics, namely weighing up the evidence for the presence, characteristics or size of different features in the population lying behind the data. That process is referred to as statistical inference. The first step in doing this is to think about the uncertainty or variation involved in the data and how to quantify this. That will enable us to discuss some standard tools, such as confidence intervals and hypothesis test, which we will explore in this chapter in some simple settings. "],["samples-and-populations.html", "6.1 Samples and populations", " 6.1 Samples and populations The distinction between a sample of data and the population from which the data come was mentioned briefly in Section 1.2.3, where a population was defined rather informally as the collection of all the observations we could ever make of the process we are studying. We will now explore this further by carrying out a kind of ‘thought experiment’. By sampling from a population whose characteristics are known, using R to do so, we can investigate how well the features of a sample guide us on the features of the underlying population. To give our thought experiment a little context, let’s assume we are taking water samples from a particular location on a river with a view to assessing the water quality. This is often quantified through the percentage of dissolved oxygen. So we are dealing with measurements on a continuous scale. The rp.sample function in the rpanel package provides a convenient means of experimenting with this situation. The plots below give a flavour of what can be done but you may find it more instructive to run the function ‘live’ to create your own images and experiment with different settings. The simple instruction rp.sample() will launch a window with a panel of interactive controls. The plots above shows histograms of multiple sets of sampled data. In each case the population is the same but the sample is different every time. It is the nature of variability that the particular data points we see, and so the details of the patterns which are displayed, change with every sample. The first row of plots has sample size 25. Here there is a lot of variability in the shapes of the histograms. Sometimes there appear to be two clusters of data, or a rather skewed shape. We know that the underlying population is the same for all samples but the detailed shapes we see can be quite different. This tells us that we should be cautious in interpreting detailed patterns when the size of our sampled dataset is small. The second row has sample size 2500 and here the variation in shape is much smaller. The third row of plots has sample size 250000. Now there is almost no variation in shape. The continuous curve shows the shape of the distribution from which the data were sampled and the histograms match this very closely every time. This illustrates the general principle that more data gives us more information, reducing the variability in the features of the population that the data express. The continuous curve is what the histograms ‘converge’ to as the sample size gets higher and higher and variability gets smaller and smaller (and the histogram bin width also get smaller and smaller). This is known as the density function and it defines the population from which we are sampling. For any interval on the axis, the area under the density function gives the probability of an observation falling in this range so, in that sense, the density function describes how the probability of observing different values changes across the axis. This particular population has a normal distribution. This has a characteristic symmetric shape which falls away smoothly as we move to values further from the centre of the distribution. Just as the mean of a sample can be characterised as the ‘centre of gravity’ of the dataset, so the mean of the distribution can be defined as the ‘centre of gravity’ of the distribution. One consequence of the symmetry of the normal distribution is that the mean sits at its point of symmetry, where the density function reaches its highest value. In the present case the mean has the value 5. Similarly, the spread of the distribution around its mean value can be quantified by the standard deviation. Like the mean, this can be defined in terms of the density function but we can also simply regard it as the value to which the sample standard deviation converges as the sample size gets higher and higher. In the present case the population standard deviation is 0.4. The plot above shows the normal distribution, with the mean highlighted and with a superimposed scale which measures distance along the axis in units of standard deviation, so ‘1’ lies at a distance of 0.4 from the mean, ‘2’ at 0.8 and so on. This shows that most of the distribution lies within 2 standard deviations of the mean. This happens with samples too so that, most of the time, the observations in a sample lie within 2 standard deviations of the mean. The plots below illustrate this. This makes sense, as the standard deviation quantifies how far away observations are from the mean, on average. So, informally, one standard deviation away from the mean, on either side, takes us ‘halfway’ into the spread of the data. From that perspective, it then seems reasonable that two standard deviations away from the mean should cover most of the observations in our sample. This works rather neatly for the normal distribution but in fact the general principle holds for many distributions, even ones which are not symmetric, although the guideline will become less effective as the size of skewness increases. An exercise in Section 6.7 below will invite you to explore this. The normal distribution is a very important one for modelling measurements on a continuous scale but there are many other possible distributions. We will meet some of these in due course, as we encounter datasets with features which need other distributional shapes to describe them. "],["standard-error.html", "6.2 Quantifying uncertainty: standard error", " 6.2 Quantifying uncertainty: standard error The setup of the ‘thought experiment’ of the previous section was necessarily rather artificial, as we defined, through the settings in rp.sample, the population from which the samples were being drawn. In practice, of course, a sample is all we have and we seek to use this to learn about the population - perhaps simply its mean, or perhaps other features of interest. This is why the title of the current chapter is ‘Inference’, as we seek to infer features of the unknown population from the - usually rather limited - data in our sample. We will explore this by continuing our thought experiment, using the population mean as the focus, as this is often a parameter of major interest. How accurate is the sample mean, \\(\\bar{x}\\), as a guide to the population mean, denoted as \\(\\mu\\)? Again, you may wish to run rp.sample live, to explore this. The plots below show, in the upper row, multiple samples, each of size 25. The second row of plots shows the mean of each sample, with the final plot in this row showing sample means accumulated over 50 different samples. The sample means are clustered round the population mean. There is variation in the sample means but the size of this is much smaller than the variation in the individual observations. This is the point at which some theory can help us. If we denote the population standard deviation by \\(\\sigma\\) and the size of the sample by \\(n\\), then a theoretical calculation tells that the standard deviation of the sample mean is \\[ \\mbox{se}(\\bar{x}) = \\sigma / \\sqrt{n} . \\] This gives us a measure of uncertainty, or inaccuracy, of the sample mean \\(\\bar x\\) as an estimate of the population mean \\(\\mu\\). In a change of terminology which reflects the fact that we are no longer dealing with the uncertainty of individual observations but of a feature of the underlying population, we refer to this as the standard error. The left hand plot below shows a large collection of sample means along with a standard error scale. We can see that the sample means mostly lie within two standard errors of the population mean. This is a very important phenomenon which we should pause to highlight. Most of the time, the sample mean lies within 2 standard errors of the population mean. We will exploit to this to create some very useful inferential tools. In fact, as we will see later, this principle is a much more general one which applies in many different settings involving a population parameter and an estimate of it. One issue remains: the standard error involves the standard deviation of the population which, usually, we do not know. It is natural to estimate this by the sample standard deviation, \\(s\\). As the standard deviation of the data changes with every sample, it is easier to investigate this in terms of the t-statistic \\[ \\frac{\\bar{x} - \\mu}{se(\\bar x)} = \\frac{\\bar{x} - \\mu}{s / \\sqrt{n}} , \\] where \\(\\bar{x}\\) and \\(s\\) denote the sample mean and standard deviation and \\(\\mu\\) denotes the population mean. We can think of this as measuring the distance between the population and sample means in units of (estimated) standard error. The right hand plot below shows the t-statistics produced by rp.sample. Reassuringly, these generally lie within \\(\\pm 2\\) which confirms that that principle highlighted above still applies. (From now on, we will generally assume that the term ‘standard error’ implies the estimation of any unknown parameters in the standard error formula.) This gives a very helpful guide to the accuracy of the sample mean as a guide to the population mean but a more precise guide is available, so we should use this. Theoretical calculations can tell us about the complete distribution of the sample mean. When we are sampling from a normal population, it turns out that the sample mean also has a normal distribution, with mean \\(\\mu\\) and the standard error shown above. When we use the estimated standard error, the t-statistic then becomes the focus and it turns out to have a t-distribution. The plot below superimposes the density function for this distribution on the t-statistics generated by rp.sample. The shape is very similar to the normal distribution with the main difference being that the t-distribution usually has slightly thicker tails. (This is due to the additional variation which arises from using the sample standard deviation in the expression for the standard error.) The t-distribution has a parameter called the degrees of freedom and in this setting that takes the value \\(n-1\\). What happens if the distribution of the data is not normal? It turns out the distribution of the sample mean tends towards a normal distribution as the sample size increases, even when the original data are not normally distributed. This remarkable result is called the Central Limit Theorem. It can be proved theoretically and one of the exercises at the end of the chapter invites you to experiment with this using rp.sample to generate data from distributions with different degrees of skewness. The practical consequence is that sample means, standard errors and t-distributions can still provide the basis of very useful analysis, although the accuracy of our results will be less good if the distribution of the data is far from normal. These ‘thought experiments’ have enabled us to consider some of the concepts involved when we aim to learn about a population parameter, such as a mean, from a sample of data. This places us in an good position to interpret what individual samples are able to tell us, using tools developed in the following sections. "],["confidence-intervals.html", "6.3 Confidence intervals", " 6.3 Confidence intervals One of the earliest people to think about the issue of quantifying the uncertainty of a sample mean as a guide to the true mean was W.S.Gossett, in a famous paper in 1908. He published under the pseudonym Student because he was working for the Guiness brewery at the time. He used data from a paper by Cushny &amp; Peebles (1905) which compared the effects of two different drugs on lengthening the hours of sleep in 10 different subjects. The experimental design involved each subject taking each of the two drugs at different times, so this is paired data. A helpful first step is to plot the paired values against one another, with the line y = x providing a reference. If there is no systematic difference between the effects of the drugs then the observations should cluster around this line. sleep_wide &lt;- pivot_wider(sleep, values_from = extra, names_from = group, names_prefix = &#39;drug_&#39;) ggplot(sleep_wide, aes(drug_1, drug_2)) + geom_point() + geom_abline(slope = 1, col = &#39;blue&#39;, linetype = 2) The extra times for drug 2 are higher than those for drug 1, apart from one patient where the times are identical. In one patient the difference between the two drugs is particularly large but otherwise the differences are broadly similar. In particular, there is no obvious change in the size of the difference as the size of the response to drug 1 changes. This allows us to focus on the differences of the responses from the two drugs as the key information to analyse. sleep_diff &lt;- sleep_wide$drug_2 - sleep_wide$drug_1 The principle highlighted in the previous section describes how the sample means \\(\\bar{x}\\) varies around the population mean \\(\\mu\\). Most of the time, the sample mean lies within 2 standard errors of the population mean. In practice, all we see from our observed sample of data is a single \\(\\bar{x}\\) and we can compute its estimated standard error. We can use this to work backwards to infer where the population mean might lie. Plausible values for the population mean are those for which the sample mean lies within 2 standard errors. From a practical perspective, we can express this more directly. Plausible values for the population mean lie within 2 standard errors of the sample mean. This provides an informal explanation of how an interval estimate can be constructed. To explore this in graphical form, the function rp.t_test will launch an interactive panel. rp.t_test(sleep_diff) The initial plot shows the data, a density estimate and the sample mean. The standard error principle described in the previous section provides a ‘ruler’ with which we can interpret the distance between the population mean and the sample mean - but where should we place it? Clicking on ‘candidate’ in the interactive panel superimposes a standard error ruler on the plot, as shown below. The position of the candidate population mean can be altered by moving the slider. Which if these candidate positions are plausible? The value 0.5 (left hand plot) does not look plausible as, if this were the correct value, we have a sample mean which is more than two standard errors away. The same is true of the candidate value 3 (right hand plot). The value 2 does look plausible as the sample mean is within two standard errors. So the plausible values of the population mean are those for which the sample mean is within a distance of two standard errors. We are viewing the population mean as a fixed value, with the sample mean subject to the variation induced by a sample of data. However, in practical terms, we can place our ruler at the sample mean and identify our plausible population means as those which lie within two standard errors of the sample mean, namely \\[ ( \\bar{x} - 2 \\mbox{se}(\\bar x), \\bar{x} + 2 \\mbox{se}(\\bar x) ) \\] where the round brackets on the outside indicate that this is a range of values, or interval, defined by its lowest and highest points. This is achieved by selecting ‘sample mean’ for the ruler position and it is shown in the left hand plot below. This is a very useful guideline, but we also have a more detailed description of the variation in the sample mean so we should use this to construct an interval more carefully. Clicking on ‘distribution’ will superimpose on the standard error ruler the t-distribution discussed in the previous section. (The t-distribution refers to the standard error scale so its area is not comparable with the area of the density estimate of the data.) Clicking on ‘detail’ will add further details, as displayed in the right hand plot above. Here there are notches in the t-distribution to indicate the positions within which lie probablity 0.95 (leaving probability 0.025 in each tail). The corresponding positions on the original axis then correspond to a more careful definition of our interval. The use of probability 0.95 makes this a 95% confidence interval. Some of the properties and interpretation of a confidence interval are explored in one of the exercises at the end of this chapter. The discussion above is intended to explain how a confidence interval is constructed. A summary statement of the confidence interval is provided by the R function t.test, which is also called when rp.t_test is used. t.test(sleep_diff) ## ## One Sample t-test ## ## data: sleep_diff ## t = 4.0621, df = 9, p-value = 0.002833 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.7001142 2.4598858 ## sample estimates: ## mean of x ## 1.58 "],["hypothesis-tests.html", "6.4 Hypothesis tests", " 6.4 Hypothesis tests With the sleep data, there is special interest in whether the population mean of the difference between the two treatments might be \\(0\\). If \\(0\\) is not a plausible value then there is evidence that the treatments have different effects. Our standard error principle tells us that if this is indeed the correct value for the population mean then the sample mean should lie within two standard errors, so we can investigate this by placing the standard error ‘ruler’ at \\(0\\). Relaunching rp.t_test with the proposed mean value mu = 0 and with ruler.position = 'reference' will do this. The result is shown in the left hand panel below. It is clear that \\(0\\) is not a plausible value, as the sample mean lies at a distance of around 4 standard errors. rp.t_test(sleep_diff, mu = 0, ruler.position = &#39;reference&#39;) We can make this more precise by adding the t-distribution as a full description of the variability of a sample mean. This is shown in the right hand plot above, along with some further details designed to quantify the evidence. This is done by measuring how much of the reference t-distribution lies beyond the sample mean. This is called the p-value. The probability of seeing a sample mean further away than the one we have actually seen is very small at \\(0.0014\\). In order to be even-handed, as sample means can lie above or below the population mean, we should also add the probability of seeing a sample mean in the corresponding positions below the proposed population mean. The p-value is therefore \\(0.0028\\). The conventional threshold, or significance level, is \\(0.05\\), corresponding to the notches on the t-distribution. Values below this are said to be significant at the \\(5\\)% level, indicating a mismatch between what we see in the data, through the sample mean, and what we expect to see if we have the correct population mean. So we have quite strong evidence that the population mean is not \\(0\\). This is a rather informal and conceptual description of what is known as a hypothesis test. The name reflects the fact that, from a more formal perspective, two different hypotheses are being considered. We can express these as: \\[ \\mbox{Null hypothesis}: \\hspace{5em} \\mu = 0 \\\\ \\mbox{Alternative hypothesis}: \\hspace{2em}\\mu \\neq 0 \\] The two hypotheses are not viewed in equal terms. The strategy is to adopt the cautious perspective of the null hypothesis which assumes that there is no difference between the treatments, only moving to the alternative if the evidence is compelling. This is rather like a court of justice where a defendant is assumed to be innocent until evidence accrues sufficiently strongly to convince the court that innocence is not a tenable position to hold. To weigh up the evidence here, the test statistic is our old friend the t-statistic \\[ t = \\frac{\\bar{x} - 0}{\\mbox{s.e.}(\\bar{x})} = \\frac{\\bar{x}}{\\mbox{s.e.}(\\bar{x})} \\] which measures the distance between the null hypothesis mean and the sample mean in units of standard error. If the null hypothesis is correct then we know that this follows a \\(t\\)-distribution with \\((n-1)\\) degrees of freedom. The evidence can then be expressed succinctly in a p-value as described above. If the null hypothesis in not correct and the population mean takes some value different from \\(0\\), then the sample mean is likely to be further from \\(0\\), the t-statistic will tend to be larger than 2 or less than -2, and the p-value will be small. This general strategy can be adapted to many other settings where a question of interest can be expressed in terms of a particular value taken by a parameter. In our present simple setting, a confidence interval and a hypothesis test each use the basic building blocks of uncertainty, such as standard error and t-statistic, but assemble these in different ways. The results are directly comparable. For example, a \\(95\\)% confidence interval will exclude the value \\(0\\) if and only if a hypothesis test with null hypothesis that the population mean is \\(0\\) is significant at the \\(5\\)% level. So which should we use? In the present case, a confidence interval has the advantage that not only does it indicate the plausibility of \\(0\\), by whether the interval covers this value, but it also gives an indication of what plausible values might be, and in particular the size of the interval expresses the degree of uncertainty in this. However, before long we will encounter problems which cannot be expressed in terms of the value of a single parameter and here the hypothesis test will be valuable. "],["comparing-two-means.html", "6.5 Comparing two means", " 6.5 Comparing two means R. A. Fisher, one of the early pioneers of statistical methods, developed the work of Gossett on the t-test and illustrated its use in comparing two means from a small agricultural experiment at the famous Rothamsted research station (Fisher et al. 1925). The data refer to the number of tillers (independent lateral branches) growing on barley in two experimental groups – electrified and caged. In this experiment, the numbers of observations are so small that we can simply type these into R directly. The function rp.t_test can give us an initial plot of the data. caged &lt;- c(17, 27, 18, 25, 27, 29, 27, 23, 17) electrified &lt;- c(16, 16, 20, 16, 20, 17, 15, 21) rp.t_test(caged, electrified, vlab = &#39;Number of tillers&#39;) The data consist of counts, and there are some identical values, but the observations are sufficiently spread that an assumption of underlying normal distributions is a reasonable approximation, strengthened by the knowledge that the description of the variability in the sample mean by a normal distribution will be even better, as discussed in Section 6.2. As the comparison of two means is a commonly occuring issue, we will keep the discussion general by denoting the measurements from each group by \\(x\\) (caged) and \\(y\\) (electrified), with population means \\(\\mu_x\\) and \\(\\mu_y\\). The key question is whether there is evidence that \\(\\mu_x\\) and \\(\\mu_y\\) are different, or equivalently that \\(\\mu_x - \\mu_y\\) could be different from \\(0\\). A difference scale, and the difference in sample means \\(\\bar{x} - \\bar{y}\\), are marked on the plot of the data above. The key quantity we now need is the standard error of \\(\\bar{x} - \\bar{y}\\). We know how to calculate standard errors for the individual sample means, \\(se(\\bar{x}) = s_x /\\sqrt{n_x}\\) and \\(se(\\bar{y}) = s_y / \\sqrt{n_y}\\), but how should we combine these? The definition of the sample standard deviation in Section 4.3 involved averaging the squared distance from the sample mean and then applying the square root to return to the scale of measurement of the data. Applying this thinking to the combination of standard errors leads to \\[ se(\\bar{x} - \\bar{y}) = \\sqrt{se(\\bar{x})^2 + se(\\bar{y})^2} \\] This makes sense because whenever we combine quantities which are subject to variability the result will always have greater variability, even when we subtract the quantities. The left hand plot below superimposes the standard error ruler on the plot of the data. The argument for the construction of a confidence interval follows the pattern of the single-sample case. Plausible values for the difference in population means are those where the difference in sample means is no more than a distance of two standard errors away. By placing the ruler at the difference in sample means, a confidence interval is seen to be approximately \\((2.0, 9.5)\\). A more careful and precise confidence interval again follows the earlier one-sample argument, now based on the t-statistic \\[ \\frac{\\bar{x} - \\bar{y}}{se(\\bar{x} - \\bar{y})} \\] which measures the distance between the sample mean difference and \\(0\\) in units of standard error. Surprisingly, the exact distribution of this t-statistic when the two populations are normally distributed with identical means cannot easily be derived. The root of the difficulty is the use of different sample standard deviations for the two groups. This helpfully handles the case where the two populations have different standard deviations but it introduces considerable difficulty into the distributional calculations. A solution was provided by Welch (1947) who showed that the distribution we need is very well approximated by a t-distribution whose degrees of freedom can be calculated through a relatively simple formula. This is the default setting in the t.test function (and rp.t_test) in R. The right hand plot above adds this distribution to the standard error ruler, with the usual notches to indicate the end points of a \\(95\\)% confidence interval, \\((1.8, 9.6)\\). As \\(0\\) is not in this interval, we have clear evidence of a difference in the mean number of tiller in the two groups. The plots below illustrate a hypothesis testing approach to this analysis, considering the suitability of the null hypothesis that \\(\\mu_x - \\mu_y = 0\\), with the alternative hypothesis that \\(\\mu_x - \\mu_y \\neq 0\\). As in the one-sample case, placing the standard error ruler at \\(0\\) on the difference scale indicates that the sample mean difference is nearly \\(6\\) standard errors away. This already makes the situation clear in principle but the detail is provided by the right hand plot which adds the t-distribution and the p-value, \\(0.0081\\), whose small size confirms the significant evidence that the population means are different. The output of the t.test function (which is called by rp.t_test) gives the detail of both the confidence interval and the hypothesis test approaches. t.test(caged, electrified) ## ## Welch Two Sample t-test ## ## data: caged and electrified ## t = 3.1753, df = 11.847, p-value = 0.008109 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 1.785743 9.630924 ## sample estimates: ## mean of x mean of y ## 23.33333 17.62500 The analysis above is able to accommodate the case where the standard deviations of the \\(x\\) and \\(y\\) populations are different. If it is justifiable to assume that these population standard deviations are identical, then it becomes possible to combine the sample standard deviations into a pooled estimate and to derive the exact distribution of the t-statistic. This is a t-distribution with \\(n_x + n_y - 2\\) degrees of freedom. However, the Welch version discussed above loses very little in performance when the populations have the same standard deviations and it protects us against an inappropriate assumption that the population standard deviations are identical. So the Welch t-test provides a very good option. References Fisher, Ronald Aylmer et al. 1925. “Applications of ‘Student’s’ Distribution.” Metron 5 (3): 90–104. Welch, Bernard L. 1947. “The Generalization of ‘STUDENT’s’problem When Several Different Population Varlances Are Involved.” Biometrika 34 (1-2): 28–35. "],["going-further-and-deeper.html", "6.6 Going further and deeper", " 6.6 Going further and deeper Two-sided tests were described. One-sided are also possible but less advisable in this context. It is possible to test for equality of variances/sd’s in the t-test but this is not advisable. M., D., and M. (2019) Hubbard, Haig, and Parsa (2019) Scientists rise up against statistical significance https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.nature.com%2Farticles%2Fd41586-019-00857-9&amp;data=05%7C02%7CAdrian.Bowman%40glasgow.ac.uk%7Cb00f4b6b25e5433dbfad08de4555a05d%7C6e725c29763a4f5081f22e254f0133c8%7C1%7C0%7C639024431234483057%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&amp;sdata=UJcOXx2EvyU9zR5Qv6Mn5M2FQ6yNvJeXE8jI91%2FsmYM%3D&amp;reserved=0 References Hubbard, Raymond, Brian D Haig, and Rahul A Parsa. 2019. “The Limited Role of Formal Statistical Inference in Scientific Inference.” The American Statistician 73 (sup1): 91–98. M., Diez D., Barr C. D., and Çetinkaya-Rundel M. 2019. OpenIntro Statistics. 4th edition. openintro.org/os. "],["quantifying-uncertainty-exercises.html", "6.7 Exercises", " 6.7 Exercises 6.7.1 Standard errors Use the rp.sample function to explore the phenomenon that most of the time, the true parameter and the estimate are within 2 standard errors. Try setting the true mean and standard deviation to different values to confirm that this priniciple holds. 6.7.2 What does ‘confidence’ mean? The meaning of a confidence interval sometimes causes a bit of confusion. The interpretation as a range of plausible values for the quantity we are estimating is a helpful but informal one. To explore the formal properties a little further we can experiment with the rp.ci function in the rpanel package for R. An example is shown below. Here we randomly sample 30 observations from a distribution with mean 0 and standard deviation 1 and compute a confidence interval for the mean. However, we do this 100 times. Each interval will be different because it is based on a different random sample of data. The confidence of the intervals is conventionally set at 95% and this is done by choosing percentiles in the confidence interval formula which capture 95% of the \\(t\\)-distribution. Then, on average over repeated random samples, the computed confidence intervals will capture the true value 95% of the time. Any particular set of 100 intervals will not have exactly 95 which capture the true value but if we keep repeating this and accumulate the tally, the proportion of intervals which capture the true value will settle down to 95%. "],["inference-with-categorical-data.html", "7 Inference with categorical data", " 7 Inference with categorical data So far, we have considered data measured on a continuous scale but there are, of course, many other types of data structure. Here we will deal with data in the form of categories. "],["simple-proportions.html", "7.1 Simple proportions", " 7.1 Simple proportions We often encounter problems where: the number of items in the sample, denoted by \\(n\\), is fixed in advance; there are two possible outcomes for each item (yes/no, success/failure, etc.); each item has the same chance of producing a ‘success’, independently of all other items. This leads to the Binomial model which describes the probabilities of the number of ‘successes’ out of the \\(n\\) items, when the probability of success for a single item is \\(p\\). If the number of ‘successes’ in the sample of size \\(n\\) is \\(x\\) then a natural estimate of \\(p\\) is the sample proportion, \\(x/n\\). We write: \\[ \\hat{p} = x/n \\] We need to quantify the uncertainty associated with estimating \\(p\\) by \\(\\hat{p}\\). As usual, the standard error does this for us. The plot below uses simulation to show the variation in \\(\\hat{p}\\) when samples of size \\(n\\) are repeatedly drawn from populations where the true proportion if \\(p\\). You might like to experiment with this code tp see the effectsd of changing n and p. n &lt;- 10 p &lt;- 0.5 x &lt;- rbinom(1000, n, p) hist(x / n, col = &#39;grey&#39;, xlim = c(0, 1), main = paste(&#39;n =&#39;, n, &#39; p =&#39;, p)) abline(v = p, col = &#39;red&#39;) There are two simple points to note: The sample proportion is subject to error but it is centred on the true proportion in the population. The size of the error in the sample proportion decreases as the size of the sample increases. In fact it is possible to show that the variance of \\(\\hat{p}\\) is \\(p (1 - p) / n\\). We can estimate the unknown \\(p\\) in this expression to obtain the standard error \\[ s.e.(\\hat{p}) = \\sqrt{\\hat{p}(1-\\hat{p})/n} \\] As in other cases, we should find that most of the sample proportions lie within two standard errors of the true proportion. In this case we do not have an exact result for a confidence interval, as we do with the Normal model, but an approximate 95% confidence interval for \\(p\\) is easily obtained as \\[ \\hat{p} \\pm 2 \\mbox{s.e.}(\\hat{p}) \\] In a briefing note for journalists, the UK Parliament provides a helpful guide to opinion polls. (The document was prepared by Peter Kellner for the British Polling Council.) This refers to an accuracy of 3% in the main percentages reported in a poll based on 1000 respondents. Where does 3% come from? When the sample size is 1000, the standard error of a proportion is \\(\\sqrt{\\hat{p}(1-\\hat{p})/1000}\\). The largest possible value of \\(\\hat{p}(1-\\hat{p})\\) is when \\(\\hat{p} = 0.5\\), so the upper limit on the standard error is \\(\\sqrt{0.5 \\times 0.5 / 1000} = 0.16\\). Two standard errors is then 0.032, or around 3%. This is a useful guide, but there are many other aspects of this to consider. We are often interested in comparing the proportions from different categories, such as support for the main parties in an election, and the uncertainty will be increased when two proportions are involved. In addition, there are all the usual issues about the extent to which the sampling is genuinely random or subject to bias of various kinds. Nonetheless, the ability of standard error to help in quantifying uncertainty is helpful. "],["comparing-proportions.html", "7.2 Comparing proportions", " 7.2 Comparing proportions Example: Smoking and lung cancer In a famous historical study of the association between smoking and lung cancer, Doll &amp; Hill compared the numbers of smokers and non-smokers in samples of lung cancer patients and controls. The data for females are shown below. cases controls smokers 41 28 non-smokers 19 32 Is there evidence of a link between smoking and lung cancer? Details of how these data were collected are given in the paper. There are interesting questions here about what constitutes an appropriate control group. In fact other hospital patients, not suffering from lung cancer, were used. The principal question of interest is whether the proportion of smokers among the cases is different from the proportion of smokers among the controls. We denote the underlying true proportion among the cases and controls by \\(p_1\\) and \\(p_2\\) respectively, with corresponding sample sizes \\(n_1\\) and \\(n_2\\). We can estimate the true proportions by the sample proportions, \\[\\begin{eqnarray*} \\hat{p}_1 = 41/60 = 0.683 \\\\ \\hat{p}_1 = 28/60 = 0.467 \\end{eqnarray*}\\] We can also calculate the standard error of each sample proportion as \\[\\begin{eqnarray*} \\mbox{se}_1 &amp; = &amp; \\sqrt{\\hat{p}_1(1-\\hat{p}_1)/n_1} = \\sqrt{0.683 \\times 0.317 / 60} = 0.060 \\\\ \\mbox{se}_2 &amp; = &amp; \\sqrt{\\hat{p}_1(1-\\hat{p}_1)/n_1} = \\sqrt{0.467 \\times 0.533 / 60} = 0.064 \\end{eqnarray*}\\] However, it is the difference between the two groups which is of interest to us. We have a natural estimate in the differences of the proportions \\(p_1-p_2\\) in the difference of the sample proportions \\[ \\hat{p}_1 - \\hat{p}_2 = 0.683 - 0.467 = 0.216 . \\] We can also calculate the standard error of this difference by combing the individual standard errors, as follows: \\[\\begin{eqnarray*} \\mbox{se}_{\\mbox{difference}} &amp; = &amp; \\sqrt{\\mbox{se}_1^2 + \\mbox{se}_2^2} \\end{eqnarray*}\\] Notice that the squared standard errors are added together, despite the fact that the estimates of the proportions are being subtracted. This is because we are measuring the uncertainty involved and so the uncertainty of the difference combines the uncertainties of the individual components. With the present data this gives \\[ \\mbox{se}_{\\mbox{difference}} = \\sqrt{0.060^2 + 0.064^2} = 0.088 \\] A \\(95\\%\\) confidence interval for the difference in proportions is then: \\[\\begin{eqnarray*} 0.216 \\pm 2 \\times 0.088 \\\\ \\mbox{i.e.} 0.216 \\pm 0.176 \\\\ \\mbox{i.e.} (0.040, 0.392) \\end{eqnarray*}\\] Since this confidence interval does not contain \\(0\\), we therefore have clear evidence that the proportions of smokers in the cases and control groups are different. "],["contingency-tables.html", "7.3 Contingency tables", " 7.3 Contingency tables The data on smoking and lung cancer can also be treated as a simple example of a contingency table, which cross-classifies counts by two different factors. In fact, this was how the data were viewed in the original paper by Doll &amp; Hill. The method of analysis we will explore can be implemented in contingency tables with any number of rows or columns. As ever, a helpful first step is to visualise the data, even when this consists of a very simple tabluation. The mosiacplot discussed earlier helps with this. The columns of the plot refer to the case and control groups. Here these are equal in size (60) but differences in in numbers would have been reflected in the width of the columns. This means that the height of each block now refers to proprtions of observations within each column. x &lt;- matrix(c(41, 19, 28, 32), ncol = 2, dimnames = list(c(&quot;smoker&quot;, &quot;non-smoker&quot;), c(&quot;cases&quot;, &quot;controls&quot;))) rp.contingency(x) If there is no association between smoking and lung cancer, then the proportions associated with each column will be identical. We can use this idea to calculate expected values, which describe the pattern we expect to see if the null hypothesis of no association is correct. Estimates of the common probabilities for each column are (69/120, 51/120) = (0.575, 0.425). The expected values by row are therefore obtained by multiplying the column totals by these probabilities. It so happens that the column totals are identical in this dataset, namely 60. 60 * 0.575, 60 * 0.575 = 34.5, 34.5 60 * 0.425, 60 * 0.425 = 25.5, 25.5 We can now compare this table of expected values (\\(E_{ij}\\)) with the table of observed values (\\(O_{ij}\\)) above. We do this through a quantity known as the chi-squared statistic, defined as \\[ \\sum_{ij} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}} \\] where the subscripts \\(i\\) and \\(j\\) index the rows and columns. The chi-squared statistic for the current dataset is \\(5.76\\). This value is meaningful only when we compare it to a reference distribution. The theory for this setting tells us that the relevant comparison is with a \\(\\chi^2\\) distribution, which is plotted below. This distribution is indexed by a parameter known as the degrees of freedom. For contingency tables with \\(r\\) rows and \\(c\\) columns, the degrees of freedom should be set to \\((r-1)(c-1)\\), which in the current case is \\(1\\). library(rpanel) rp.tables(panel = FALSE, distribution = &quot;chi-squared&quot;, degf1 = 1, observed.value = 5.76) The observed value of the test statistic, which is also shown in the plot, is much higher than values we expect to see from this reference distribution. The upper 5% point of the distribution is 3.84, which gives us a specific benchmark. We therefore have significant evidence that the proportions of smokers are different in the cases and controls. This formn of analysis has, unsurprisingly, confirmed the conclusions of the comparison of proportions in the previous section. The chi-squared test can be easily implemented in R through the chisq.test function. By default this includes a correction for 2x2 tables in order to improve the accuracy of the reference distribution. However, the conclusion is unchanged. chisq.test(x) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: x ## X-squared = 4.9105, df = 1, p-value = 0.02669 "],["exercises-5.html", "7.4 Exercises", " 7.4 Exercises 7.4.1 Opinion polls Opionion polls reported in newspapers are sometimes treated with scepticism. This is particularly true of polls published before general elections! The Independent newspaper ran a series of articles on this some time ago. In particular, it was claimed that,in ‘a sample of \\(1000\\) people, the main percentages should be accurate … to within three percent, nineteen times out of twenty’. Is this correct? How can it be justified? Use the formula for the standard error of a proportion to think this through. 7.4.2 Respiratory disease in infancy Holland, Bailey &amp; Bland (1978, Journal of Epidemiology and Community Health) reported on a study of the effects of bronchitis in infancy on the occurrence of respiratory problems later in life. The following table reports the occurrence of these events in a sample which was studied by the authors. Cough at 14 No cough at 14 Bronchitis at 5 26 247 No bronchitis at 5 44 1002 Is there evidence of a link? 7.4.3 Smoking and lung cancer The earlier analysis used a chi-squared test to assess the evidence that the proportion of smokers is different in cases and controls. Construct a confidence interval for the difference of these proportions as an alternative approach, making sure that you understand how to interpret the interval you produce. 7.4.4 Confidence intervals and p-values In the two examples above, both confidence intervals and hypothesis tests have now been used. These should be equivalent in terms of the evidence for the presence of an effect. What are the relative merits of the confidence interval and hypothesis testing approaches in this context? "],["likelihood-a-powerful-principle.html", "8 Likelihood: a powerful principle ", " 8 Likelihood: a powerful principle "],["the-idea.html", "8.1 The idea", " 8.1 The idea Explain that this principle shows us how to construct estimators. The idea of likelihood is a very powerful one. To start gently, consider the single observation at ethylene chloride concentration 35.55 which killed 15 beetles out of a group of 29, giving the proportion 0.517. The binomial distribution describes all the possible values that we might observe, and the probabilities of observing them, for a particular size of group and with a particular probability (prob) of success (death in the present case) for each beetle. A formula for the binomial distribution can be derived by the rules of probability but we can use R to compute these conveniently, using the dbinom(x, size, prob) function. The plots below show all the outcomes (0 to 29) and their probabilities when size = 29 and with prob set to 0.35, 0.50 and 0.65. The code is shown only for the case when prob is 0.35. ggplot() + geom_col(aes(0:29, dbinom(0:29, 29, 0.35))) + geom_col(aes(15, dbinom(15, 29, 0.35)), fill = &#39;red&#39;) + labs(x = &#39;x&#39;, y = &#39;Probability&#39;) The idea of likelihood turns this perspective around. Instead of specifying the parameter of the model, namely the probability of success, and then considering the consequences in terms of the observations we may see, likelihood starts with the observed data and considers what parameter values might be consistent with that. In the plots above, the probability associated with the observed value of 15 is highlighted in red. If we assume that the true parameter is 0.35 then the probability of observing an outcome of 15 is very low and the same is true if we assume the true parameter is 0.65. The probability of observing 15 is much higher when we assume the true parameter to be 0.5. This gives a means of quantifying the relative plausibility of different parameter values. To use more scientific notation, the binomial probability for any outcome \\(k\\) for the number killed can be written as \\(pr(k; m, p)\\), where \\(pr\\) is a (mathematical) function which gives the probability of outcome \\(k\\) when the group size is \\(m\\) and the probability of success for an individual beetle is \\(p\\). So \\(pr\\) is a function of \\(k\\) for fixed values of \\(m\\) and \\(p\\). The likelihood function is simply \\(pr(k; m, p)\\) viewed as a function of \\(p\\), with \\(k\\) fixed at the observed value. (The parameter \\(m\\) is fixed by the design of the experiment.) To emphasise this change in perspective, the likelihood function is written as \\(L(p; k, m)\\) to indicate that \\(k\\) is fixed and the likelihood is a function of \\(p\\). p &lt;- seq(0.25, 0.75, length = 50) dfrm &lt;- data.frame(p, likelihood = dbinom(15, 29, p)) ggplot(dfrm, aes(p, likelihood)) + geom_line() The plot above shows the likelihood function. The maximum likelihood estimate of \\(p\\) is, as the name suggests, the value of \\(p\\) which maximises the likelihood function. Inspection of the plot shows this to be just above 0.5. In fact it is 0.517, the sample proportion of beetles killed. It is not a surprise that a good estimate of the proportion of beetles killed in the population at this concentration is the proportion of beetles killed in the sample. However, it is impressive that this has resulted simply by employing the concept of the likelihood function, without building in any further information. This strategy has supplied an obvious estimate in this simple case but it can also provide estimates in much more complex cases where it is not at all clear what might constitute a good estimate. "],["inference-1.html", "8.2 Inference", " 8.2 Inference "],["an-example.html", "8.3 An example", " 8.3 An example "],["other-approaches-to-inference.html", "9 Other approaches to inference ", " 9 Other approaches to inference "],["computational-inference.html", "9.1 Computational inference", " 9.1 Computational inference The problems we have discussed so far have been very simple once where theory can be worked out analytically to provide simple formulate and procedures. With more complex data structures, that can become much more difficult so this section will discuss some ways of approaching inference by computational means. The underlying principles are essentially the same but the method of implementation is different. 9.1.1 Simulation methods There are some circumstances where the model whose suitability we wish to assess is completely known, without any dependence on unknown parameters. A simple example arises from spatial point patterns where we may wish to assess the evidence that the point pattern is generated by a process which is not simply uniform. The plot below shows data on large (breast height diameter &gt;= 50cm) longleaf pine trees in a 200m x 200m sqaure of forest. library(spatstat) x &lt;- longleaf$x[longleaf$marks &gt;=50] y &lt;- longleaf$y[longleaf$marks &gt;=50] library(ggplot2) ggplot(mapping = aes(x, y)) + geom_point() We can generate our own trees patterns in a uniform random manner simply by locating each tree at an x and y position each of which is generated uniformly in the range (0, 200). here are some examples of patterns generated in this way. n &lt;- length(x) for (i in 1:3) { plt &lt;- ggplot(mapping = aes(runif(n, 0, 200), runif(n, 0, 200))) + geom_point() print(plt) } How should we compare the observed pattern with the randomly generated ones? A simple way of doing this is to measure for each tree the distance to its nearest neighbour. If the pattern exhibits clustering, then these distances should be small, while if the pattern is uniform they will tend to be larger. We might then use as our test statistic the average of the distances to each nearest neighbour. We can generate the distribution of this average distance in the case where the locations are uniformly distributed simply by repeatedly simulating locations and retaining the average nearest neighbour distance for each one. The nndist function from the spatstat package can help us with the nearest neighbour distances. nsim &lt;- 10000 tstat &lt;- rep(0, nsim) for (i in 1:nsim) { pp &lt;- cbind(runif(n, 0, 200), runif(n, 0, 200)) tstat[i] &lt;- mean(nndist(pp[ , 1], pp[ , 2])) } tstat.obs &lt;- mean(nndist(x, y)) ggplot(mapping = aes(tstat)) + geom_histogram() + geom_vline(xintercept = tstat.obs, col = &quot;red&quot;) length(tstat[tstat &lt; tstat.obs]) / nsim ## [1] 0.3116 The histogram shows the distribution (subject to the variation present through simulation) of the average nearest neighbour distance when of atoms really are uniform. The red line shows the average neighbour distance for the observed data. Informally, but clearly, we can see that from this perspective the observed data is entirely consistent with the assumption of uniformity. An empirical p-value can be computed simply from the proportion of simulated values which fall below the observed one (as we look for evidence against uniformity in small nearest neighbour distances). 9.1.2 Randomisation tests Randomisation is very important principle in designing experiments as it helps to overcome issues of bias. We will consider this in detail elsewhere. Randomisation can also be used as the basis for analysis when we are reluctant to make the normality and independence assumptions which underlie standard methods. The dataset below provides a very simple example of this it refers to an experiment to assess the possibly different effects of two fertilisers applied to tomato plants growing in a row. (This example comes from the book Statistics for Experimenters by Box, Hunter and Hunter.) Row position 1 2 3 4 5 6 7 8 9 10 11 Fertiliser A A B B A B B B A A B Yield (pounds) 29.9 11.4 26.6 23.7 25.3 28.5 14.2 17.9 16.5 21.1 24.3 If the positions the fertilisers in the row have been randomised, and if the two fertilisers A and B are in fact equivalent in the effects, then the labels attached to the observations are immaterial. There is no connection between the labels and the outcomes. This allows us to calculate the distribution of the difference between the mean yields of A and B which is generated by the randomisation procedure. The histogram below shows this. For convenience (indeed, laziness!) it has been constructed from random permutations rather than the systematic list of all possible randomisations. fertiliser &lt;- c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;A&quot;, &quot;A&quot;, &quot;B&quot;) yield &lt;- c(29.9, 11.4, 26.6, 23.7, 25.3, 28.5, 14.2, 17.9, 16.5, 21.1, 24.3) nperm &lt;- 1000 tstat &lt;- rep(0, nperm) for (i in 1:nperm) { smp &lt;- sample(fertiliser) tstat[i] &lt;- diff(tapply(yield, smp, mean)) } tstat.obs &lt;- diff(tapply(yield, fertiliser, mean)) ggplot(mapping = aes(tstat)) + geom_histogram() + geom_vline(xintercept = tstat.obs, col = &quot;red&quot;) There are several new functions used in the R code here (c, diff, tapply). You will hopefully feel ready to explore these through the help system by this stage. The end result is a histogram which represents the distribution, induced by randomisation, of the mean sample difference between A and B when the two treatments have identical effects. When we compare this with the observed mean difference it is clear that the two are compatible. So there is no evidence that A and B have different effects. If required, a p-value could be obtained by the means discussed in the simulation test section above. However, the conclusion is already clear. 9.1.3 The bootstrap We generally regard observed data as having been generated by some underlying distribution. If we construct an estimator of some quantity of interest, such as a population mean, then we need to know how accurate estimate is as a guide to the true value. The simple but surprising idea behind the bootstrap is that we mimic the variation of the sample around the distribution by the variation of resampled datasets around our original sample. When the bootstrap was first proposed in the 1970s, it took people by surprise but in fact it can be shown to be well founded theoretically. Suppose we are dealing with a distribution which has long tails. In other words, we may find that outliers (unusually large or small observations) are often present in sample datasets. Methods based on the assumption of a normal distribution will be strongly affected by this. A simple device is to compute a trimmed mean. This simply computes the main of the remaining data after a proportion of the highest and lowest observations has been removed. This is an attractive way of defending against the effect of long tails, but how do we quantify the uncertainty of our sample estimator as a guide to the true trimmed mean of the underlying distribution? The bootstrap takes the observed sample and resamples the data with replacement. A trimmed mean is calculated on each resample. In the simplest approach to construction of a confidence interval, this is generated simply through percentiles of the trimmed means generated by resampling. The example below shows the mechanics of this, using a simple normal random sample to represent the observed data. x &lt;- rnorm(25) tm.obs &lt;- mean(x, trim = 0.1) nboot &lt;- 10000 tmdiff &lt;- rep(0, nboot) for (i in 1:nboot) tmdiff[i] &lt;- mean(sample(x, replace = TRUE), trim = 0.1) - tm.obs tm.obs - quantile(tmdiff, c(0.975, 0.025)) ## 97.5% 2.5% ## -0.3548995 0.4030490 "],["bayesian-inference.html", "9.2 Bayesian inference", " 9.2 Bayesian inference The methods which have been described in this course have a very long and well established history. They are in widespread use and have brought insight to an enormous array of applications across the board. From a philosophical point of view these methods are referred to as frequentist. The name comes from the idea of continued sampling of data, under the same conditions, for which it can be shown that relative frequencies converge eventually to probabilities. An alternative approach to inference is to consider the uncertainty associated with quantities of interest such as parameters, expressed not only through a model for the observed data but also through a prior distribution for the unknown parameters. Sometimes that prior information can be informative, based on knowledge or data about the situation being modelled. Sometimes the prior information can be vague or uninformative. Bayesian thinking allows uncertainty to be expressed as a combination of these two things, in a posterior distribution. A major step forward took place in the 1990s when computational methods of deriving posterior distributions were developed. For the first time this allowed complex problems to be handled by Bayesian models, with the attractive property that uncertainty at different levels can be propagated to generate posterior uncertainty about the quantities of interest, incorporating all the appropriate sources of uncertainty. This has led to extremely powerful tools for complex systems. Treatment of this topic is beyond the scope of the current course. "],["exercises-6.html", "9.3 Exercises", " 9.3 Exercises 9.3.1 Coverage property of bootstrap confidence intervals During the session an example of a confidence interval for a trimmed mean was discussed. Write some code to evaluate the coverage of this method of constructing a confidence interval. You can do this by repeatedly simulating sample from the true distribution (rnorm(25)), constructing a bootstrap confidence interval using the code discussed, and counting how many times the calculated confidence interval covers the true value (0). "],["linear-models.html", "10 Linear models", " 10 Linear models Linear models provide a very powerful framework for investigating the relationship between a variable whose behaviour we would like to understand and a set of variables which may be linked to that behaviour. This general problem is referred to as regression and linear models refer to the case where our response variable is on a continuous scale and we employ straight line relationships to describe the relationships with the explanatory variables. This approach will be extended to other types of data in later chapters. "],["simple-regression-rodents-revisited.html", "10.1 Simple regression: rodents revisited", " 10.1 Simple regression: rodents revisited Before embarking on a general description of linear models, it is helpful to consider this first in its simplest setting. We will do this in the context of the rodent data, where the discussion in Chapter 2 led to the adoption of log scales and the exclusion of the porcupine as preliminary parts of the process of building a model. A suitable dataframe can be created as rodent1 &lt;- dplyr::filter(rodent, rownames(rodent) != &quot;North American Porcupine&quot;) We then proposed a straight line model to describe the connection between log(Mass) and log(Speed). The process of fitting the model involves quantifying, for any particular line we might consider, how well this line captures the pattern in the data. As we saw in the earlier discussion, the lm function will handle all the calculations for us and create a model object which we can keep for later use. model &lt;- lm(log(Speed) ~ log(Mass), data = rodent1) An interactive exploration of this is available through the rp.lm function in the rpanel package. rp.lm(log(Speed) ~ log(Mass), data = rodent1) A static plot is shown below. Red lines indicate the deviations of the data from the model and the sum-of-squares of these deviations measures how close the model is to the observed data. The fitted model corresponds to the straight line which minimises this sum-of-squares. This is referred to as the principle of least squares. The problem can be expressed and solved in a general way, giving simple formulae for the slope and the intercept of the fitted line. These are estimates of the slope and intercept in the population which lies behind the data we have observed. The model formula given to the lm function provides a very powerful means of specifying the model we wish to fit. log(Speed) ~ log(Mass) The response variable whose behaviour we wish to investigate is placed on the left-hand side of the ‘tilde’ sign ~ and the covariate, or explanatory variable or predictor variable on the right hand side. (The particular terminology we use for the variable on the right-hand side may be influenced by the nature of our data and the problem we are tackling.) In more mathematical notation, and using general notation for the response \\(y\\) and the covariate \\(x\\), this corresponds to the formula \\[ y = \\beta_0 + \\beta_1 x + \\varepsilon , \\] where the parameters \\(\\beta_0\\) and \\(\\beta_1\\) refer to the intercept and slope of the linear model and \\(\\varepsilon\\) describes the random variation present in each observation. One the model has been fitted, there is an immediate opportunity to assess whether it provides a good description of the data. With the rodent data, a visual inspection suggests that the fitted linear model does indeed capture the trend adequately. The individual data points show considerable variation around the fitted line and a description of this is also part of the modelling process. The deviations of the points from the fitted model are called the residuals. Essentially, we calculate the standard deviation of these residuals (with a slight technical modification) to estimate the variation of the observations around the model. The estimates of intercept and slope are usually of principal interest and these are shown below. The value given for log(Mass) is the estimated slope of the fitted line. The interpretation of the value 0.142 is that for every increase of one unit on the log(Mass) scale the position on the fitted line will increase by 0.142 on the log(Speed) scale. The value given for the intercept is of less interest as it refers to where the fitted line lies when log(Mass) takes the value 0, which is not of practical interest in the rodent context. summary(model)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.989936 0.15518074 19.267444 2.199202e-14 ## log(Mass) 0.142214 0.05858014 2.427683 2.475992e-02 The theory of linear models provides expressions for the standard errors of the estimated slope and intercept parameters and these are implemented in the lm to provide the standard errors, t-statistics (‘t value’) and p-values (‘Pr(&gt;|t|)’) shown above. Our main question of interest in this example is whether log(Speed) and log(Mass) are linked and the p-value (0.025) is sufficiently small to indicate that the hypothesis of no link (a slope of 0) is implausible. A confidence interval for the slope parameter would provide an alternative route to the same conclusion. An interesting question here is: what is the population from which our data have been sampled? Most the quadrupedal rodents we know of have been tabulated in the dataset. One view is that the data are sampled from the evolutionary process which has allowed these animals to adapt to their environment. In other evolutionary circumstances we could have observed different rodents. The rodent data is a very simple example, involving only one response and one covariate, but it nonetheless illustrates some of the key steps in building and using a model. These include: model formulation; model fitting; model checking; model analysis; This general pattern provides a helpful guide in constructing models in more complex settings, as illustrated by the discussion in the following section. "],["linear-models-with-multiple-covariates.html", "10.2 Linear models with multiple covariates", " 10.2 Linear models with multiple covariates Investigations are rarely as simple as involving a single response and a single explanatory variable. Here is a more complex example which will be used throughout much of the rest of this chapter to illustrate the general principles behind linear models. Example: Dissolved oxygen in the River Clyde The Scottish Environment Protection Agency (SEPA) has a statutory obligation to monitor the state of the environment. As part of that duty, water samples are taken regularly from sampling stations along the Clyde River. Data are available for a twenty year period from the mid-1970’s until the mid-1990’s. A natural measure of water quality is dissolved oxygen (DO). There is interest in identifying the pattern of DO along the river, the nature of any time trends and the relationship between DO and physical variables such as temperature and salinity. We will read the data using the load function. This loads into our workspace objects which have previously been created and stored in a file in R format, using the save function. In this case, the file contains a single object - a dataframe called clyde. To keep things simple, we will start with data from a single station which lies 6 miles from the city centre. path &lt;- rp.datalink(&quot;DO_Clyde&quot;) load(path) clyde.sub &lt;- filter(clyde, Station == 6) 10.2.1 Model formulation As ever, creating a helpful plot is a good start. Note that, when applied to a dataframe, the plot function is an efficient way of creating a scatterplot matrix which displays the relationships between all pairs of variables. Here, the variables are ordered to ensure that the response variable DO is conveniently placed first and that Station is excluded as it is redundant for this subset of the data. plot(clyde.sub[, c(4, 2, 3, 5, 6)]) The top row of plots show how DO relates to the other variables. There is little evidence of change in mean DO across Year, a strong seasonal effect with a cyclical pattern across Doy and clear reductions in mean DO with increasing values of Temperature and Salinity. It is also worthwhile inspecting the other plots in the array, where a very strong relationship between day of the year and temperature is apparent. The temperature measurement are taken from the water so they have much greater stability than measurements made from the air, which would exhibit much more variability. The relationship between day of the year and temperature is so strong that there is a sense in which they essentially carry the same information. At any particular time of the year the water temperature can be predicted quite accurately. There is therefore a strong argument that only one of these two variables should be included in a model. Temperature is a good candidate, as its relationship with DO is simpler, with a broadly linear pattern. This suggests that an initial model might include the two covariates Temperature and Salinity and that the relationships of these with DO might be described well in linear form. That can be expressed in the model formula: DO ~ Temperature + Salinity Linear models can be applied in a very wide variety of settings so it is worthwhile considering a general expression which can be adapted to any of the particular situations and contexts we meet. A linear model which relates a response variable \\(y\\) to a set of covariates \\(x_1, \\ldots, x_p\\) can be written as: \\[ y = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\ldots + \\beta_p x_{p} + \\varepsilon . \\] The terms involving the parameters \\(\\beta_0, \\ldots, \\beta_p\\) for the systematic or structural part of the model. They describe what happens to the mean value of \\(y\\) as the explanatory variables change. The parameters \\(\\beta_j (j = 1, \\ldots, p)\\) can be interpreted as the amount of change in the mean value of \\(y\\) when \\(x_j\\) increases by one unit and the other explanatory variables are held fixed. The error term \\(\\varepsilon\\) reflects the fact that data are almost always subject to variation, from natural processes, measurement error and other sources. 10.2.2 Model fitting Continuing with the general formulation, the model can be expressed in terms of the observed data: \\[\\begin{eqnarray*} &amp; y_1, x_{11}, x_{21}, \\ldots, x_{p1} \\\\ &amp; y_2, x_{12}, x_{22}, \\ldots, x_{p2} \\\\ &amp; \\ldots\\\\ &amp; y_n, x_{1n}, x_{2n}, \\ldots, x_{pn} \\end{eqnarray*}\\] as: \\[ y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_p x_{pi} + \\varepsilon_i . \\] where the subscript \\(i\\) refers to the \\(i\\)th of the \\(n\\) observations in the sample. To fit the model, the aim is to choose the values of the parameters which bring the structural part of the model, \\(\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_p x_{pi}\\), as close as possible to the observed data, \\(y_i\\). A natural way to measure closeness is through the sum-of-squares: \\[ \\sum_{i=1}^n \\{y_i - (\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\ldots + \\beta_p x_{pi}) \\}^2 . \\] The principle of least squares chooses the values of \\(\\beta_0, \\ldots, \\beta_p\\) which minimise this sum-of-squares. The minimising values are the least squares estimates, denoted by \\(\\hat{\\beta}_0, \\ldots, \\hat{\\beta}_p\\). The ‘hat’ notation indicates that these are estimates of the true, underlying values \\(\\beta_0, \\ldots, \\beta_p\\). There is a great deal of well developed theory which is available to find the estimates efficiently. Our initial model for the DO data can be fitted through the lm function as: model &lt;- lm(DO ~ Temperature + Salinity, data = clyde.sub) summary(model)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.4417324 0.34234313 33.421826 7.707445e-71 ## Temperature -0.2888456 0.03542437 -8.153866 1.380703e-13 ## Salinity -0.4007846 0.03634180 -11.028199 5.117585e-21 The fitted linear model can be written as \\[ y = \\hat{\\beta_0} + \\hat{\\beta_1} x_{1} + \\hat{\\beta_1} x_{2} . \\] which corresponds to a plane in 3D. This plane has been superimposed onto a 3D plot of the data below. The plane slopes in both the Temperature and the Salinity axes so the effects of these two variables are described simultaneously. The green vertical lines show the discrepancies between the observed data (red points) and the fitted model. The points at which these lines meet the plane are referred to as the fitted values, defined as \\[ \\hat{y}_i = \\hat{\\beta_0} + \\hat{\\beta_1} x_{1i} + \\hat{\\beta_1} x_{2i} . \\] The discrepancies between the data and the fitted model, referred to as residuals are then \\[ r_i = y_i - \\hat{y}_i . \\] The plot below uses the rp.lm function from the rpanel package to illustrate the fitted model. This function is designed to illustrate and explore various issues of linear models in simple cases where there are no more than two predictor variables. Here the three-dimensional display can be rotated in the web version of the book or in R on your own computer. The model corresponds to a plane which tilts in both the Temperature and Salinity directions. The coefficients discussed above give the rate of tilt. Vertical lines for the residuals have also been added. rp.lm(DO ~ Temperature + Salinity, data = clyde.sub, residuals.showing = TRUE) 10.2.3 Model checking Before we rush into using our fitted model to draw conclusions, it would be wise to check that it does indeed fit the data well, providing a good description of the observed data. This model has a number of potential assumptions and it is helpful to make these explicit. The relationship between the mean value of \\(y\\) and each \\(x_j\\) is linear (a straight line) if the other explanatory variables are held fixed. A consequence is that the error term \\(\\varepsilon\\) is assumed to have mean \\(0\\). In order to draw useful conclusions it is very helpful to make an assumption about the distribution of the error term. This is usually assumed to be Normal. A further convenient assumption is that the variance of the error term is the same for all observations and indeed at any future observations with different values of the explanatory variables. Another assumption which it is helpful to make is that the error terms are independent. This means that knowing the value of one particular value \\(\\varepsilon_i\\) does not give us any information about any other value \\(\\varepsilon_j\\). Notice that all four of the model assumptions listed above can be expressed as statements about the error terms \\(\\varepsilon_i\\). We have fitted values \\(\\hat{y}_i\\) and residuals \\(\\hat{\\varepsilon}_i\\) \\[\\begin{eqnarray*} \\hat{y}_i &amp; = &amp; \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1i} + \\ldots + \\hat{\\beta}_p x_{pi} \\\\ \\hat{\\varepsilon}_i &amp; = &amp; y_i - \\hat{y}_i . \\end{eqnarray*}\\] If the assumptions are valid then a plot of the residuals against the fitted values should show only random variation, without any systematic patterns in either location or scale. plot(model) A normal probability plot is designed to show an approximately straight line when data are normally distributed and so it can be applied to the residuals to check normality. There are one or two issues to consider here. The plots of residuals versus fitted values may suggest a little curvature. There are also one or two observations which are unusual. We may need to revisit these later. 10.2.4 Inference on coefficients Since the estimates are derived from the observed data, they are themselves influenced by the random variation on the data. It is extremely useful to measure the precision of the estimates through their estimated standard deviations, known in this context as the standard errors. When we ask for a summary of a fitted model, this will produce a variety of summary information, including a table of parameter estimates and standard errors, as in the table printed above. Armed with an estimate \\(\\hat{\\beta}_j\\) of the parameter \\(\\beta_j\\), plus a standard error \\(s.e.(\\hat{\\beta}_j)\\), an approximate 95% confidence interval for \\(\\beta_j\\) can be constructed as \\((\\hat{\\beta}_j \\pm 2 \\ s.e.(\\hat{\\beta}_j))\\), with the more precise version: \\[ (\\hat{\\beta}_j \\pm t_{n-(p+1); 0.975} \\ s.e.(\\hat{\\beta}_j)). \\] where \\(df\\) denotes the degrees of freedom, namely the numbers of parameters, of the model. The confidence interval on the left is a rather rough and ready, approximate one, using two standard errors. The interval on the right provides a more accurate version, where \\(t(n-df; 0.975)\\) denotes the upper percentile of the \\(t_{n-df}\\) distribution beyond which lies probability \\(0.025\\). A confidence interval can be interpreted informally as a range of plausible values for the true, but unknown, parameter. The summary function provides useful information on the estimated parameters of the model, among other things. The confint function in `R is a useful way of producing confidence intervals. summary(model)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.4417324 0.34234313 33.421826 7.707445e-71 ## Temperature -0.2888456 0.03542437 -8.153866 1.380703e-13 ## Salinity -0.4007846 0.03634180 -11.028199 5.117585e-21 confint(model) ## 2.5 % 97.5 % ## (Intercept) 10.7652204 12.1182443 ## Temperature -0.3588485 -0.2188427 ## Salinity -0.4726005 -0.3289687 Confidence intervals are of immediate use because hypotheses of interest can often be formulated as simple statements about parameters. For example, if variables \\(x_j\\) is unrelated to the mean value of \\(y\\) then this corresponds to \\(\\beta_j = 0\\). If a confidence interval for \\(\\beta_j\\) does not contain the value \\(0\\) then there is convincing evidence that the term involving \\(\\beta_j\\) does contribute to the model. However, if the confidence interval does contain the value \\(0\\) then we would have justification for removing this term from the model. There are no surprises in the confidence intervals for Temperature and Salinity as it was clear from the intial plots that these variables have a strong relationship with DO. It is more interesting to investigate possible effects of Year now that the strong effects of other variables have been accounted for. model_with_year &lt;- lm(DO ~ Year + Temperature + Salinity, data = clyde.sub) summary(model_with_year)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -188.5669174 35.78842019 -5.268937 4.795096e-07 ## Year 0.1008971 0.01805327 5.588854 1.078251e-07 ## Temperature -0.3199249 0.03275569 -9.767004 1.124808e-17 ## Salinity -0.3686144 0.03361273 -10.966511 8.037259e-21 confint(model_with_year, &#39;Year&#39;) ## 2.5 % 97.5 % ## Year 0.06521962 0.1365746 Interestingly, there is now evidence of an increase in DO with Year. The plot below helps in understanding what is happening here. As we cannot easily view the effects of three variables simultaneously, we will consider a model with Year and Temperature for illustration. If the plot below is rotated to show DO against Year on the front face this replicates the scatterplot at the start of the chapter, with huge variation and little evidence of a relationship evident. Rotating the plot shows the large amount of variation which is accounted for by Temperature, allowing a small but noticeable effect of Year to be seen. rp.lm(DO ~ Year + Temperature, data = clyde.sub, residuals.showing = TRUE) An equivalent analysis involves the construction of the quantity \\[ \\hat{\\beta}_j / s.e.(\\hat{\\beta}_j) , \\] known as the t-statistic. This is used in a formal test with the null hypothesis that \\(\\beta_j = 0\\). If the t-statistic is greater than \\(t(n-df; 0.975)\\) (or the more rough-and-ready value \\(2\\)) then we have significant evidence that this term should be retained in the model. The small p-value for Year can be seen in the table of coefficients above. This is simply a re-expression of the conclusions drawn from the confidence interval. 10.2.5 Comparing models It is often convenient to compare two models which provide competing descriptions of the data. The smaller model, containing a subset of the terms of the larger one, is referred to as the null hypothesis. A hypothesis test can be carried out by comparing the residual sum-of-squares \\[ RSS = \\sum_{i=1}^n \\{y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1i} + \\hat{\\beta}_2 x_{2i} + \\ldots + \\hat{\\beta}_p x_{pi}) \\}^2 \\] for each model. The RSS quantifies the degree to which the model fits the observed data. For two `nested’ models with residual sums-of-squares \\(RSS_0\\) and \\(RSS_1\\), a hypothesis test can be carried out by referring the F-statistic \\[ F = \\frac{(RSS_0 - RSS_1) / (df_1 - df_0)}{RSS_1 / (n - df_1)} \\] to an \\(F_{df_1 - df_0, n - df_1}\\) distribution. Here \\(df_0\\) and \\(df_1\\) denote the degrees of freedom, namely the numbers of parameters, of the two models. model1 &lt;- lm(DO ~ Temperature + Salinity, data = clyde.sub) model2 &lt;- lm(DO ~ Year + Temperature + Salinity, data = clyde.sub) anova(model1, model2) ## Analysis of Variance Table ## ## Model 1: DO ~ Temperature + Salinity ## Model 2: DO ~ Year + Temperature + Salinity ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 148 363.18 ## 2 147 299.53 1 63.646 31.235 1.078e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The very small p-value suggests that, after adjusting for Temperature and Salinity, there is still information in Year which contributes to the explanation of the patters in DO. (This confirms what we saw from the table of regression coefficients earlier.) "],["prediction-black-cherry-trees.html", "10.3 Prediction: Black Cherry trees", " 10.3 Prediction: Black Cherry trees Example: Black cherry trees Data are available from 31 Black Cherry trees. The data file consists of three variables: Girth diameter in inches at 4.5 feet above ground level Height tree height in feet Volume volume of timer in cubic feet The aim is to investigate how well we can predict the volume of wood in each tree trunk from simple measurements which can be made near the ground. The dataframe trees is available in R. Actually, the variable Girth does not really contain girth measurements. We should begin by renaming this variable. Plot the data to explore how well Volume can be predicted from the other two measurements. Can you build a model which gives good predictions? We begin by renaming the Girth variable, as it is really Diameter. Setting the Girth to NULL will remove it. trees$Diameter &lt;- trees$Girth trees$Girth &lt;- NULL A pairs plot gives us a useful overall view. It would be neater to place the Volume variable first, so that it appears in the top row. The square bracket syntax allows us to put the columns of the dataframe in a different order. It is also useful to apply a log transformation. Can you see why? (And what more appropriate transformation could there be for trees? :-)) pairs(log(trees)[ , c(2, 1, 3)]) Now we fit a regression model model &lt;- lm(log(Volume) ~ log(Diameter) + log(Height), data = trees) summary(model)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.631617 0.79978973 -8.291701 5.057138e-09 ## log(Diameter) 1.982650 0.07501061 26.431592 2.422550e-21 ## log(Height) 1.117123 0.20443706 5.464388 7.805278e-06 This model can also be explored visually through the rp.lm function. Although Diameter and Height both carry information which can help in predicting the response (as we can see through the confidence intervals), the predictive power of the model needs to be considered. The coefficient of determination, usually denoted by \\(R^2\\), measures the proportion of variation in the response variable which is explained by the explanatory variables in the model. One measure of total variation in the response variable is \\[ TSS = \\sum_{i=1}^n (y_i - \\bar{y})^2 . \\] This is equivalent to fitting a model which has no explanatory variables - it only has an intercept, estimated by \\(\\\\bar{y}\\). The variation which remains after including the explanatory variables in the model is simply the residual-sum-of-squares. \\[ RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 , \\] where \\(\\hat{y}_i\\) denotes the ‘fitted value’ for the \\(i\\)th observation. \\(R^2\\) is then defined as the proportionate drop in variation explained by these two models, namely \\[ R^2 = \\frac{TSS - RSS}{TSS} \\] \\(R^2\\) can be adjusted to account for different numbers of explanatory variables in different models being compared. modelD &lt;- lm(log(Volume) ~ log(Diameter), data = trees) modelH &lt;- lm(log(Volume) ~ log(Height), data = trees) summary(modelD)$r.squared ## [1] 0.9538743 summary(modelH)$r.squared ## [1] 0.4207309 The model which uses Diameter alone is almost as good as that of the model which uses both. There is therefore a strong argument, for practical reasons, to use Diameter information alone, as it is easy to measure. "],["factors-and-interactions-in-linear-models.html", "10.4 Factors and interactions in linear models", " 10.4 Factors and interactions in linear models Example: Cholestorol in women in Iowa and Nebraska report a subsample of measurements from an early study of health and nutrition by , with a particular focus on cholesterol. Women were recruited from both Iowa and Nebraska and there is interest in whether there is any evidence of a systematic difference between the groups. If there is no evidence of a difference, the groups might reasonably be pooled. An upward trend in cholesterol with age is expected. The data are available in the cholesterol dataframe in the rpanel package. The variables recorded are Age, Cholesterol and Location. Sometimes datasets include grouping variables or factors, consisting of codes which identify to which of several possibe groups each observation belongs. In the example described above, the Location variable consists of only two groups, coded as Iowa or Nebraska. It turns out that, if we use a suitable coding, this kind of variable can also be expressed in a linear model formulation. We might write the model for observation \\(i\\) as: \\[ \\mbox{Cholesterol}_i = \\beta_0 + \\beta_1 \\mbox{Age}_i + \\beta_2 z_i + \\varepsilon_i \\] where the variable \\(z_i\\) takes the value \\(0\\) if Location is Iowa and \\(1\\) if it is Nebraska. The parameter \\(\\beta_2\\) then describes the shift in cholesterol level from Iowa to Nebraska. As Location has the special property of a factor, this can be expressed in a model formula as: Cholesterol ~ Age + Location and R will construct the appropriate 0/1 coding for Location. The rp.lm function can launch an interactive exploration of this. The fitted model is shown in the left hand plot below. rp.lm(Cholesterol ~ Age + Location, data = cholesterol) However, there is another possibility to consider. It could be that the slope of the regression line with Age is also different in Iowa and Nebraska. The model for observation \\(i\\) should then be written as: \\[ \\mbox{Cholesterol}_i = \\beta_0 + \\beta_1 \\mbox{Age}_i + \\beta_2 z_i + \\beta_3 Age_i z_i + \\varepsilon_i \\] When \\(z_i\\) is \\(1\\), this adds an adjustment to the slope with Age for Nebraska only. The model formula for this is written as: Cholesterol ~ Age * Location and the fitted model is shown in the right hand panel above. This is referred to as an interaction between Age and Location. A useful general definition of an interaction is that: An interaction between two explanatory variables occurs when the effect of one variable on the response depends on the setting of the other variable. In the present case, the effect of Age depends on the setting of Location because an adjustment to the slope is included when the subject is from Nebraska. To examine whether this more complex model is needed, we can examine the coefficients: model &lt;- lm(Cholesterol ~ Age * Location, data = cholesterol) summary(model)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 35.8112138 55.116605 0.6497355 0.521562661 ## Age 3.2381449 1.008827 3.2098104 0.003516155 ## LocationNebraska 65.4865523 61.983368 1.0565181 0.300450053 ## Age:LocationNebraska -0.7177069 1.162845 -0.6171990 0.542471382 The interaction term Age:LocationNebraska has a p-value of 0.54 so we may reasonably adopt the model without the interaction term. This is referred to as the additive model. An examination of that one shows that the shift from Iowa to Nebraska (LocationNebraska) is also not signficantly different from \\(0\\), so there is some justification for pooling the groups. model &lt;- lm(Cholesterol ~ Age + Location, data = cholesterol) summary(model)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 64.489772 29.3024531 2.200832 3.648161e-02 ## Age 2.697967 0.4959532 5.439963 9.361235e-06 ## LocationNebraska 28.651025 16.5409806 1.732124 9.466292e-02 The rp.lm function in interactive mode displays a model lattice, sometimes also referred to as a Hasse diagram. This lays out all the linear models we can consider for this situation. Clicking on a particular node will fit and display the model. Displays of the coefficients and model comparisons are also available. This is a simple example of comparing regression lines for two groups. The chapter on ‘Issues in Experimental Design’ describes further uses of factors in linear models. "],["transforming-the-response-variable-giving-in-the-church-of-england.html", "10.5 Transforming the response variable: Giving in the Church of England", " 10.5 Transforming the response variable: Giving in the Church of England As an example, we will look again at the cofe data. The rp.wrangle function from the rpanel package provides a convenient way of recreating the collated data, using the code discussed earlier in Chapter 3. When given a dataframe, the plot function produces a scatterplot matrix which displays all possible scatterplots of two variables in a neat layout. As we saw earlier, the effects of Attachment and IMD on Giving do not look strong. The strongest association is actually between the two covariates, Attachment and IMD! library(rpanel) cofe_2019 &lt;- rp.wrangle(&#39;cofe_2019&#39;) plot(cofe_2019[ , c(&quot;Giving_per_member&quot;, &quot;Attachment&quot;, &quot;IMD&quot;)]) model &lt;- lm(Giving_per_member ~ Attachment + IMD, data = cofe_2019) summary(model)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 877.27025 125.336466 6.999322 2.840637e-08 ## Attachment -16437.88039 4902.285513 -3.353105 1.854021e-03 ## IMD -12.92725 3.399288 -3.802929 5.184416e-04 The fitted linear model can be written as \\[ y = \\hat{\\beta_0} + \\hat{\\beta_1} x_{1} + \\hat{\\beta_1} x_{2} . \\] which corresponds to a plane in 3D. This plane has been superimposed onto a 3D plot of the data below. The green vertical lines show the discrepancies between the observed data (red points) and the fitted model. The points at which these lines meet the plane are referred to as the fitted values, defined as \\[ \\hat{y}_i = \\hat{\\beta_0} + \\hat{\\beta_1} x_{1i} + \\hat{\\beta_1} x_{2i} . \\] The discrepancies between the data and the fitted model, referred to as residuals are then \\[ r_i = y_i - \\hat{y}_i . \\] rp.lm(Giving_per_member ~ Attachment + IMD, data = cofe_2019, panel = FALSE, residuals.showing = TRUE) There are some concerns about how well the model fits. We might consider whether we have a suitable scale of measurement for giving. This can be investigated more systenatically by considering all possible ‘power transformations’ of \\(y\\). If these transformations are suitably scaled then the \\(log\\) transformation corresponds to power 0. The boxcox function from the MASS package implements this using the very powerful principle of likelihood. This gives a strong indication that the log transformation is appropriate. model &lt;- lm(Giving_per_member ~ Attachment + IMD, data = cofe_2019) library(MASS) boxcox(model) Model checking plots for the new model show some improvement. model &lt;- lm(log(Giving_per_member) ~ Attachment + IMD, data = cofe_2019) plot(model) The plot below displays the model we have now reached. However, before rushing into drawing conclusions, it would be helpful to understand what the model is doing. with(cofe_2019, rp.regression(cbind(Attachment, IMD), Giving_per_member, model = &quot;Attachment and IMD&quot;, residuals.showing = TRUE)) Before we get to interpretation, it would also be wise to consider whether the model is actually a good description of the data. What are residuals? Why are the useful? Model checking. plot(model) How well does the model fit? Do we have a suitable scale of measurement for giving? model &lt;- lm(Giving_per_member ~ Attachment + IMD, data = cofe_2019) library(MASS) boxcox(model) model &lt;- lm(log(Giving_per_member) ~ Attachment + IMD, data = cofe_2019) plot(model) Move to log giving. Further plots and summary. summary(model)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.26552184 0.325300362 22.334810 4.509678e-23 ## Attachment -47.19325370 12.723473891 -3.709148 6.794607e-04 ## IMD -0.03410173 0.008822568 -3.865284 4.325664e-04 Neither explanatory variable is significant on its own. The influence of one variable becomes apparent only after adjustment for the other. (Very strong with this new dataset?) model1 &lt;- lm(log(Giving_per_member) ~ Attachment, data = cofe_2019) model2 &lt;- lm(log(Giving_per_member) ~ IMD, data = cofe_2019) summary(model1)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.14451 0.1722563 35.670740 7.951727e-31 ## Attachment -18.33212 12.0445039 -1.522032 1.362799e-01 summary(model2)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.20664603 0.180248229 34.433881 2.922882e-30 ## IMD -0.01489757 0.008256136 -1.804424 7.909529e-02 This is not about prediction. R-squared etc. Other covariates highly correlated with Attachment. Collinearity. Look at vif in the car package. Look at the car package in general. "],["simple-seasonal-terms.html", "10.6 Simple seasonal terms", " 10.6 Simple seasonal terms "],["exercises-7.html", "10.7 Exercises", " 10.7 Exercises 10.7.1 Rodents Write some code in ggplot to plot the rodent data on log scales, omitting the porcupine, with a fitted regression line and with vertical lines to indicate the residuals. This should recreate the plot shown at the start of this chapter. Use the rp.lm function to interact with model fitting on the rodent data, including the porcupine. One of the checkboxes activates the ability to move data points vertically, by clicking on and dragging an observation. Investigate the influence of the porcupine in the fitted model. 10.7.2 Giving in the Church of England These data record the average annual giving in pounds per church member in the dioceses of the Church of England in the early 1980’s. Three potentially relevant covariates are also recorded for each diocese. Giving: the average annual giving in pounds per church member; Employ: the percentage of the population who are employed; Elect: the percentage of the population on the electoral roll of the church; Attend: the percentage of the population who usually attend church. Background details are available in Pickering (1985; Applied Economics 17, 619-32). Try including different explanatory variables in the model. Which variables are significant if they are included individually? If you include those which are significant on their own, are they significant together? Can you see what is going on? The data are available in the dataframe CofE in the rpanel package. 10.7.3 Black Cherry trees We looked at this example in the lecture. We might think of approximating each tree trunk by a cylinder. Geometry then suggests that the volume of wood could be approximately proportional to \\(Height * Diameter^2\\), using the formula for the volume of a cylinder or a cone. Consider what this means on a log scale. Can you use the fitted regression model to determine whether this model captures the essential relationship? Hint: If it does, the coefficients of Height and Diameter should be 1 and 2 respectively. 10.7.4 Cats In the discussion of linear models, with the Clyde data, we saw how to include a factor, and interactions. Look again at the cats data, available in the MASS package, and fit a model which has Hwt as response variable, Bwt as explanatory variable and Sex as a factor. What does this model tell you? 10.7.5 Respiratory distress syndrome Premature babies often suffer from a variety of problems, of which a major one is respiratory distress syndrome (RDS). It is thought that the occurrence of this syndrome might be related to a property of the blood called red cell deformability. This refers to the ability of red cells to change shape to pass through small pores. The rate (lrate, on a log scale) of blood flow through a set of three micrometre pores was recorded for two groups (group) of babies, some of whom suffered from respiratory distress syndrome and some who did not. The gestational age (GA) in weeks of each baby was also recorded. The data were kindly provided by Dr. Haider, Queen Mother’s Hospital, Glasgow. They are available in the dataset rds (with header) through the rp.datalink function. Plot the data by whatever means you wish, to examine whether there is a difference in lrate between the two groups. Now plot lrate against GA, using different colours and or plotting symbols to distinguish between the two groups. (You might use the col and pch arguments of the plot function. The details of these, and a very large number of other arguments, are available through ?par.) What does this suggest? The lm function fits a simple linear regression, for example as model &lt;- lm(lrate ~ GA). The slope and intercept of the fitted regression line are then available in model$coefficients. Can you work out how to draw lines on your plot to correspond to two linear regressions, one for each group? The abline function may be useful. 10.7.6 The Clyde data The Scottish Environment Protection Agency regularly record the water quality, in terms of dissolved oxygen (DO) on a percentage scale, at a number of sampling stations (Station, recorded as miles from the city centre) on the River Clyde. Data are available in the Clyde dataframe in the rpanel package, along with date (Day, Month, Year), the day of the year (Doy between 1 and 365) and an identifier (id) of the survey on which each measurement was made. Select a particular station and build a model for the relationship between DO and the variables Temperature, Salinity and Year. Repeat this analysis at other stations of your choice, to see whether the relationships you have identified are consistent at different places in the river estuary. In 1985 the Shieldhall sewage works, which lies approximately two miles from the city centre, was upgraded. Is there any evidence of a resulting improvement in the water quality? 10.7.7 Water quality in Loch Leven Loch Leven is situated in lowland Scotland in the Perth and Kinross area. It is the largest shallow lake in Great Britain with an area of 13.3km\\(^2\\), mean depth 3.9m and a maximum depth 25.5m. There are approximately 150 variables measured at Loch Leven (chemical, physical, biological and meteorological) and most of this monitoring is carried out by the Centre for Ecology &amp; Hydrology in Edinburgh. One of the features of interest is the water quality and hence the relationship between chlorophyll\\(_a\\) (as an indicator of water quality) and soluble reactive phosphorus (a nutrient) is very important. This exercise explores this relationship. The data are available in the Loch_Leven dataframe in the rpanel package. provided. The monthly means for chlorophyll\\(_a\\) (lchla) and soluble reactive phosphorus (srp) are provided from January 1988 to December 2007, along with the year and month of each measurement. Plot the data to examine the relationship between chla (viewed as the response variable) and srp. How is this relationship affected by month? 10.7.8 SO\\(_2\\) over Europe As part of the European monitoring and evaluation programme (EMEP) values of atmospheric SO2, on a log scale, have been measured at monitoring stations across Europe. The dataframe SO2 in the rpanel package records logSO2 from 1990 to 2001, along with a site code, the latitude, longitude, year and month of each measurement. Select a particular station and explore which variables are linked with the values of SO2. Repeat this analysis at other stations of your choice, to see whether the relationships you have identified are consistent at different places across Europe. 10.7.9 Maximal heart rate and age The maximal rate at which a heart can beat is a parameter which is often of interest in exercise physiology. It is commonly stated that maximal heart rate declines with age according to the formula 220 -age. In fact, (robsergs_2002_jexphsioon?) investigated the history of this formula and identified that it has surprisingly uncertain origins. In an interesting early study on changes in ‘working capacity’ with sex and age, Åstrand (1952) recorded measurements of a variety of physiological parameters in subjects who undertook exercise at maximal intensity. This includes heart rate. The data are available in the dataframe Astrand in the rpanel package. Plot Heart_rate against age and consider the underlying pattern. A smooth trend curve may help. There are good reasons, both experimental and physiological why the measurements for younger children may follow a different pattern from older children and adults. Plot the data to consider only those with age 10 and above. Fit a linear model for Heart_rate as a function of Age. Is this consistent with the widely used formula 222 - age? Are there identifiable differences between males and females in the relationship between Heart_rate and Age? 10.7.10 The effects of conservation A recent paper in Science by Langhammer et al. (2024) reviewed a large number of studies of conservation projects in order to evaluate what benefit these have brought. To allow comparisons among projects which have operated over different timescales, the principal response variable was the rate of improvement in the relevant outcome. This was expressed in a standardised ‘effect size’ scale. The data were used in Exercise 5.4.1 but the details are repeated here for convenience. The data are available at (https://data.kent.ac.uk/146/6/Data%20from%20Langhammer%20et%20al.csv?download=1) with a description of the recorded variables at (https://data.kent.ac.uk/146/5/Read%20me%20R2.txt?download=1). The data are available through rp.datalink (but not yet). Details on the variables are at https://data.kent.ac.uk/146/5/Read%20me%20R2.txt?download=1. Read the data. The response variables of interest is in column 11. Rename this as ‘Effect size’. Plot Effect size against Year_publication, using colour to indicate the type of Intervention and size to indicate the standard deviation of the effect size (Intervention Std-Dev RoC). Fit a linear regression to assess the evidence that Effect size is related to Year_publication and superimpose the fitted line on the plot. If there is evidence of a relationship, how would you interpret this? References Åstrand, Per-Olof. 1952. “Experimental Studies of Physical Working Capacity in Relation to Sex and Age.” PhD thesis, Munksgaard Forlag. Langhammer, Penny F, Joseph W Bull, Jake E Bicknell, Joseph L Oakley, Mary H Brown, Michael W Bruford, Stuart HM Butchart, et al. 2024. “The Positive Impact of Conservation Action.” Science 384 (6694): 453–58. "],["issues-in-experimental-design.html", "11 Issues in experimental design", " 11 Issues in experimental design In Chapter 1 we asked the question “Where do data come from?” and this led to some discussion on designing experiments. Now that we have seen some statistical modelling at work, it is worthwhile revisiting some of the issues of experimental design. "],["power.html", "11.1 Power", " 11.1 Power One of the key questions when designing an experiment is how many samples we should have. As we have seen, more data gives more precision in estimating the things of interest to us. If we do not have enough samples we may have little chance of identifying effects of interest. If we collect more data than we really need then we are wasting resurces and, in medical settings, subjecting patients to treatements for which there is strong evidence already of inferiority, which would be unethical. The hypothesis testing framework gives us a route to tackling this problem because we can consider the probability that the test produces a significant result for different values of the true parameter of interest. We can experiment with the rp.power function in the rpanel package for R, in order to explore some of these ideas. An illustrative screenshot is shown below. This considers the case where we simply have two populations to compare. We need to consider effect size: how far apart are the two means likely to be, or perhaps more importantly, how far apart do they need to be before the difference matters? (Effect size is some times measured as the difference in means divided by the population standard deviation.) standard deviation: what is the level of the variation present in each population, and can we assume that to be the same across the two populations? normality: is it reasonable to assume a normal distribution for each population? After considering these issues, we can propose a particular sample size and then calculate the power (the probability that a two-sample t-test correctly detects the presence of the effect). The comparison of two populations is a particularly simple one. What do we do if our experiment is more complex. There are books of power calculations for different situations but these can only extend so far. One useful strategy is to consider appropriate parameter settings (means, standard deviation, etc.), write some code in R to simulate data and then apply our planned method of analysis. We can simply count the proportion of cases where the analysis correctly identifies that the effect of interest is present. That computational route to an understanding of power is a very useful one. "],["principles-of-experimental-design.html", "11.2 Principles of experimental design", " 11.2 Principles of experimental design In an ideal world experimental design would be relatively straight forward. Variation would not be a problem and we could just design experiments on the basis that if we did the same thing twice, the results would be identical. We could do the experiment on all the material (or at any rate the result would be the same no matter which individuals/piece of tissue/cell culture was used). We would be in control of all the factors which affect the outcome. But if that were the case, we would already know the answer! The main problems are: variation; not knowing what will affect the outcome; variation; we can’t do the experiment on the entire population of possible material; variation. Here are three key concepts that may help in focusing our thinking about designing experiments. Control This means more than ensuring we have a relevant control group against which to compare ‘treatment’ groups. It also refers to the careful choice of settings for the factors whose effects we would like to assess. It is also important to recognise those factors we can’t completely control but which we know will have an effect on what we measure. The discussion of blocks below is one example of this. Replicate The more data we have, the more precise our estimates will be. However, there also needs to be a realistic balance between the information gained from the experiment against the resource available to collect the data. As noted above, ethical issues can also arise. Randomise This is a very good way of avoiding the biases which can arise if we make our own decisions on how to allocate ‘treatments’ to experimental units. 11.2.1 Control 11.2.1.1 Holding Factors Constant The classical approach to controlling factors which you know about is to experiment with one factor whilst at the same time holding all the others constant. This is often referred to as ‘One Factor at a Time’ experimentation. It works well for simple experimental situations, but can give a misleading picture in more complex situations, for example when there are interactions between the factors which we are examining. There are many instances in the literature of a number of subjects where one group of researchers have found one result for a particular factor and a second group have found a very different result. This apparent paradox may be explained by the fact that one or more of the factors being held constant by both groups is being held constant at a different level (pH, temperature, Na+, concn, etc.). There are thus inherent dangers in one factor at a time experimentation, which an experimenter must be aware of. 11.2.1.2 More Complex Designs Traditionally the approach to dealing with multiple interacting factors has been the Factorial Experiment, but with three or more factors, this can require very large numbers of experimental units. There are alternatives to the factorial experiment, which make quite reasonable assumptions and which can reduce significantly the number of experimental units required. Such techniques can be very powerful, but should be used with care under the guidance of someone who is aware of the assumptions and what they mean in practice. 11.2.1.3 Blocks The second way to control factors which you know about is to divide the experimental material into sub-parts where each sub-part consists of material which is very similar. Cell material may be divided into sub-parts:- small, medium and large cells; people into:- male and female or underweight, normal and overweight; and so on. These sub-parts are referred to as Blocks. Within blocks, the variation is much reduced over that to be found in the material as a whole and it thus obscures the differences due to treatments rather less than would otherwise be the case. In general terms, the whole experiment is conducted in each block and the results from the different blocks are essentially pooled. Blocking makes for much more efficient experimentation. Suppose for example that to do all the assays required within a experiment, you will have to use more than one batch of reagents. There is a possibility that the different batches will be slightly different in some respect. This difference could affect the outcome of the assays conducted using the two batches. It would therefore be better to conduct the whole experiment (perhaps with no replication) using a single batch and to then do it a second time using another batch, to get the replication, rather than doing half the treatments with one batch and the other half with a second (possibly slightly different) batch. Similarly if you can’t fit all your vessels in one water bath, it would be better to have one from each treatment in one water bath rather than all of one treatment in one water bath and all of the other treatment in another water bath, unless of course the temperature of the water bath is the experimental treatment. 11.2.1.4 Record covariates In experimental situations, there are sometimes factors which you know about, but which you can’t hold constant and can’t organise into blocks. The last resort in ‘control’ is to record such factors and try to deal with them statistically during analysis. 11.2.2 Replicate Given that there is variation in measurements taken under seemingly ‘identical’ conditions, even after taking precautions as outlined in the previous section, then the best that we can do is to measure as accurately as possible how big that variation is. Armed with a reasonable estimate of the variation, you will be able to see if the treatment effects are larger than the ‘background’ variation. Unfortunately the larger the variation, the larger the number of replicates of each treatment you will need to: estimate the background variation accurately; see the treatment effects distinctly through the background variation. You can usually get an estimate of the likely variation for a particular experiment from published work or previous experiments which you or your colleagues have carried out. From that and a knowledge of the approximate size of the expected treatment effects, you can calculate the number of replicates which will be required to give you a reasonable chance of detecting the treatment effects given the background variation. More experiments produce inconclusive results because of inadequate replication than any other cause. Again there are statistical techniques which can reduce replication by concentrating it all in one part of the experimental design, but these techniques should be used with caution, under the guidance of someone who is fully aware of the assumptions and what they mean in practice. 11.2.3 Randomise Having dealt with the causes of variation which we know about, we must ensure that those causes that we don’t know about are evenly distributed over the treatments so as not to introduce bias into the experiment. This sounds like an impossible task since, by definition, we do not know what these causes are. Luckily there is a very powerful statistical technique which comes to our rescue - randomisation. The principle is simple - ensure that each piece of experimental material has an equal chance of being assigned to each treatment. That way it is very unlikely that all the healthy material will be assigned to one treatment and all the unhealthy material to another. They should get approximately equal shares of each. In practice you have to use some external mechanism to generate a random sequence, as human attempts to be random are usually very systematic. Randomisation should avoid the introduction of any selection bias. Techniques for randomisation include the use of random number tables, but will require that a complete list of the pieces of experimental material can be enumerated. Randomisation should mean that having accounted for the factors you know (which you were able to control), then those that you do not know will be prevented from interfering with the experiment. Circumstances may arise where it is not possible to use randomisation over some factors, under these circumstances, the only solution is to measure and use that variable or factor as a covariate. 11.2.4 Replication and Pseudo-Replication Consider an experiment where each treatment is replicated three times. This provides a means of dealing with the inherent variation in the results obtained, even if we have carried out identical manipulations, using the same batches of materials. The three replicates give us a measure of how variable the process is. However, if the process is very variable it may be difficult to distinguish between the treatments. In some settings, when we record the value from each replicate we may be able to do this multiple times. If we can, why not do this three times for each replicate. This will give us a replication of nine rather than three for very little extra effort. Unfortunately this is not true replication. Most experimental methods involve a number of steps, illustrated here in the context of an experiment requiring the preparation of cellular suspecsions: collect material; prepare cellular suspension; subject cellular suspension to one of a number of treatments; extract an indicator component as a surrogate for treatment success; assay indicator component. Each of these steps may be replicated a variable number of times. Replication applied early in the process will lead to extra replication of later steps and it is thus tempting to replicate later steps more than earlier ones. In each case however the replication measures something different. Collect material Here the replication measures the variation in collection or the variation between plants, animals, etc. This helps you to see how well the results will apply to other individuals. This is all about variation in the source material. Increase in the number of individuals to measure obviously increases the cost and it is usual to try to keep this to a minimum. However, if several individuals are not independently subjected to the same ‘treatment’ or environmental influence, there is no true replication in the study and the remainder of the steps are measuring within individual variability, in the absence of any indication of between individual variability. Prepare cellular suspension Here the replication measures the variability in the preparation of the suspension. This may be due to different people carrying out the preparation, use of different apparatus, different reagent stocks, temperature, etc. This helps you to see how different the results are likely to be for multiple preparations. This is all about variation in technique. This measures the variability within the preparation and there will obviously be a need for more replication at this level if the preparation is very heterogeneous. Apply treatment This is the replication of the treatment and measures how different applications of the same treatment are and enables us to assess whether the differences due to treatment are larger than the differences that we see between replications of the same treatment. This is what the experiment is all about. Extract indicator Here the replication measures the variation in the extraction process, and will be due to similar reasons to those outlined under ‘Prepare cellular suspension’ above. This is again all about variation in technique. Assay indicator Here the replication measures the variation in the assay process, and will be due to similar reasons to those outlined above. This is again all about variation in technique. Each of these sources of variation (and others) may be important in the overall conduct of the experiment; but they do measure different things. The importance of these components of variation depends on their relative size. Assessment of the relative size of components of variation is important in assessing the viability of sampling, preparation, extraction or assay procedures. In general terms, the variation due to these procedures should be less than that due to the treatment. If this is not true, it will not be possible to arrive at a justifiable conclusion. The replication that we applied was at the ‘Apply treatment’ stage. The addition replication proposed is at the ‘Assay indicator’ stage and it contributes nothing to our estimation of the variation due to treatment and thus our ability to distinguish between the performance of the different types of bead. When replication is carried out at an inappropriate stage, this is referred to as pseudo-replication. Pseudo-replication misleads you into attributing the wrong number of degrees of freedom to your treatment comparisons and thus to think that the inherent variation between replicates is smaller than it really is. This in turn can lead you to think that there are real differences between treatments when there are not. Most commonly pseudo-replication occurs by substituting extra replication at the final assay stage for replication at the treatment stage, as it is easier and requires less effort and resources. This phenomenon is to be found quite widely in experimental work and often finds its way into published papers. The statistical significance of a particular treatment or phenomenon can only be determined relative to the variation that would be expected if the treatment or phenomenon did not apply. This means that a source of ‘error’ must be identified and measured which provides this comparison. A failure to do this will result in an experiment or study in which it is not possible to determine the statistical significance of what has been observed. "],["exercises-8.html", "11.3 Exercises", " 11.3 Exercises 11.3.1 Student cost of living The University of Glasgow SRC would like information on the cost of living of full-time students enrolled at the University. Go through each of the terms defined in the notes above and consider what they are in this context, from element to sample statistic. 11.3.2 US presidential election 1936 The Literary Digest was a magazine which surveyed 10 million people, beginning with its own readers, who they planned to vote for in the 1936 US presidential election. A massive 2.4 million people responded, leading to the prediction of a clear win for the candidate Alf Landon. In fact, Franklin Delano Roosevelt had a landslide victory. What went wrong? 11.3.3 The parable of the sower Revisit the notes on this example of a randomised block design and consider how you would allocate the different varieties of barley to the field. 11.3.4 Mururoa Use the rp.mururoa function in the rpanel package to consider how to generate sampling positions in the Mururoa example. Once you have made your decision, click on the Take sample button to produce an estimate of the radiation map. 11.3.5 A Scottish firth Use the rp.firth function in the rpanel package to consider how to generate sampling positions in the Scottish firth example. Once you have made your decision, click on the Take sample button to produce an estimate of the radioactive particles map. "],["issues-in-building-models.html", "12 Issues in building models ", " 12 Issues in building models "],["what-are-models-for.html", "12.1 What are models for?", " 12.1 What are models for? The examples in this chapter have raised a number of issues which will be developed in later material. One particularly important issue is to keep the purpose of a model firmly in mind. In other words, what is our model for? Why are we constructing it? The examples of Giving in the Church of England and the Black Cherry Trees illustrate two different answers to this question. In the first we are interest in identifying and understanding the relationships involved, in this case the links between different covariates and giving. In the second we are focussed strongly on prediction. We would like a practical and effective means of predicting the volume of wood which will be harvested from an orchard or forest. Whether the method of prediction also provides a full scientific description of the relationships between the variables matters much less. Extrapolation is inappropriate. We would not want to predict the annual giving per member in a fictional diocese with high IMD and high attachment. These issues are discussed in interesting articles by Breiman (2001) and Hand (2019). References Breiman, Leo. 2001. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” Statistical Science 16 (3): 199–231. Hand, David. 2019. “What Is the Purpose of Statistical Modelling.” Harvard Data Science Review 1 (1): 1–6. https://hdsr.mitpress.mit.edu/pub/9qsbf3hz/release/7. "],["model-selection.html", "12.2 Model selection", " 12.2 Model selection Suppose we have a model which aims to relate a response variable to m potential covariates. Suppose m = 100. Even if none of these covariates have any association with the response, it is likely that some will show up as ‘significant’. m &lt;- 100 y &lt;- rnorm(1000) X &lt;- matrix(rnorm(m * 1000), ncol = m) mdl &lt;- lm(y ~ X) p &lt;- summary(mdl)$coefficients[ , 4] length(which(p &lt; 0.05)) ## [1] 4 We can adjust the threshold for p-values to be ‘significant’. Bonferroni: 0.05 -&gt; 0.05 / m Bonferroni-Holm: Sort the p-values from lowest to highest: \\(p_1, ..., p_m\\). If \\(p_1 &lt; 0.05 / m\\), mark as ‘significant’ and proceed; otherwise stop. At step i, if \\(p_1 &lt; 0.05 / (m - i)\\), mark as ‘significant’ and proceed; otherwise stop. False Discovery Rate: Ensure that out of all ‘significant’ effects only 5% are false. Criteria such as AIC seek to balance how well the model fits against its complexity. This can provide a very helpful way of identifying a useful model, or a set of models. Remember that the aims of understanding (which variables are linked to the response) and prediction (how well can I predict the response) may lead to different approaches to selecting a model. Consider whether variables can be split into categories: those whose potential effects are of primary interest; those which are not of primary interest but which we believe are likely to influence our response variable; others which have been recorded more speculatively. We might then fit a model which contains groups 1 and 2 but which tests only those variables in group 1. Group 3 might be screened in a secondary manner by including them in the model and carrying out a global test of whether there is any ‘signal’ in this group as a whole. Helpful discussion of general issues is provided by @ref(lewer_2025_bmjmed). "],["generalised-linear-models.html", "13 Generalised linear models", " 13 Generalised linear models The earlier discussion of linear models highlighted various assumptions. A basic assumption is that the response data has values which lie on a continuous scale. There are, of course, many other types of data. This chapter will discuss how we can adapt the ideas of linear regression to ensure that our model properly reflects the essential characteristics of other types of data while retaining as much as possible of the general approach and strategy of linear modelling. "],["logistic-regression.html", "13.1 Binomial data: logistic regression", " 13.1 Binomial data: logistic regression Example: How to measure toxicity? Flour beetles (Tribolium confusum) are common pests which can infest stores of flour and grain. Strand (1930) described an experiment to compare the effectiveness of different fumigants in eradicating these beetles. An interesting aspect of this is how toxicity should be measured. It is problematic to target very high proportions of eradication. Strand’s paper includes a quote from R.A. Fisher, one of the founding fathers of statistical methods. “In as many as 99 per cent of the insects killed, the accuracy of the comparison between two insecticides would depend upon the comparatively few insects which survived, and to compare them with any accuracy many thousands of insects would have to be used.” Strand, and Fisher, advocate that the concentration required to kill 50% of the insects provides a much more accurate way to compare the effectiveness of different compounds. This is now generally called the ‘lethal dose 50’ or ‘LD50’. The paper includes experimental data on the numbers of beetles killed by a variety of toxins at different strengths, including some replication. These data are available in the flour_beetles dataframe in the rpanel package. How might we estimate the LD50 values for each compound? A plot of the flour beetle dataset is shown below. (These data were used in an earlier exercise in visualisation.) The use of scales = 'free_x' in facet_wrap allows the x-axis of each facet to adapt to the range of concentrations used in each particular toxin. library(rpanel) library(tidyverse) flour_beetles &lt;- mutate(flour_beetles, Proportion_killed = Dead / (Living + Dead)) ggplot(flour_beetles, aes(Concentration, Proportion_killed, col = Replicate)) + geom_point() + facet_wrap(vars(Toxin), scales = &quot;free_x&quot;) What model should we adopt to assist in the estimation of LD50 values? We will consider the case of ethylene dichloride, where there is a single replicate, to help in thinking this through. The plot below might suggest a simple linear model but would be uncomfortable for a variety of reasons. These include the fact that proportions must lie between 0 and 1, a constraint which a linear model would not respect. In addition, we know that each observed proportion arises from the number of ‘successes’ (deaths) in each group of beetles and so this has a binomial distribution, not a normal distribution. ethylene &lt;- filter(flour_beetles, Toxin == &#39;Ethylene_dichloride&#39;) ggplot(ethylene, aes(Concentration, Proportion_killed)) + geom_point() Because the response variable is a proportion, it is necessary to define a model which keeps the true proportion within the range \\([0,1]\\). One approach is to consider that each beetle has its own tolerance to the toxin, with death occurring when that tolerance is passed. If the distribution of tolerances across the beetle population has a normal distribution then the increase in proportion killed as dosage rises can be given a specific shape, know as the probit curve. As this is not a particularly convenient object to work with, the logistic curve has become the standard shape for the dose/proportion relationship. The formula for this curve is: \\[ p(x) = \\frac{e^{\\alpha + \\beta x}}{1 + e^{\\alpha + \\beta x}} , \\] where \\(x\\) denotes concentration and \\(p(x)\\) denotes the proportion killed. The parameters \\(\\alpha\\) and \\(\\beta\\) control the position of the curve along the x-axis (\\(\\alpha\\)) and the steepness of the curve (\\(\\beta\\)). An alternative way to write this model is \\[ \\mbox{logit}(p(x)) = \\log \\left( \\frac{p(x)}{1 - p(x)} \\right) = \\alpha + \\beta x . \\] This emphasises that the ‘engine’ of the model is a linear expression, referred to as the linear predictor, but that the effect of this has been transformed to match the [0, 1] proportion scale inherent in the observed data. As a result of the logit transformation, this model is referred to as a logistic regression. The rp.logistic function in the rpanel package allows an interactive exploration of the logistic curve and how its shape changed with the parameters \\(\\alpha\\) and \\(\\beta\\). This can be launched with the ethylene data as shown below. (Notice the use of with as a convenient wrapper so that we can refer directly to the names of variables inside the ethylene dataframe.) You may like to experiment with this. with(ethylene, rp.logistic(Concentration, cbind(Dead, Living)) How should we fit this model to the observed data? The idea of finding the parameter values which place the model as close as possible to the observed data is an appealing one but it is not so obvious that the linear model approach of minimising a sum-of-squares is a suitable strategy in this binomial context. The very powerful concept of likelihood will be introduced in the next section. For the moment we can implement the model in practice in R through the glm function, whose name is an acronym of generalised linear models and which uses likelhood to fit the model to the data. Notice that the response variable is defined here as a two-column matrix, created using cbind, which gives both the number of Dead and Living. Both are needed to give a full description of the data. The family argument also specifies that the response variable has a binomial distribution. model &lt;- glm(cbind(Dead, Living) ~ Concentration, family = &#39;binomial&#39;, data = ethylene) summary(model)$coefficients ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -12.0982883 1.43633497 -8.423027 3.668796e-17 ## Concentration 0.3547108 0.04084555 8.684196 3.814301e-18 The fitted curve can now be superimposed on the plot of the data. The code below uses the predict function, with a new set of Concentration values along a regular grid, to calculate values along the fitted curve. This uses type = \"response\" to ensure that these predicted values sit on the proportion scale and not on the linear predictor scale. As the plot contains both data points and predicted values, which are of different lengths, the aes and data information is not passed to ggplot as global definitions but is instead passed separately to the different geometries as required. rng &lt;- range(ethylene$Concentration) xgrid &lt;- seq(rng[1], rng[2], length = 50) pgrid &lt;- predict(model, newdata = data.frame(Concentration = xgrid), type = &quot;response&quot;) ggplot() + geom_point(aes(Concentration, Proportion_killed), data = ethylene) + geom_line(aes(xgrid, pgrid, col = I(&quot;blue&quot;))) The right hand plot above uses some additional code (not shown) to locate the estimated LD50 as the concentration value at which the fitted logistic curve reaches 0.5. References Strand, August Leroy. 1930. “Measuring the Toxicity of Insect Fumigants.” Industrial &amp; Engineering Chemistry Analytical Edition 2 (1): 4–8. "],["likelihood.html", "13.2 The concept of likelihood", " 13.2 The concept of likelihood Refer back to the earlier likelihood material. The model for all the flour beetle data from the ethylene dichloride toxin is more complex. Each group of beetles exposed to ethylene dichloride at a particular concentration has its own particular group size \\(m_i\\), outcome \\(k_i\\) and underlying probability of success \\(p_i\\), where \\(i\\) takes the values \\(1, \\ldots, n\\) to identify the different groups. Here \\(n=9\\). When there are multiple observations then, under some assumptions, the elementary rules of probability tell us that the probability associated with the set of all observations is the product of the probability of each. The key assumption is that the observations are independent, meaning that knowledge of the particular variation associated with one tells us nothing about the variation at another. That seems a reasonable assumption in the present case. So, following our earlier notation, the likelihood function based on the observations \\(k_1, \\ldots, k_n\\), the group sizes \\(m_1, \\ldots, m_n\\) and the underlying probabilities \\(p_1, \\ldots, p_n\\) is \\[ L(\\alpha, \\beta; k_1, \\ldots, k_n, m_1, \\ldots, m_n) = L_1(p_1; k_1, m_1) \\ldots L_n(p_n; k_1, m_n) \\] The combined likelihood function on the left hand side is written in terms of the two model parameters \\(\\alpha\\) and \\(\\beta\\) because in the logistic model the individual probabilities are constructed as \\(p_i = \\frac{e^{\\alpha + \\beta x_i}}{1 + e^{\\alpha + \\beta x_i}}\\), so there are only two unknown parameters. As likelihood functions often involve products, it can be convenient to apply use the log function to turn these into sums. The log scale does not change the location of the maximum likelihood estimates, as the values which maximise the likelihood also maximise the log-likelihood. It turns out that there are other very powerful reasons for using the log scale and we will explore these later. The log-likelihood function is defined as \\[ l(\\alpha, \\beta; k_1, \\ldots, k_n, m_1, \\ldots, m_n) = log(L_1(p_1; k_1, m_1)) + \\ldots + log(L_n(p_n; k_1, m_n)) \\] The rp.logistic function mentioned above has an option (as a checkbox on the side panel) to display the log-likelihood function. Try this. You can add the maximum likelihood estimate. There are other options which we will explore later. As a more static illustration, the code below uses the expand.grid function to create a grid of possible values of \\(\\alpha\\) and \\(\\beta\\) in the matrix pars and defines a function ll.fn which evaluates the log-likelihood at any particular choice of parameters. The apply function, described in Section 19.1, then applies this function repeatedly to the rows of pars. The ggplot geometry geom_contour_filled then displays the likelihood surface as a function of \\(\\alpha\\) and \\(\\beta\\). The maximum likelihood estimates, \\(\\hat\\alpha\\) and \\(\\hat\\beta\\), where the ‘hat’ notation indicates that these are estimates of the unknown parameters \\(\\alpha\\) and \\(\\beta\\), are marked on the plot. This confirms the results of the glm function which fitted the model to these data in the previous section. alpha &lt;- seq(-25, -4, length = 50) beta &lt;- seq(0.1, 0.8, length = 50) pars &lt;- expand.grid(alpha = alpha, beta = beta) ll.fn &lt;- function(theta, data) { lp &lt;- theta[1] + theta[2] * data$Concentration probs &lt;- exp(lp) / (1 + exp(lp)) likes &lt;- dbinom(data$Dead, data$Living + data$Dead, probs) sum(log(likes)) } pars$log_likelihood &lt;- apply(pars, 1, ll.fn, data = ethylene) ggplot(pars, aes(alpha, beta, z = log_likelihood)) + geom_contour_filled(breaks = c(-10000, seq(-400, 0, length = 15))) + geom_point(aes(model$coefficients[1], model$coefficients[2]), col = &#39;red&#39;, size = 3) The concept of likelihood provides a very powerful mechanism for fitting models to data but it can deliver much more insight than parameter estimates. There are also very general and powerful mechanisms for carrying out inference. "],["poisson-regression.html", "13.3 Count data: Poisson regression", " 13.3 Count data: Poisson regression Example: Flaws in cloth Bissell (1972) provides data on the numbers of flaws found in rolls of cloth of different lengths. These are availanle in the bissell dataframe in the sm package, with variables Flaws and Lengths. How should we best estimate the rate of flaws in the manufacturing process? A very simple model for these data is to assume a constant rate of flaws, with the underlying pattern in the number recorded in each roll driven by multiplying rate by length. The plot below suggests this may be plausible, although there is large variation in individual measurements. As the response variable is a count, a further simple assumption is that this variation is captured by the Poisson distribution, which arises naturally in the context of counting events which occur at a constant rate. A Poisson distribution has a single parameter, namely the mean count, and it has the interesting property that the standard deviation of the distribution is the square root of the same parameter. data(bissell, package = &#39;sm&#39;) library(tidyverse) ggplot(bissell, aes(Length, Flaws)) + geom_point() If we represent the counts of flaws by \\(Y_1, \\ldots, Y_n\\) and the corresponding lengths by \\(x_1, \\ldots, x_n\\), where \\(n=32\\) is the number of observations, then this simple model can be written as \\[ Y_i \\sim Po(\\beta x_i) . \\] This model can be fitted by the same glm function used in logistic regression, through the argument family = poisson(link = 'identity'). This specifies that the Poisson distribution is to be used to describe the variation. To reflect the simple proportional structure of the model the intercept term, which is assumed to be presdent by default, needs to be omitted and this is achieved by including the term -1 in the mnodel formula. As our simple proportional model is fixed at the origin (length = 0, flaws = 0), we do not need to employ a transformation to ensure that the fitted values for counts are always positive. To illustrate the process of maximum likelihood estimation, the code below also computes and displays the likelihood function, using the Poisson probabilities provided by the dpois function. (The code for the likelihood function is omitted as you are invited to try this as an exercise.) model1 &lt;- glm(Flaws ~ Length - 1, family = poisson(link = &#39;identity&#39;), data = bissell) summary(model1)$coefficients ## Estimate Std. Error z value Pr(&gt;|z|) ## Length 0.01510237 0.0008961606 16.8523 1.009133e-63 A more standard way of formulating a Poisson regression model is to employ a linear predictor of the usual form, \\(\\alpha + \\beta x\\). As this can take negative values, a transformation is needed to ensure that the fitted values from the model remain positive, as befits counts. The exponential transformation is then a natural choice so that the model becomes \\(Y_i \\sim Po(e^{\\alpha + \\beta x_i})\\). If we let \\(x\\) denote log(length), then in fact this corresponds to \\(Y_i \\sim Po(e^{\\alpha} x_i^\\beta)\\). This retains a proportional structure, with rate \\(e^\\alpha\\), but allows some curvature in the shape of the scaling by length. If 1 is a plausible value for \\(\\beta\\) then the model reduces to the earlier simple form. The results below show that this is indeed plausible, as a confidence interval for \\(\\beta\\) includes 1. Similarly, the rate constant \\(e^{-4.173} = 0.0154\\), which is very close to the estimated rate in the earlier model. model2 &lt;- glm(Flaws ~ log(Length), family = &#39;poisson&#39;, data = bissell) summary(model2)$coefficients ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.1729521 1.1351658 -3.676073 2.368520e-04 ## log(Length) 0.9969044 0.1758873 5.667859 1.445929e-08 confint(model2) ## 2.5 % 97.5 % ## (Intercept) -6.4707481 -2.021167 ## log(Length) 0.6624967 1.351929 bissell$fitted2 &lt;- exp(model2$coefficients[1]) * bissell$Length^model2$coefficient[2] ggplot(bissell, aes(Length, Flaws)) + geom_point() + geom_line(aes(Length, fitted(model2)), col = &#39;blue&#39;) "],["general-glms.html", "13.4 A general formulation of GLMs", " 13.4 A general formulation of GLMs The models we have seen so far, involving logistic and Poisson regression, are actually examples of a wider class called generalised linear models. These involve the specification of a distribution which describes the variation in the data; the choice of distribution usually comes from a special class called the exponential family; a linear predictor which incorporates the explanatory variables in a linear expression; a link function whose inverse transforms the linear predictor onto a suitable scale for the mean value of the data. "],["further-examples.html", "13.5 Further examples", " 13.5 Further examples 13.5.1 Two covariates: vasoconstriction of the digits Example: Vaso-constriction in the finger aftrer deep inspiration In a paper with the title above, Gilliatt (1948) collected phsiological data on vaso-constriction of fingers after a single deep breath. The measurement process was not sufficiently precise to determine the degree of vaso-constriction, simply its presence or absence. Interest lay in how the presence of vaso-constriction relates to the volume and flow rate of air during the intake of breath. Finney (1947) reported the data and discussed a variety of models in some depth. The data are available in the vasoconstriction dataframe in the rpanel package. Note that the table of data in Finney (1947) seems to contain a misprint, with observation 32 showing a rate of 0.03 while the plots indicate this to be 0.30. The vasoconstriction dataframe incorporates this correction. A plot of the data is shown below, with the response shown as a 1 (presence) or 0 (absence). data(vasoconstriction, package = &#39;rpanel&#39;) library(tidyverse) ggplot(vasoconstriction, aes(log(Rate), log(Volume), label = Vasoconstriction)) + geom_text() With two covariates, the fitted model can be displayed in 3D. With the plot in the web form of this book, the mouse can be used to rotate and ‘zoom’ the display, allowing the shape of the fitted logistic regression surface to be inspected. model &lt;- glm(Vasoconstriction ~ log(Rate) + log(Volume), family = &#39;binomial&#39;, data = vasoconstriction) r_grid &lt;- seq(min(vasoconstriction$Rate), max(vasoconstriction$Rate), length = 20) v_grid &lt;- seq(min(vasoconstriction$Volume), max(vasoconstriction$Volume), length = 20) f_grid &lt;- expand.grid(&#39;Rate&#39; = r_grid, &#39;Volume&#39; = v_grid) f_val &lt;- predict(model, f_grid, type = &#39;response&#39;) f_mat &lt;- matrix(f_val, ncol = 20) library(sm) scaling &lt;- rp.plot3d(log(vasoconstriction$Rate), vasoconstriction$Vasoconstriction, log(vasoconstriction$Volume)) sm.surface3d(cbind(log(r_grid), log(v_grid)), f_mat, scaling) rgl::rglwidget() 13.5.2 Doctors and smoking Example: Smoking and cardiac-related death A very famous study of the health of medical doctors was initiated by Doll &amp; Hill. This involved sending out a questionnaire on health and lifestyle with subsequent follow-up, in particular identifying subsequent cardiovascular-related death. This study was the first of its kind and it contined for several decades. The data, condensed into person-years of exposure by Breslow, are available in the doctors_smoking dataframe in the rpanel package. The left had plot below displays the data. The right hand plot shows a fitted model which described the effects of age, smoking and their interaction. Notice that the call to glm to fit the model includes an offset which allows for the fact that longer exposure will inevitably lead to higher deaths. The effects of age have been described by a quadratic model using a poly(age, 2) term. This allows the effect of age to be non-linear (in addition to the exponential transformation involved in the Poisson model) but this is kept simple as there is such a small number of age points. library(tidyverse) data(doctors_smoking, package = &#39;rpanel&#39;) ggplot(doctors_smoking, aes(age, rate, col = smoking)) + geom_point() model &lt;- glm(deaths ~ poly(age, 2) * smoking + offset(log(person_years)), family = poisson(), data = doctors_smoking) doctors_smoking &lt;- mutate(doctors_smoking, fitted = fitted(model) * 100000 / person_years) ggplot(doctors_smoking, aes(age, rate, col = smoking)) + geom_point() + geom_line(aes(y = fitted)) References Finney, DJ. 1947. “The Estimation from Individual Records of the Relationship Between Dose and Quantal Response.” Biometrika 34 (3/4): 320–34. Gilliatt, Roger W. 1948. “Vaso-Constriction in the Finger After Deep Inspiration.” The Journal of Physiology 107 (1): 76–88. "],["exercises-9.html", "13.6 Exercises", " 13.6 Exercises 13.6.1 The Glasgow Coma Scale In the 1970’s, neurosurgeons in Glasgow (Teasdale and Jennett 1974) began to develop a scale for assessing the severity of brain injuries. The Glasgow Coma Scale sought to provide a clear and consistent way of assessing the condition of a patient through scores which evaluated eye opening, verbal and motor responses to stimuli. Positive scores represent better response and an overall score combines the three components. The scale subsequently became established internationally as a very valuable tool in clinical practice and in research. Teasdale et al. (2014) give a review after 40 years of its use, including reference to an evaluation of how the score relates to the probability of the death of the patient in a large clinical trial (Collaborators et al. 2008). The trial involved a large number of subjects but the data can be represented in compact form by recording the number of people with a particular coma scale score and the number of those who subsequently died within 14 days of injury. The code below sets this up. coma &lt;- data.frame( score = 3:14, n = c(667, 455, 478, 677, 946, 709, 633, 690, 748, 962, 1498, 1499), death = c(342, 275, 232, 231, 241, 152, 113, 108, 52, 65, 76, 51)) Investigate the relationship between the proportion of deaths and the coma score. Consider whether the log-odds scale for the proportion of deaths creates a simple form of relationship. Solution A plot of proportions of deaths against coma scale already indicates a strong association with mortality, with higher scores indicating better responses and lower probability of death. The log-odds scale does indeed create a plot which is approximately linear. coma &lt;- data.frame( score = 3:14, n = c(667, 455, 478, 677, 946, 709, 633, 690, 748, 962, 1498, 1499), death = c(342, 275, 232, 231, 241, 152, 113, 108, 52, 65, 76, 51)) %&gt;% mutate(p = death / n) ggplot(coma, aes(score, p)) + geom_point() + ylab(&#39;Proportion of deaths&#39;) + scale_x_continuous(breaks = 3:14) coma &lt;- mutate(coma, log_odds = log(p / (1 - p))) ggplot(coma, aes(score, log_odds)) + geom_point() + ylab(&#39;Proportion of deaths&#39;) + scale_x_continuous(breaks = 3:14) 13.6.2 Bissell data Write code to evaluate and plot the likelihood function for the simple proportional model with the Bissell data described in Section 13.3. Solution As ever, there are many ways in which this can be done. The solution below follows the pattern of the likelihoods computed in the Section on logistic regression. p &lt;- seq(0.0125, 0.0175, length = 50) pmat &lt;- matrix(p, ncol = 1) l.fn &lt;- function(x) prod(dpois(bissell$Flaws, x * bissell$Length)) likelihood &lt;- apply(pmat, 1, l.fn) dfrm &lt;- data.frame(p, likelihood) ggplot() + geom_line(aes(p, likelihood), data = dfrm) + geom_vline(aes(xintercept = model$coefficients), linetype = 2, col = &#39;red&#39;) + geom_text(aes(model$coefficient, min(likelihood), label = as.character(round(model$coefficients, 4))), col = &#39;red&#39;, nudge_y = 0) 13.6.3 Vasoconstriction data For the vasoconstriction data, Finney (1947) implies that a model which relates the presence of vaso-constriction to the sum of log(Volume) and log(Rate) may be effective. Use logistic regression to fit this model and superimpose the fitted curve on a display of the data. Solution As ever, there are many ways in which this can be done. The solution below follows the pattern of the likelihoods computed in the Section on logistic regression. data(vasoconstriction, package = &#39;rpanel&#39;) library(tidyverse) logVR &lt;- log(vasoconstriction$Volume) + log(vasoconstriction$Rate) model &lt;- glm(Vasoconstriction ~ logVR, family = &#39;binomial&#39;, data = vasoconstriction) rng &lt;- range(logVR) vrgrid &lt;- seq(rng[1], rng[2], length = 50) pgrid &lt;- predict(model, newdata = data.frame(logVR = vrgrid), type = &quot;response&quot;) ggplot() + geom_jitter(aes(log(Rate) + log(Volume), Vasoconstriction), width = 0, height = 0.02, data = vasoconstriction) + geom_line(aes(vrgrid, pgrid, col = I(&quot;blue&quot;))) 13.6.4 The Framingham heart study This famous study followed up the health of a large number of people in Framingham, in the United States, over a long period. A version of the data suitable for teaching (not research) is available in the cvdd dataframe in the riskCommunicator package. How would you use these data to create a risk model for death within 10 years? 13.6.5 The shape of a log-likelihood function The log-likelihood function for logistic regression with the ethylene data, shown in the Section on likelihood in this chapter and viewed through the rp.logistic function, has a long diagonal ridge with very steep sides. That is rather an awkward shape. What happens if we change the parameterisation of the linear predictor in the model from \\(\\alpha + \\beta x\\) to \\(\\alpha + \\beta (x - \\bar{x})\\), where \\(\\bar{x}\\) denotes the mean of the \\(x_i\\) values? Investigate this by creating a new variable which subtracts the mean from the Concentration values, then refit the model with this variable and re-run the code shown earlier in the chapter to produce a contour plot. You will have to experiment with different ranges of alpha and beta to see the peak of the log-likelihood function. What does this plot tell you? What are the implications for the parameter estimates and a confidence region? References Collaborators, MRC Crash Trial et al. 2008. “Predicting Outcome After Traumatic Brain Injury: Practical Prognostic Models Based on Large Cohort of International Patients.” British Medical Journal 336 (7641): 425–29. Finney, DJ. 1947. “The Estimation from Individual Records of the Relationship Between Dose and Quantal Response.” Biometrika 34 (3/4): 320–34. Teasdale, Graham, and Bryan Jennett. 1974. “Assessment of Coma and Impaired Consciousness: A Practical Scale.” The Lancet 304 (7872): 81–84. Teasdale, Graham, Andrew Maas, Fiona Lecky, Geoffrey Manley, Nino Stocchetti, and Gordon Murray. 2014. “The Glasgow Coma Scale at 40 Years: Standing the Test of Time.” The Lancet Neurology 13 (8): 844–54. "],["models-with-random-effects.html", "14 Models with random effects", " 14 Models with random effects "],["examples.html", "14.1 Examples", " 14.1 Examples In order to motivate the ideas and issues to be discussed in this chapter, some illustrative examples are given below. In each case we will begin by thinking carefully about the structure of the data and the sources of variability. As ever, appropriate visualisations will help. 14.1.1 The manufacture of penicillin Example: The manufacture of penicillin In a historical experiment, four different processes for the manufacture of penicillin were compared. One issue is that an important raw material, corn steep liquor, can be quite variable in its composition. To allow for this, five different batches (blends) of corn liquor were used. Fortunately, there was enough raw material in each blend to allow all four manufacturing processes to be applied and the subsequent yields recorded. Principal interest lies in whether there are any identifiable differences among the manufacturing processes, but the size of the variation in the blends is also of secondary interest. The data are reported in Box &amp; Hunter (1978), Statistics for Experimenters. The dataset is available in the penicillin dataframe in the faraway package. The dataset is available in the penicillin dataframe in the faraway package. data(penicillin, package = &#39;faraway&#39;) str(penicillin) ## &#39;data.frame&#39;: 20 obs. of 3 variables: ## $ treat: Factor w/ 4 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;: 1 2 3 4 1 2 3 4 1 2 ... ## $ blend: Factor w/ 5 levels &quot;Blend1&quot;,&quot;Blend2&quot;,..: 1 1 1 1 2 2 2 2 3 3 ... ## $ yield: num 89 88 97 94 84 77 92 79 81 87 ... Here is an initial plot of the which aims to include all the different sources of variation. The effect of treatment on yield is the main focus, but the variability associated with the different blends is also represented by the use of separate lines and colours to connect the observations which arise from the same blend. It looks as though there is substantial blend variability, as the lines are somewhat separated, but it is not clear whether there are any systematic differences among the treatments. library(ggplot2) ggplot(penicillin, aes(treat, yield, group = blend, col = blend)) + geom_line() A particular issue we need to consider is whether we are building a model which describes the effects associated with the particular blends used in this study or a model which uses the information available to describe the patterns of variation across the wider population of blends. 14.1.2 Childhood growth Example: Childhood growth Potthoff and Roy (1964) report data from an experiment where the distance between two anatomical points on the head was meaasured for a number of children, both males and females, over several years. How should we model the underlying growth patterns? The data are available in the Orthodont dataframe in the nlme package. Inspection of the data reveals a simple structure, with variables distance, age, Subject and Sex. The plot below confirms growth with age, as we would expect. There is also a suggestion that boys are slightly larger than girls, on average, at all the ages considered. Again, this is not surprising. Connecting the observations from each subject by lines emphasises that these data points are linked. This needs to be suitably reflected in the model we build. As in the previous example, is our model for these particular subjects or for the wider population? data(Orthodont, package = &#39;nlme&#39;) str(Orthodont) ## Classes &#39;nfnGroupedData&#39;, &#39;nfGroupedData&#39;, &#39;groupedData&#39; and &#39;data.frame&#39;: 108 obs. of 4 variables: ## $ distance: num 26 25 29 31 21.5 22.5 23 26.5 23 22.5 ... ## $ age : num 8 10 12 14 8 10 12 14 8 10 ... ## $ Subject : Ord.factor w/ 27 levels &quot;M16&quot;&lt;&quot;M05&quot;&lt;&quot;M02&quot;&lt;..: 15 15 15 15 3 3 3 3 7 7 ... ## $ Sex : Factor w/ 2 levels &quot;Male&quot;,&quot;Female&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## - attr(*, &quot;outer&quot;)=Class &#39;formula&#39; language ~Sex ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; ## - attr(*, &quot;formula&quot;)=Class &#39;formula&#39; language distance ~ age | Subject ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; ## - attr(*, &quot;labels&quot;)=List of 2 ## ..$ x: chr &quot;Age&quot; ## ..$ y: chr &quot;Distance from pituitary to pterygomaxillary fissure&quot; ## - attr(*, &quot;units&quot;)=List of 2 ## ..$ x: chr &quot;(yr)&quot; ## ..$ y: chr &quot;(mm)&quot; ## - attr(*, &quot;FUN&quot;)=function (x) ## ..- attr(*, &quot;source&quot;)= chr &quot;function (x) max(x, na.rm = TRUE)&quot; ## - attr(*, &quot;order.groups&quot;)= logi TRUE library(ggplot2) ggplot(Orthodont, aes(age, distance, group = Subject, col = Sex)) + geom_line() ggplot(Orthodont, aes(age, distance, group = Subject, col = Sex)) + geom_line() + facet_wrap(~ Subject) 14.1.3 The strength of paste Example: The strength of paste A company uses a chemical paste in one of its production processes and receives deliveries of the paste in batches. The quality control department of the company is concerned about the variability in the strength of paste and decided to investigate. Ten batches of paste were randomly selected for a number of deliveries (one batch is received per delivery). From each of the batches a random sample of three casks was selected and two random determinations were made from random samples from each cask. Where does most of the variability lie – at the batch, cask or individual measurement level? Data source: O.L. Davies and P.L. Goldsmith (eds), Statistical Methods in Research and Production, 4th ed., Oliver and Boyd, (1972), section 6.5. The data are available in the Pastes dataframe in the lme4 package. The variables of interest are strength, batch and cask. The data are plotted below in two different forms. We are used to viewing plots to identify systematic effects such as changes in means. It is more difficult to compare the sizes of variations. A further special feature of the data is that the cask variation is nested inside the batch variation. We should be careful to respect the hierarchical nature of the data structure in any model we construct. data(Pastes, package = &#39;lme4&#39;) ggplot(Pastes, aes(strength, batch, col = cask)) + geom_point() ggplot(Pastes, aes(strength, cask)) + geom_point() + facet_wrap(~ batch) 14.1.4 Common features A common feature of all these examples is that the data structure creates links between different observations. In the penicillin data, there are several groups which are formed by the observations made on the same blend. In the Orthodont data, there are groups created by repeated observations over time on the same subject. In the Pastes data, the grouping is created in a hierarchical manner as observation within cask within batch. The links created by these groupings should be respected in our model. In addition, we would like to construct models which treat some sources of variation as random samples of the variation present in the wider population. This is what we mean by random effects. References Potthoff, Richard F, and Samarendra N Roy. 1964. “A Generalized Multivariate Analysis of Variance Model Useful Especially for Growth Curve Problems.” Biometrika 51 (3-4): 313–26. "],["formulating-and-fitting-mixed-effects-models.html", "14.2 Formulating and fitting mixed effects models", " 14.2 Formulating and fitting mixed effects models We will use the penicillin data to consider how to formulate and fit random effects. Some precide notation will help. We begin by denoting the yield on treatment \\(i\\) and blend \\(j\\) by \\(y_{ij}\\). A model which accounts for differences in both treatments and blends is: \\[ y_{ij} = \\mu + \\alpha_i + \\beta_j + \\varepsilon_{ij} \\] where \\(\\alpha_i\\) gives the adjustment from the overall mean \\(\\mu\\) for the mean of treatment \\(i\\) and \\(\\beta_j\\) gives the adjustment for the mean of blend \\(j\\). This is a convenient was of expressing the model but there are actually more parameters here than we need. A simple solution is to set \\(\\alpha_1 = 0\\) so that the other treatment effects refer to the differences from treatment level 1. Similarly, we can set \\(\\beta_1 = 0\\). The error terms \\(\\varepsilon_{ij}\\) are assumed to be independent of one another, with a \\(N(0, \\sigma^2)\\) distribution. This is a classic randomised blocks design. As it is a simple linear model so the lm function can be used to fit it. model_tb &lt;- lm(yield ~ treat + blend, data = penicillin) summary(model_tb)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 90 2.744692 32.7905663 4.097853e-13 ## treatB 1 2.744692 0.3643396 7.219436e-01 ## treatC 5 2.744692 1.8216981 9.350597e-02 ## treatD 2 2.744692 0.7286793 4.801777e-01 ## blendBlend2 -9 3.068659 -2.9328774 1.253678e-02 ## blendBlend3 -7 3.068659 -2.2811269 4.159348e-02 ## blendBlend4 -4 3.068659 -1.3035011 2.168578e-01 ## blendBlend5 -10 3.068659 -3.2587527 6.844448e-03 Is this model appropriate? By giving each blend its own mean adjustment we are dealing with the variation in the particular blends we have used. However, these particular blends are really representing all the blends we might have to work with. How can our model reflect this? We could amend our model to: \\[ y_{ij} = \\mu + \\alpha_i + b_j + \\varepsilon_{ij} \\] where \\(b_j\\) now represents the random adjustment which applies when we select a particular blend. This can be expressed as \\[ b_j \\sim N(0, \\sigma^2_b) \\] where \\(\\sigma_b\\) quantifies the variation across blends for the whole population of blends. Our model has fixed effects which refer to the treatments, which involve application of the same processes every time, with parameters \\(\\beta_i\\); random effects which describe the effects of particular blends which will change every time the experiment is conducted, with the parameter \\(\\sigma_b\\), the standard deviation of these random effects. This combination is referred to as a mixed effects model. In order to fit this kind of model, we can bring to bear the powerful principle of likelihood. This assesses the plausibility of a particular set of parameter values in a model by evaluating the probability (or probability density) associated with the observed response data if these parameters were the ‘true’ ones. Consider first the randomised blocks model above. In a very general notation, we might write our model in terms of the distribution of the response data as \\[ f(y | \\alpha, \\beta, \\sigma^2) \\] where \\(y\\) denotes the vector of observed responses; \\(\\alpha\\) and \\(\\beta\\) denote the parameters in the model; \\(\\sigma^2\\) denotes the variance of the error term. The maximum likelihood estimates (MLEs) of the parameters are those values which maximise the likelihood. If we take this approach for the random effects model where the \\(\\beta\\) parameters are replaced by a vector random effects \\(b\\), we now have two random quantities to consider, with distribution \\[ f(y, b | \\alpha, \\sigma_b^2, \\sigma^2) \\] The trouble is that we don’t actually observe \\(b\\). One solution is to use the rules of probability to construct the marginal distribution of \\(y\\) as \\[ f(y | \\alpha, , \\sigma_b^2, \\sigma^2) = \\int_b f(y, b | \\alpha, \\sigma_b^2, \\sigma^2) db = \\int_b f(y | b, \\alpha, \\sigma_b^2, \\sigma^2) f(b) db \\] For a particular set of values, \\(b\\), we can evaluate \\(f(y | b, \\alpha, \\sigma_b^2, \\sigma^2)\\). Our model assumes a normal distribution for \\(f(b)\\). So this approach provides a route to a likelihood. One complication is that the maximum likelihood estimators of variances can be biased. The simplest example is the sample variance from a single sample of data, where the MLE is \\((\\sum_i (y_i - \\bar{y})^2 / n)\\) while the unbiased estimator has the divisor \\(n-1\\). This problem can be more pronounced in the context of random effects where multiple variances are involved, sometimes estimated from a relatively small number of instances of each random effect. A solution is to estimate the variances by integrating out the fixed effect parameters, \\(\\alpha\\), as well as the random effects \\(b\\). These estimated variances are then used in estimation of the fixed effect parameters. This is known as Restricted Maximum Likelihood, or REML, estimation. One consequence to be aware of is that REML estimation cannot be used when comparing models which have different fixed effect structures, as the likelihoods are then not comparable. Maximising likelihoods for mixed models is relatively straightforward in simple cases but much trickier for more complex models. There are efficient methods of numerical maximisation which can help. In practice, the lmer function in the lme4 package gives us a powerful mechanism for fitting models which include random effects. The code below applies this to the penicillin data. In the model formula, the term (1 | blend) indicates that we wish a random effect which allows a random adjustment to the mean for each blend. (The 1 is simply a symbol to indicate a mean.) We can now examine the estimates of the fixed parameters. As we suspected, these don’t suggest any strong treatment effects. We can also the estimates of the standard deviation of the blend variation as well as the standard deviation of the error associated with each observation. library(lme4) model_tr &lt;- lmer(yield ~ treat + (1 | blend), data = penicillin) summary(model_tr)$coefficients ## Estimate Std. Error t value ## (Intercept) 84 2.474874 33.9411255 ## treatB 1 2.744692 0.3643396 ## treatC 5 2.744692 1.8216981 ## treatD 2 2.744692 0.7286793 summary(model_tr)$varcor ## Groups Name Std.Dev. ## blend (Intercept) 3.4339 ## Residual 4.3397 "],["inference-with-mixed-models.html", "14.3 Inference with mixed models", " 14.3 Inference with mixed models In addition to its fundamental role in the estimation of parameters, likelihood also has a very general and powerful set of principles which allow us to contruct confidence intervals for parameters and assess the suitability of simpler, or more complex, versions of our current model. However, these methods are based on theory which may not always describe the situation well when we have mixed models. There are two broad approaches we might take to address this. The first makes some progress by computing appropriate ‘degrees of freedom’ when comparing models. The Kenward-Rogers approach is popular. However, there may still remain underlying issues that the assumed distribution of the model comparison test statistic is not correct. A second, very general, approach is to employ the bootstrap. If we repeatedly simulate new data from a fitted model, for each new dataset we can compute the model comparison statistic ((log-)likelihood ratio) for a more complex model. This builds up a picture of the distribution of our model comparison statistic when the simpler model is correct. This provides a reference distribution against which we can compare the value of the model comparison statistic on the original data. The PBmodcomp function from the pbkrtest package can help us with this. Recall that REML should not be used when models with different fixed effects are to be compared, so the function will refit the model using ML if necessary. library(pbkrtest) model_r &lt;- lmer(yield ~ (1 | blend), data = penicillin) PBmodcomp(model_tr, model_r) ## Bootstrap test; time: 20.25 sec; samples: 1000; extremes: 332; ## large : yield ~ treat + (1 | blend) ## stat df p.value ## LRT 4.0474 3 0.2564 ## PBtest 4.0474 0.3327 We can look at a slightly lower level of detail by displaying the simulated test statistics and comparing these with the one observed on our original data. Conveniently, the observed test statistic is available as one of the attributes of the simulations returned by PBrefdist. The attributes of refdist can be removed by applying the c function, before passing the vector to ggplot. refdist &lt;- PBrefdist(model_tr, model_r) lrtstat &lt;- attributes(refdist)$stat[&#39;tobs&#39;] ggplot() + geom_histogram(aes(c(refdist))) + geom_vline(xintercept = lrtstat, col = &#39;red&#39;) Halekoh et a. (2014) provide a systematic description of the pbkrtest package, and examples of its use. With a little more work, we can produce estimates of the individual random effects. ranef(model_tr)$blend ## (Intercept) ## Blend1 4.2878788 ## Blend2 -2.1439394 ## Blend3 -0.7146465 ## Blend4 1.4292929 ## Blend5 -2.8585859 How do these compare with the estimates of the blend effects in our earlier simple randomised blocks model? As the result of ranef is a dataframe, and the estimated effects are mean centred, we will first produce differences from Blend 1. reff &lt;- ranef(model_tr)$blend[ , 1] reff &lt;- reff - reff[1] reff[-1] ## [1] -6.431818 -5.002525 -2.858586 -7.146465 coefficients(model_tb)[5:8] ## blendBlend2 blendBlend3 blendBlend4 blendBlend5 ## -9 -7 -4 -10 Notice that the random effects model estimates the sizes of the random effects to be smaller. This effect is known as shrinkage. It is linked to the joint estimation of the random effects through a common distribution. "],["further-examples-of-mixed-models.html", "14.4 Further examples of mixed models", " 14.4 Further examples of mixed models 14.4.1 Childhood growth An initial model can be written as \\[ y_{ijk} = \\alpha + \\beta x_{ij} + \\gamma_k + \\varepsilon_{ij} , \\] where \\(y_{ijk}\\) denotes the \\(j\\)th measurement for subject \\(i\\), with the sex groups identified through the subscript \\(k\\). In addition to the regression effect on \\(x\\), the \\(\\gamma_k\\) capture the mean adjustment due to sex. The plot below superimposes the fitted model on the data. As expected, this model is too simple as it does not capture the variation across subjects. Some lie consistently above the regression line and other consistently below. ortho_model_0 &lt;- lm(distance ~ age + Sex, data = Orthodont) ggplot(Orthodont, aes(age, distance, col = Sex)) + geom_point() + geom_line(aes(y = fitted(ortho_model_0))) + facet_wrap(~ Subject) This can be addressed by introducing a random effect which allows an adjustment to the intercept for each subject: \\[ y_{ij} = (\\alpha + a_i) + \\beta x_{ij} + \\varepsilon_{ij}. \\] where \\(a_i \\sim N(0, \\sigma^2_a)\\). The plot below shows this to be a much better description of the data. The estimates of the parameters, with their standard errors, suggest that there is indeed a small sex effect, with a t-value around \\(3\\). We can also quantify the subject variation, which is estaimted to be larger than the residual variation. library(lme4) ortho_model_1 &lt;- lmer(distance ~ age + Sex + (1 | Subject), data = Orthodont) ggplot(Orthodont, aes(age, distance, col = Sex)) + geom_point() + geom_line(aes(y = fitted(ortho_model_1))) + facet_wrap(~ Subject) summary(ortho_model_1)$coefficients ## Estimate Std. Error t value ## (Intercept) 17.7067130 0.83392247 21.233044 ## age 0.6601852 0.06160592 10.716263 ## SexFemale -2.3210227 0.76141685 -3.048294 summary(ortho_model_1)$varcor ## Groups Name Std.Dev. ## Subject (Intercept) 1.8074 ## Residual 1.4316 We should also pay attention to whether the fixed effects may be more complex. In particular, is there interaction between age and sex? The analysis below does indicate eome evidence for this. ortho_model_2 &lt;- lmer(distance ~ age * Sex + (1 | Subject), data = Orthodont) ggplot(Orthodont, aes(age, distance, col = Sex)) + geom_point() + geom_line(aes(y = fitted(ortho_model_2))) + facet_wrap(~ Subject) summary(ortho_model_2)$coefficients ## Estimate Std. Error t value ## (Intercept) 16.3406250 0.9813122 16.6518101 ## age 0.7843750 0.0775011 10.1208235 ## SexFemale 1.0321023 1.5374208 0.6713206 ## age:SexFemale -0.3048295 0.1214209 -2.5105197 14.4.2 The strength of paste A mixed effects model allows different (hierarchical) levels of error to be constructed. Each batch has its own ‘adjustment’ from the overall mean. Within each batch, each cask has its own further ‘adjustment’ from the overall and batch means. Within each cask, the individual within-cask measurements can be viewed as further ‘adjustments’ from the overall, batch and cask means. If we index the batches, casks and within-cask measurements by \\(b\\), \\(c\\) and \\(w\\), then we can express this in a model as: \\[ y_{bcw} = \\mu + \\varepsilon_b + \\varepsilon_{bc} + \\varepsilon_{bcw} \\] where \\[ \\varepsilon_{b} \\sim N(0, \\sigma^2_b), \\hspace{3em} \\varepsilon_{bc} \\sim N(0, \\sigma^2_c), \\hspace{3em} \\varepsilon_{bcw} \\sim N(0, \\sigma^2_w) \\] The lme4 package gives us the tools to fit such a model. The estimates of standard deviation at batch, cask, and within-cask levels suggests that the cask level is where the variation is strongest. Another look at the plot shown earlier in this chapter does suggest that this is indeed the case. library(lme4) pastes_model &lt;- lmer(strength ~ 1 + (1|batch/cask), data = Pastes) summary(pastes_model)$coefficients ## Estimate Std. Error t value ## (Intercept) 60.05333 0.6768701 88.72209 summary(pastes_model)$varcor ## Groups Name Std.Dev. ## cask:batch (Intercept) 2.90408 ## batch (Intercept) 1.28737 ## Residual 0.82341 When using any model, we would like to have some reassurance that it describes the data adequately. Plot of residuals provide an informal, but very useful, way of checking this. If the model is adequate then the residuals should simply display random variation, with no obvious patterns or structure. The plot below reassures us that the model is an adequate fit. plot(pastes_model) 14.4.3 The sleep deprivation study Example: The sleep deprivation study Belenky et al. (2003) describe an experiment where the reaction times of subjects were recorded after different numbers of days of sleep deprivation. How should we model the relationship between reaction time and sleep deprivation while properly including a description of the sources of variation? A subset of the data reported by Belenky et al. (2003) is available in the sleepstudy dataframe of the lme4 package. Initial plots shows an upward trend in mean reaction time as the number of days os sleep deprivation increases. That is entirely expected. There is also considerable variability across subjects. library(lme4) str(sleepstudy) ## &#39;data.frame&#39;: 180 obs. of 3 variables: ## $ Reaction: num 250 259 251 321 357 ... ## $ Days : num 0 1 2 3 4 5 6 7 8 9 ... ## $ Subject : Factor w/ 18 levels &quot;308&quot;,&quot;309&quot;,&quot;310&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... library(ggplot2) ggplot(sleepstudy, aes(Days, Reaction, col = Subject)) + geom_line() ggplot(sleepstudy, aes(Days, Reaction)) + geom_point() + facet_wrap(~ Subject) A model which includes a fixed effect for Days and a random effect for Subject is a natural starting point. The fitted values for this model do not track the data very effectively, with some subjects showing a stronger regreession slope and others a weaker one. sleep_model_1 &lt;- lmer(Reaction ~ Days + (1 | Subject), data = sleepstudy) ggplot(sleepstudy, aes(Days, Reaction)) + geom_point() + geom_line(aes(y = fitted(sleep_model_1))) + facet_wrap(~ Subject) This suggests that we might do better with a model which allow random variations in both intercept and slope for each subject. The plot below suggests this to be a much better fit. \\[ y_{ij} = (\\alpha + a_i) + (\\beta + b_i) x_{ij} + \\varepsilon_{ij}. \\] where \\[ a_i \\sim N(0, \\sigma^2_a), \\hspace{3em} b_i \\sim N(0, \\sigma^2_b), \\hspace{3em} \\] sleep_model_2 &lt;- lmer(Reaction ~ Days + (Days | Subject), data = sleepstudy) ggplot(sleepstudy, aes(Days, Reaction)) + geom_point() + geom_line(aes(y = fitted(sleep_model_2))) + facet_wrap(~ Subject) The evidence for the superiority of the model with this form of random effects can be evaluated in a more formal model comparison. library(pbkrtest) PBmodcomp(sleep_model_2, sleep_model_1) ## Bootstrap test; time: 21.99 sec; samples: 1000; extremes: 0; ## Requested samples: 1000 Used samples: 999 Extremes: 0 ## large : Reaction ~ Days + (Days | Subject) ## stat df p.value ## LRT 42.139 2 7.072e-10 *** ## PBtest 42.139 0.001 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 14.4.4 Reading attainment in primary school Example: Reading attainment in primary school-children These data arose from a longitudinal study of a cohort of 407 pupils who entered 33 multi-ethnic inner London infant schools in 1982, and who were followed-up until the end of their junior schooling in 1989. The reading ability of pupils was tested on up to six occasions: annually from 1982 to 1986 and in 1989. Data are also available on the age of the pupils at the occasions when testing was performed and also their sex and ethnic group. The pupils took a variable number of the assessments and so the data are unbalanced. (Data source: Statistics in Education by Ian Plewis.) The data set has eight columns: School Number (1 to 33) Pupil Number (1 to 751) Assessment Occasion (1 to 6) Reading attainment score A standardised reading score (to be ignored here) Ethnic Group (white or black {African Caribbean}) Sex (boy or girl) Age (in years, but mean-centred) The questions of interest are: How does reading ability develop as children grow older? Does this development vary from pupil to pupil or from school to school? If so, does it vary systematically from one type of pupil to another (e.g. boys vs girls, white vs black or both), and according to the characteristics of the school? First we read the data. path &lt;- rp.datalink(&quot;reading&quot;) d &lt;- read.table(path, header = TRUE) ggplot graphics again provides a simple way of producing some attractive and helpful visualisations. library(ggplot2) ggplot(d, aes(Age, Readatt)) + geom_point() + facet_grid(Ethnicity ~ Sex) ggplot(d, aes(Age, Readatt, col = Sex)) + geom_point() + facet_wrap(~ School) We need to think carefully about how the variability should be modelled. For the moment, we will include only Age in the fixed effects, as this is the variable which clearly influences reading ability strongly. A random effect for each pupil, which allows them to display greater or lesser individual ability, is a good starting point. A residual plot helps us to assess the adequacy of this model, in an informal manner. Some non-random patterns suggest that we need to improve the model. library(lme4) model1 &lt;- lmer(Readatt ~ Age + (1 | School/Pupil), data = d) plot(model1) Another plot may help. This one shsows the trajectory of each individual over time. An interesting feature is a suggestion that these trajectories ‘fan out’ a little over time. This may suggest that pupils who are good readers at an early stage improve their reading ability over time at a greater rate than those who are poorer readers. This sounds plausible. ggplot(d, aes(Age, Readatt, group = Pupil)) + geom_line() + facet_wrap(vars(School)) We can incorporate this into a model by describing these trajectories through straight lines (plus random error) whose slopes are described by random effects. The lme syntax enables this to be specified easily. The residual plot nolonger shows non-random features and a formal test confirms that the simple random effects model is inadequate, in comparison with the model which allows random slopes. model2 &lt;- lmer(Readatt ~ Age + (Age | School/Pupil), data = d) plot(model2) The residual plot has improved. We might also confirm the suitability of the random slopes model by performing a more formal model comparison. anova(model1, model2) ## refitting model(s) with ML (instead of REML) ## Data: d ## Models: ## model1: Readatt ~ Age + (1 | School/Pupil) ## model2: Readatt ~ Age + (Age | School/Pupil) ## npar AIC BIC logLik -2*log(L) Chisq Df Pr(&gt;Chisq) ## model1 5 3793.3 3820.6 -1891.6 3783.3 ## model2 9 3202.7 3252.0 -1592.4 3184.7 598.57 4 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Now that our model has a good random effects structure, we can examine the fixed effects in greater detail. The summary gives no evidence of an ethnicity effect, but does show a sex effect with girls producing a better average performance, in addition to the age effect which we already know is present. model3 &lt;- lmer(Readatt ~ Age + Sex + Ethnicity + (Age | School/Pupil), data = d) summary(model3)$coefficients ## Estimate Std. Error t value ## (Intercept) 7.11786404 0.08078558 88.108100 ## Age 0.99302498 0.01567775 63.339762 ## Sexgirl 0.13267829 0.06133103 2.163314 ## Ethnicitywhite -0.09410643 0.06497229 -1.448409 It would be wise for us to examine whether interactions might be present. That is left as an exercise. As a further exercise, check what conclusions are reached if the random effects structure is simply ignored. References Belenky, Gregory, Nancy J Wesensten, David R Thorne, Maria L Thomas, Helen C Sing, Daniel P Redmond, Michael B Russo, and Thomas J Balkin. 2003. “Patterns of Performance Degradation and Restoration During Sleep Restriction and Subsequent Recovery: A Sleep Dose-Response Study.” Journal of Sleep Research 12 (1): 1–12. "],["non-linear-mixed-models.html", "14.5 Non-linear mixed models", " 14.5 Non-linear mixed models Indomethacin kinetics Pharmacokinetics is an area where non-linear models arise naturally. In this experiment, six human volunteers were injected with identical doses of the drug indomethacin and their plasma concentrations of the drug (in mcg/ml) were subsequently measured at 11 time points until 8 hours post-injection. How should the decay of the drug, and the variation across volunteers, be modelled? Source: Kwan, Breault, Umbenhauer, McMahon and Duggan (1976). Kinetics of Indomethicin absorption, elimination, and enterohepatic circulation in man. Journal of Pharmacokinetics and Biopharmaceutics, 4, 255-280. The data are available in the Indometh dataframe in the datasets package (which is usually supplied with R). Plots of the data is shown below. library(ggplot2) ggplot(Indometh, aes(time, conc, group = Subject, col = Subject)) + geom_line() ggplot(Indometh, aes(time, conc)) + geom_line() + facet_wrap(~ Subject) Pharmacokinetics often uses compartmental models which result in descriptions of decay over time in the form of sums of exponential terms. For this example, two exponential terms should suffice. If \\(y_{ij}\\) denotes the measurement on individual \\(i\\) at time point \\(j\\), then \\[ y_{ij} = \\beta_{1i} \\exp(-\\beta_{2i} t_j) + \\beta_{3i} \\exp(-\\beta_{4i} t_j) + \\varepsilon_{ij}, ~~~~~~ \\beta_2&gt;0, \\beta_4&gt;0. \\] In order to make the problem identifiable, we can insist that \\(\\beta_{2i} &gt; \\beta_{4i}\\) for each \\(i\\). The left hand plot below, shows the results of fitting a single regression model to all the data, while the right hand plot fits a separate regression for each subject. model &lt;- nls(conc ~ SSbiexp(time, A1, lrc1, A2, lrc2), data = Indometh) ggplot(Indometh, aes(time, conc)) + geom_point(aes(col = Subject)) + geom_line(aes(y = fitted(model)), col = &#39;red&#39;) library(nlme) model &lt;- nlsList(conc ~ SSbiexp(time, A1, lrc1, A2, lrc2), data = Indometh) ggplot(Indometh, aes(time, conc, group = Subject)) + geom_point(aes(col = Subject)) + geom_line(aes(y = fitted(model), col = Subject)) A random effects model can be constructed as \\[ y_{ij} = (\\beta_{1} + b_{1i}) \\exp(-\\exp(\\beta_{2} + b_{2i}) \\, t_j) + (\\beta_{3} + b_{3i}) \\exp(-\\exp(\\beta_{4} + b_{4i}) \\, t_j) + \\varepsilon_{ij}, \\] where the additional use of \\(\\exp\\) ensures that the coefficients inside the original \\(\\exp\\) terms are positive. The code below shows what it is possible to do in nlme. The Pinheiro &amp; Bates book explains this in greater detail and suggests some further amendments to the random effects and variance structure of the model. model &lt;- nlme(model, random = pdDiag(A1 + lrc1 + A2 + lrc2 ~ 1)) summary(model) ## Nonlinear mixed-effects model fit by maximum likelihood ## Model: conc ~ SSbiexp(time, A1, lrc1, A2, lrc2) ## Data: Indometh ## AIC BIC logLik ## -91.19342 -71.48653 54.59671 ## ## Random effects: ## Formula: list(A1 ~ 1, lrc1 ~ 1, A2 ~ 1, lrc2 ~ 1) ## Level: Subject ## Structure: Diagonal ## A1 lrc1 A2 lrc2 Residual ## StdDev: 0.5714106 0.1580778 0.1115978 8.179593e-06 0.08149341 ## ## Fixed effects: list(A1 ~ 1, lrc1 ~ 1, A2 ~ 1, lrc2 ~ 1) ## Value Std.Error DF t-value p-value ## A1 2.8275372 0.2640124 57 10.709866 0e+00 ## lrc1 0.7736221 0.1100261 57 7.031262 0e+00 ## A2 0.4614716 0.1128084 57 4.090755 1e-04 ## lrc2 -1.3441022 0.2310754 57 -5.816725 0e+00 ## Correlation: ## A1 lrc1 A2 ## lrc1 0.055 ## A2 -0.102 0.630 ## lrc2 -0.139 0.577 0.834 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.1733831 -0.3562727 -0.1285253 0.3423165 3.0025096 ## ## Number of Observations: 66 ## Number of Groups: 6 plot(model) "],["generalised-linear-mixed-models.html", "14.6 Generalised linear mixed models", " 14.6 Generalised linear mixed models A GLMM aims to combine the structures of a GLM and a mixed model. The idea os that the linear predictor of a GLM is extended to include random effecs. This sounds easy but the presence of the link function makes it computationally rather challenging. The ctsib in the faraway package provides data on the ability of subjects to maintain stability under different surface and vision conditions. We will follow Faraway (2016) by creating a variable which simply indicates whether the subject was stable (1) or not (0). data(ctsib, package = &#39;faraway&#39;) ctsib$stable &lt;- ifelse (ctsib$CTSIB == 1, 1, 0) str(ctsib) ## &#39;data.frame&#39;: 480 obs. of 9 variables: ## $ Subject: int 1 1 1 1 1 1 1 1 1 1 ... ## $ Sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ Age : int 22 22 22 22 22 22 22 22 22 22 ... ## $ Height : num 176 176 176 176 176 176 176 176 176 176 ... ## $ Weight : num 68.2 68.2 68.2 68.2 68.2 68.2 68.2 68.2 68.2 68.2 ... ## $ Surface: Factor w/ 2 levels &quot;foam&quot;,&quot;norm&quot;: 2 2 2 2 2 2 1 1 1 1 ... ## $ Vision : Factor w/ 3 levels &quot;closed&quot;,&quot;dome&quot;,..: 3 3 1 1 2 2 3 3 1 1 ... ## $ CTSIB : int 1 1 2 2 1 2 2 2 2 2 ... ## $ stable : num 1 1 0 0 1 0 0 0 0 0 ... For explanatory variables which are factors we can simply calculate the proportions of cases which are stable at each level. with(ctsib, proportions(table(stable, Vision), margin = 2)) with(ctsib, proportions(table(stable, Surface), margin = 2)) with(ctsib, proportions(table(stable, Sex), margin = 2)) ## Vision ## stable closed dome open ## 0 0.89375 0.86250 0.53125 ## 1 0.10625 0.13750 0.46875 ## Surface ## stable foam norm ## 0 0.95833333 0.56666667 ## 1 0.04166667 0.43333333 ## Sex ## stable female male ## 0 0.8041667 0.7208333 ## 1 0.1958333 0.2791667 For explanatory variables which are continuous, it helps to use geom_jitter to add some random noise as a visial device to counteract overplotting of points. A smooth trend line can also help to highlight any underlying patterns. There don’t seem to strong effects of the covariates. ggplot(ctsib, aes(Age, stable)) + geom_jitter(width = 0.2, height = 0.03) + geom_smooth() ggplot(ctsib, aes(Height, stable)) + geom_jitter(width = 0.2, height = 0.03) + geom_smooth() ggplot(ctsib, aes(Weight, stable)) + geom_jitter(width = 0.2, height = 0.03) + geom_smooth() The glmer function in the lme4 package allows us to fit a model which includes random effects. The information on the model coefficients suggests that there are indeed effects of Surface and Vision but that there is little evidence that the other variables are linked to the patterns of stability. library(lme4) model &lt;- glmer(stable ~ Surface + Vision + Sex + Age + Height + Weight + (1 | Subject), family = binomial, nAGQ = 25, data = ctsib) summary(model)$coefficients ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 16.171235022 12.72282819 1.27104090 2.037141e-01 ## Surfacenorm 7.285383867 1.05515484 6.90456374 5.035794e-12 ## Visiondome 0.675898619 0.52736491 1.28165262 1.999645e-01 ## Visionopen 6.088910517 0.97240581 6.26169700 3.808103e-10 ## Sexmale 3.096645061 1.69615612 1.82568399 6.789788e-02 ## Age -0.006672407 0.07645747 -0.08726953 9.304573e-01 ## Height -0.192262598 0.08896009 -2.16122300 3.067812e-02 ## Weight 0.075159408 0.05910398 1.27164710 2.034985e-01 We can confirm that there is little additional explanatory power in these variables by fitting a simpler model which omits them and carryong ourt a model comparison. model_sv &lt;- glmer(stable ~ Surface + Vision + (1 | Subject), family = binomial, nAGQ = 25, data = ctsib) anova(model, model_sv) ## Data: ctsib ## Models: ## model_sv: stable ~ Surface + Vision + (1 | Subject) ## model: stable ~ Surface + Vision + Sex + Age + Height + Weight + (1 | Subject) ## npar AIC BIC logLik -2*log(L) Chisq Df Pr(&gt;Chisq) ## model_sv 5 247.30 268.17 -118.65 237.30 ## model 9 247.93 285.49 -114.96 229.93 7.3725 4 0.1175 "],["further-reading-4.html", "14.7 Further reading", " 14.7 Further reading A technical paper on lme4 is avalable. Gałecki &amp; Burzykowski: Linear Mixed-Effects Models using R. See nlmeU package with datasets. The book is at: https://www.cartagena99.com/recursos/alumnos/apuntes/Galecki2013Selection.pdf "],["flexible-models.html", "15 Flexible models", " 15 Flexible models ` "],["flexible-methods.html", "15.1 Methods for constructing flexible models", " 15.1 Methods for constructing flexible models In the previous lecture, some ways of constructing flexible (or ‘nonparametric’) regression curves were described. There are many ways of doing this. Here are some further possibilities. 15.1.1 Fitting models locally 15.1.2 Splines In mathematics, a spline denotes a function \\(g(x)\\) which is essentially a piecewise polynomial over an interval \\((a, b)\\), such that a certain number of its derivatives are continuous for all points of \\((a, b)\\). More precisely, \\(g(x)\\) must satisfy the following properties. For some given positive integer \\(r\\) and a sequence of points \\(t_1,\\ldots,t_k\\), called knots, such that \\(a&lt;t_1&lt;\\ldots&lt;t_k&lt;b\\), it is required that: \\(g(x)\\) has \\(r-2\\) continuous derivatives; the \\((r-1)\\)th derivative of \\(g(x)\\) is a step function with jumps at \\(t_1,\\ldots,t_k\\). Often \\(r\\) is chosen to be \\(3\\), and the term cubic spline is then used for the associated curve. In statistics, splines are used not for data interpolation, as in numerical analysis, but for data smoothing. Specifically, the quantity \\[ D = \\sum_{i=1}^n \\left(y_i - s(x_i) \\right)^2 + \\lambda \\int_a^b s&#39;&#39;(x)^2 dx , \\] where \\(\\lambda\\) is some positive constant, is used as an objective function in a minimisation problem with respect to the unknown regression function \\(s(x)\\) on the basis of the data \\((x_i, y_i)\\). The connection between \\(D\\) and splines is that the mathematical function which minimises \\(D\\) is a cubic spline, whose specific expression depends on the data and on the choice of \\(\\lambda\\). If \\(\\lambda=0\\), minimisation of \\(D\\) corresponds to interpolation of the data, which is not a useful method of fitting the model, since the residuals are all set equal to \\(0\\). To avoid this, the second term in the expression of \\(D\\) is then inserted as a roughness penalty. The choice of \\(\\lambda\\) determines the relative weight attributed to the two terms, the residual sum of squares and the roughness penalty \\(\\int s&#39;&#39;(x)^2 dx\\). Increasing \\(\\lambda\\) penalises fluctuations, and so produces a smoother curve. The term penalised least squares is then used in conjunction with \\(D\\). If \\(\\lambda\\to\\infty\\), the second derivative is effectively constrained to be 0, and the outcome is then the least squares line. Hence, \\(\\lambda\\) plays a similar role to the smoothing parameter \\(h\\) used in earlier sections. The philosophy underlying this approach is an elegant and powerful one and it can be modified to apply to many other situations. Extensive discussions are given by Eubank (1988), Wahba (1990), Green and Silverman (1994) and Wood (2017). Another way of estimating a smooth regression curve is to use a set of functions which provide a set of local building blocks. For example b-splines, which are made up on polynomial pieces, provide a convenient and efficient option. A curve estimate can then be produced by fitting the regression \\[ Y_i = \\beta_0 + \\beta_1 b_1(x_i) + \\beta_2 b_2(x_i) + \\ldots + \\beta_p b_p(x_i) + \\varepsilon_i , \\] where the \\(b_j\\) are the local b-spline functions. The \\(b_j\\) are centred at different positions along a regular grid of \\(x\\) values. This creates a regression problem of the usual form \\(y = B \\beta + \\varepsilon\\), where \\(y\\) denotes the vector of response values, \\(\\beta\\) is a vector of unknown coefficients and \\(B\\) is a matrix whose columns evaluate each basis function at the observed values of \\(x\\) (with an initial column of 1’s, as usual, to deal with the intercept term). The solution \\(\\hat\\beta = (B^TB)^{-1}B^Ty\\) arises from standard linear model theory. Of course, one important question is how many basis functions should be used. An attractive solution is to use a large number of basis functions but to control the complexity, or smoothness, of the model through a penalty function, in the manner discussed in smoothing splines above. Eilers &amp; Marx (2020) describe this approach in detail. Rather than use the second derivative of the regression function as the basis of the penalty, there are advantages in penalising the sequence of coefficients through a term such as \\(\\lambda \\sum_{j=2}^p (\\beta_j - \\beta_{j-1})^2\\). Second-order differences can also be used and, indeed, that is a more common strategy. The penalised least squares problem then becomes to find \\(\\beta\\) to minimise \\[ (y - B\\beta)^T(y - B \\beta) + \\lambda \\beta^T D^T D \\beta , \\] where \\(D\\) is a differencing matrix. This leads to the solution \\[ \\hat\\beta = (B^TB + \\lambda P)^{-1}B^Ty \\] where \\(P\\) denotes the penalty matrix \\(D^TD\\). This is referred to as a p-spline model. If this estimate is written in the form \\(\\hat{y} = S y\\) then the concept of approximate degrees of freedom, discussed earlier as \\(tr(S)\\), can be applied. This gives a more intuitive scale of complexity. Specifying the degrees of freedom leads to a unique corresponding \\(\\lambda\\). The earlier concept of cross-validation, or the general principal of AIC, can be applied to provide a more automatic proposal for the amount of smoothing applied. "],["inference-with-simple-flexible-models.html", "15.2 Inference with simple flexible models", " 15.2 Inference with simple flexible models Historical data on Winter NAO data was discussed in Chapter on Trends and Patterns. If we fit a linear trend this is not significant. We can ask similar questions of the flexible regression: is there evidence of change of any kind?; is there evidence that this change is not linear? The flexible regression plots below display ‘reference bands’ which indicate where we expect the smooth curve to lie if indeed there is no trend at all, and in the second case if there is only linear trend. These graphical displays can be backed up by more formal methods which compare the sums-of-squares associated with the ‘no effect’, linear and smooth models. The calculation to produce p-values are a little more complex than in the standard case of linear models. library(rpanel) library(tidyverse) library(sm) path &lt;- rp.datalink(&#39;NAO&#39;) NAO &lt;- read.table(path, header = FALSE, skip = 1) %&gt;% dplyr::select(Year = 1, NAO = 2) ggplot(NAO, aes(Year, NAO)) + geom_point() + geom_smooth(method = &#39;lm&#39;) summary(lm(NAO ~ Year, data = NAO))$coefficients ggplot(NAO, aes(Year, NAO)) + geom_point() + geom_smooth() sm.regression(NAO$Year, NAO$NAO, model = &#39;no effect&#39;) sm.regression(NAO$Year, NAO$NAO, model = &#39;linear&#39;) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -10.812202830 6.469981059 -1.671134 0.09667481 ## Year 0.005689807 0.003328096 1.709628 0.08929780 ## Test of no effect model: significance = 0.015 ## Test of linear model: significance = 0.026 "],["a-simple-additive-model.html", "15.3 A simple additive model", " 15.3 A simple additive model Now that we have tools available to estimate smooth curves and surfaces, linear regression models can be extended to additive models as \\[ y_i = \\beta_0 + s_1(x_{1i}) + \\ldots + s_p(x_{pi}) + \\varepsilon_i, \\hspace{15mm} i=1, \\dots , n. \\] The \\(m_i\\) are functions whose shapes are unrestricted, apart from an assumption of smoothness and the constraint, for identifiability, that \\(\\sum_i s_{j}(x_{ji}) = 0\\) for each \\(j\\). As a consequence, we usually estimate \\(\\beta_0\\) by \\(\\bar{y}\\). This gives a very flexible set of modelling tools. To see one approach to how these models can be fitted, consider the case of only two covariates, \\[ y_i = \\beta_0 + s_1(x_{1i}) + s_2(x_{2i}) + \\varepsilon_i, \\hspace{1.5cm} i=1, \\dots , n. \\] A rearrangement of this as \\(y_i - \\beta_0 - s_2(x_{2i}) = s_1(x_{1i}) + \\varepsilon_i\\) suggests that an estimate of component \\(m_1\\) can then be obtained by smoothing the residuals of the data after fitting \\(\\hat{s}_2\\), \\[ \\hat{m}_1 = S_1 (y - \\bar{y} - \\hat{s}_2) \\] and that, similarly, subsequent estimates of \\(s_2\\) can be obtained as \\[ \\hat{m}_2 = S_2 (y - \\bar{y} - \\hat{s}_1) . \\] These smoothing operations are repeated until convergence. This is called the backfitting algorithm. However, the regression and p-spline approaches described above allow models to be fitted directly. Each term \\(s_1, s_2, \\ldots\\) can be represented by its own set of basis functions to create the usual linear model form \\(y = B \\beta\\), where \\(\\beta\\) now denotes a collections of several sets of coefficients, one for each term, and the columns of \\(B\\) similarly cover the basis functions for all terms in the model. Combined with a set of penalty matrices and associated penalty parameters, the direct solution \\(\\hat\\beta = (B^TB + P)^{-1}B^Ty\\) still applies, where \\(P\\) now denotes the sum of the individual penalty matrices weighted by their associated penalty parameters. Here is an example of an additive model fitted to the trawl data. This uses the gam function in the mgcv package in R, which is a very powerful set of tools comprehensively described by Wood (2017). There are many options for how the model is fitted but here we will simply accept the defaults. library(sm) library(mgcv) library(gratia) ind &lt;- (trawl$Year==0 &amp; trawl$Zone==1) trawl.model &lt;- gam(Score1 ~ s(Latitude) + s(Longitude), data = trawl, subset = ind) draw(trawl.model, residuals = TRUE) The non-linear effect of Longitude is apparent, as we saw in earlier examples. Here the model selection methods included in mgcv have led to a proposed linear relationship with Latitude. "],["inference-with-additive-models.html", "15.4 Inference with additive models", " 15.4 Inference with additive models While models of this type provide very flexible and visually informative descriptions of the data, it is also necessary to consider how models can be compared and inferences drawn. Although we are outside the strict realm of a standard linear model, as a result of the smoothness constraints, we generally proceed by analogy with the linear model. For an additive model, the residual sum-of-squares can easily be defined as \\[ \\mbox{RSS} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2, \\] where \\(\\hat{y}_i\\) denotes the fitted value, produced by evaluating the additive model at the observation \\(x_i\\). We can write the residual sum-of-squares as \\[ \\mbox{RSS} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = y^T (I-S)^T (I-S) y, \\] where \\(S\\) denotes the projection matrix discussed earlier. The approximate degrees of freedom for error can be defined as \\[ \\mbox{df} = \\mbox{tr}\\{(I-S)^T (I-S)\\} . \\] In an obvious notation, comparisons of two models can expressed quantitatively in \\[ F = \\frac{(\\mbox{RSS}_2-\\mbox{RSS}_1) / (\\mbox{df}_2-\\mbox{df}_1)} {\\mbox{RSS}_1 / \\mbox{df}_1} , \\label{eq:add_F} \\] by analogy with the \\(F\\)-statistic used to compare linear models. Unfortunately, this analogy does not extend to distributional calculations and no general expression for the distribution of the F-statistic is available. However, Hastie &amp; Tibshirani (1990; sections 3.9 and 6.8) suggest that at least some approximate guidance can be given by referring the observed nonparametric \\(F\\)-statistic to an F distribution with \\((\\mbox{df}_2-\\mbox{df}_1)\\) and \\(\\mbox{df}_1\\) degrees of freedom. A different approach is to examine whether particular groups of coefficients in the regression spline, for example those associated with the building blocks for a particular term in the additive model, might all be zero. The details of this are comprehensively discussed in Wood (2017) and this is the approach implemented in mgcv. The reef data provide a simple illustration of how model comparisons may be made. The table below indicates that both Latitude and Longitude show significant effects on the catch score. summary(trawl.model)$s.table ## edf Ref.df F p-value ## s(Latitude) 1.000000 1.000000 9.648493 0.003879748 ## s(Longitude) 7.023069 8.064042 26.768506 0.000000000 Is the additive model sufficient or do we need an interaction term (which would simply create a smooth surface over Latitude and Longitude simultaneously)? We can examine that by adding an interaction term. The evidence for its presence is not convincing. ind &lt;- (trawl$Year==0 &amp; trawl$Zone==1) trawl.model1 &lt;- gam(Score1 ~ s(Latitude) + s(Longitude) + ti(Latitude, Longitude), data = trawl, subset = ind) summary(trawl.model1)$s.table ## edf Ref.df F p-value ## s(Latitude) 1.000000 1.000000 6.213004 0.0196678 ## s(Longitude) 7.799202 8.444384 23.363779 0.0000000 ## ti(Latitude,Longitude) 6.972077 8.728649 1.154443 0.3665392 "],["further-reading-5.html", "15.5 Further reading", " 15.5 Further reading Bowman, A.W. &amp; Azzalini, A. (1996). Applied Smoothing Techniques for Data Analysis. OUP: Oxford. Green, P.J. and Silverman, B.W. (1994). Nonparametric Regression and Generalized Linear Models: A Roughness Penalty Approach. Chapman &amp; Hall: London. Hastie, T. and Tibshirani, R. (1990). Generalized Additive Models. Chapman &amp; Hall: London. Eubank, R.L. (1999) Nonparametric regression and spline smoothing. Marcel Dekker, New York. Wood, S. (2017). Generalized additive models: an introduction with R. 2nd edition. Chapman &amp; Hall/CRC, London. Eilers, P.H.C. and Marx, B.D. (2020). Practical Smoothing: the joy of p-splines. Cambridge University Press. "],["exercises-10.html", "15.6 Exercises", " 15.6 Exercises 15.6.1 Fitting a p-spline model Verify the solution given earlier for the value of \\(\\beta\\) which minimises the p-splines criterion \\[ (y - B\\beta)^T(y - B \\beta) + \\lambda \\beta^T D^T D \\beta , \\] 15.6.2 The backfitting algorithm The sm.regression function in the sm package estimates a flexible regression curve for a single covariate and a response variables. (You can use any other function which does the same thing, if you prefer.) Write code which implements the backfitting algorithm, discussed earlier, and apply this to an additive model which relates Score1 to Latitude and Longitude in the trawl data. You will need a convergence criterion. You might simply use a small proportionate change in the fitted values. You will need to consult the help file for sm.regression to see what arguments are required and what is returned by the function. 15.6.3 Water quality in the Clyde estuary The R instructions below will read a dataset containing measurements of dissolved oxygen (DO) in the River Clyde and then create a dataframe clyde4 containing a subset for a particular sampling location. path &lt;- rp.datalink(&quot;DO_Clyde&quot;) load(path) clyde4 &lt;- subset(clyde, Station == 4) Use the gam function in mgcv to fit an additive model which relates DO to the covariates Year, Temperature and Salinity. Comment on what the fitted modell tells you about these relationships. "],["models-for-time.html", "16 Models for time ", " 16 Models for time "],["time-series-data.html", "16.1 Time series data", " 16.1 Time series data The weather on Ben Nevis At 1345m above sea level, Ben Nevis is the highest mountain in the UK and the weather at the summit can be very severe. In the 19th century, a remarkable effort was made to document conditions at the summit. Following a public appeal, an observatory was set up on the summit to allow a small group of meteorologists to make recordings. This was done hourly from 1883 to 1904, producing one of the most remarkable weather datasets from the Victorian era. Hawkins et al. (2019) describe a recent citizen science project which has made the data available in digital form. The data are available from the CEDA Archive which contains a very large collection of environmental data from atmospheric and earth observation research. path &lt;- rp.datalink(&quot;Ben_Nevis&quot;) Ben_Nevis &lt;- read.csv(path) %&gt;% rename(Tmin = 5, Tmax = 6) %&gt;% mutate(Date = as.Date(paste(Year, Month, Day), format = &quot;%Y %m %d&quot;), yday = yday(Date), year = decimal_date(Date)) ggplot(Ben_Nevis, aes(Date, Tmax)) + geom_line() + ylim(-20, 20) ggplot(Ben_Nevis, aes(Date, Tmin)) + geom_line() + ylim(-20, 20) In time series data, we are often interested in whether there is trend or seasonal effects and what these look like. We may also be interested in the influence of covariates. These may apply to the whole time series, such as characteristics of the location at which different time series are recorded, or they may themselves be time series running in parallel. Here it is clear that there is a strong seasonal effect. How should we model this as its shape is clearly non-linear? We might use a trigonometric expression in the day of the year (yday) such as \\(\\beta cos(2 \\pi (yday - \\nu) / 366)\\), where the parameter \\(\\nu\\) corresponds to the day of the year at which the temperature is largest and \\(\\beta\\) fixes the amplitude of tbe seasonal effect. In fact, if we expand the expression by trigonometric identities then it turns out that this can be expressed as \\(\\beta_s sin(2 \\pi \\, yday / 366) + \\beta_s cos(2 \\pi \\, yday / 366)\\). This expression is linear in the parameters \\(\\beta_s\\) and \\(\\beta_c\\), using the new variables \\(cos(2 \\pi \\, yday / 366)\\) and \\(sin(2 \\pi \\, yday / 366)\\). This is a very convenient device for fitting a simple form of seasonal effect. The plot below sugests that we have made a start but that a more complex description of the seasonal effect may be needed. This could be pursued by employing sin and cos functions of higher frequency, but a different appraoch will be described later in this chapter. Ben_Nevis$cs &lt;- cos(2 * pi * Ben_Nevis$yday / 366) Ben_Nevis$sn &lt;- sin(2 * pi * Ben_Nevis$yday / 366) model &lt;- lm(Tmax ~ cs + sn, data = Ben_Nevis) summary(model) ## ## Call: ## lm(formula = Tmax ~ cs + sn, data = Ben_Nevis) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.492 -2.512 -0.236 2.372 13.219 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.60724 0.04087 39.33 &lt;2e-16 *** ## cs -4.59861 0.05776 -79.61 &lt;2e-16 *** ## sn -2.40571 0.05783 -41.60 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.565 on 7606 degrees of freedom ## Multiple R-squared: 0.5159, Adjusted R-squared: 0.5158 ## F-statistic: 4053 on 2 and 7606 DF, p-value: &lt; 2.2e-16 ggplot(Ben_Nevis, aes(Date, Tmax)) + geom_point() + ylim(-20, 20) + geom_line(y = fitted(model), col = &#39;red&#39;) ggplot(Ben_Nevis, aes(Date, residuals(model))) + geom_point() A statistical feature which makes time series distinctive is that there is often autocorrelation, meaning that the variation or random component of successive points tend to be linked to the variation at adjacent points. The autocorrelation and partial autocorrelation function, displayed below, quantify this, with horizontal dashed lines to indicate the likely range of variation when the true correlations are zero. The partial autocorrelation function is more useful as it fits simple autoregressive models of different order and in this case there seems to be a strong indication that a model of prder 1 is needed. acf(residuals(model)) pacf(residuals(model)) The nlme package offers an alternative set of tools for constructing mixed model and for dealing with different forms of variation more generally. The gls function allows correlation to be accommodated. Notice that the standard errors for the model terms are larger than in the earlier model. This reflects the fact that the presence of correlation reduced the information available in the observations. library(nlme) model &lt;- gls(Tmax ~ cs + sn, data = Ben_Nevis, correlation = corAR1()) summary(model)$tTable ## Value Std.Error t-value p-value ## (Intercept) 1.607650 0.1057893 15.19672 2.103881e-51 ## cs -4.598474 0.1492943 -30.80140 1.977170e-196 ## sn -2.406502 0.1494328 -16.10424 2.104182e-57 We have assumed that there is no long term trend in the series. This can be chacked by adding a term for year, on a continuous scale. The result does not provide convincing evidence that we need this, so we might revert to the model which has a seasonal effect, with correlated errors. The plot of the fitted values is indistinguishable from the plot of the model without correlation. library(nlme) model_trend &lt;- gls(Tmax ~ year + cs + sn, data = Ben_Nevis, correlation = corAR1()) summary(model_trend)$tTable ## Value Std.Error t-value p-value ## (Intercept) -61.66373513 33.2285989 -1.855743 6.352888e-02 ## year 0.03340042 0.0175410 1.904134 5.693044e-02 ## cs -4.59011863 0.1489906 -30.808103 1.650794e-196 ## sn -2.40180011 0.1490847 -16.110307 1.914231e-57 References Hawkins, Ed, Stephen Burt, Philip Brohan, Michael Lockwood, Harriett Richardson, Marjory Roy, and Simon Thomas. 2019. “Hourly Weather Observations from the Scottish Highlands (1883–1904) Rescued by Volunteer Citizen Scientists.” Geoscience Data Journal 6 (2): 160–73. https://doi.org/10.1002/gdj3.79. "],["visualisation.html", "16.2 Visualisation", " 16.2 Visualisation Many of the usual approaches to visualisation apply to time series data but there are some methods which are particularly suitable when we expect to see a seasonal or periodic effect. The stl function is one example. This decomposes the time series into a trend, a seasonal component and residuals, using a method of flexible regression (loess). This is applied below to the Mauna Loa records of atmospheric CO\\(_2\\). Notice that the functions expects a special time series structure, created by the ts function. path &lt;- rp.datalink(&quot;CO2_Mauna_Loa&quot;) d.mlo &lt;- read.csv(path, skip = 64, header = FALSE) names(d.mlo) &lt;- c(&quot;Year&quot;, &quot;Mn&quot;, &quot;Date1&quot;, &quot;Date&quot;, &quot;CO2&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;Sta&quot;) d.mlo$CO2 &lt;- as.numeric(d.mlo$CO2) d.mlo$CO2[d.mlo$CO2 &lt; 0] &lt;- NA ggplot(d.mlo, aes(Date, CO2)) + geom_line() mlo.ts &lt;- ts(d.mlo$CO2[78:786], start = c(1883, 6), frequency = 12) plot(stl(mlo.ts, s.window = &quot;periodic&quot;)) Exercise: Try the stl decomposition on the Ben Nevis data. Ben_Nevis &lt;- mutate(Ben_Nevis, yday = yday(Date)) Ben_Nevis.ts &lt;- ts(Ben_Nevis$Tmax, start = c(1883, 335), frequency = 365) "],["models.html", "16.3 Models", " 16.3 Models There are well developed tools for modelling the random variation in a time series, including autoregressive and moving average approaches. In many modelling situations it is trend and seasonal effects, along with the influence of potential covariates, which dominate, but the modelling process should incorporate a suitable description of the random variation. Inferences drawn about the effects of interest may be misleading of the random component is not suitably modelled. The stl visualisation approach discussed above focussed on the estimation of trend and seasonal effects. This can be implemented in a wider gam framework which includes inferential tools. The code below fits trend and seasonal effect (using bs = 'cc') in a gam model for the Ben_Nevis data and then assesses the residuals for the presence of autocorrelation. First order correlation looks strong, suggesting that we may need an autoregressive component of opder 1. library(mgcv) Ben_Nevis &lt;- mutate(Ben_Nevis, year = Year + yday / 366) model &lt;- gam(Tmax ~ s(year) + s(yday, bs = &#39;cc&#39;), data = Ben_Nevis) plot(model) plot(residuals(model), fitted(model)) pacf(residuals(model)) What happens if we add a time series component to our model. We can do this through the gamm function. With a model of some complexity and with such a long time series, the fitting process takes some time. model &lt;- gamm(Tmax ~ s(year) + s(yday, bs = &#39;cc&#39;), correlation = corAR1(form = ~ year), data = Ben_Nevis) plot(model) plot(residuals(model$gam), fitted(model$gam)) pacf(residuals(model$gam)) Exercise: try this on the NAO data. Does the use of AR(1) correlation change the estimate of trend? "],["further-reading-6.html", "16.4 Further reading", " 16.4 Further reading Repeated measurements dealt with in random effects. Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and George Athanasopoulos https://otexts.com/fpp2/ "],["exercises-11.html", "16.5 Exercises", " 16.5 Exercises 16.5.1 Simulating time series The code below explores what time series structures look like, as well as the effectiveness of fitting models, incluidng smooth trends plus time series errors. There could be a useful exercise to explore the effectiveness of gamm in this setting. Plot the estimates of the correlation parameter etc. library(mgcv) n &lt;- 100 fn &lt;- function(x) cos(2 * pi * x / n) rho &lt;- 0.5 x &lt;- 1:n y &lt;- arima.sim(model = list(order = c(1, 0, 0), ar = rho), n = n) y &lt;- fn(x) + y library(tidyverse) ggplot(data.frame(y), aes(x = y, y = stats::lag(y))) + geom_line() # plot(y) # plot(y, lag(y)) model &lt;- gamm(y ~ s(x), correlation = corARMA(form = ~ x, p = 1)) plot(model$gam) curve(fn, 1, n, add = TRUE, col = &#39;blue&#39;) coef(model$lme$modelStruct$corStruct, unconstrained = FALSE) ## Phi ## 0.4116627 "],["models-for-space.html", "17 Models for space", " 17 Models for space We are all familiar with maps - indeed, we often take them for granted. In fact they are a device for encoding a very large amount of information very concisely and efficiently. Spatial data arises when we take measurements which identify spatial position. Some standard types of problems and data which arise: spatial position is the key focus (point patterns); values are measured at specified locations (geostatistical data); data is available in image form (pixellated); values are available for sub-regions (chloropleth data). Why are spatial data special? Characteristics include: standard plotting does not respect the meaning of distance in each co-ordinate; it is often helpful to include map and/or topographic information to give spatial context; there can be special features of spatial variation . "],["visualising-spatial-data.html", "17.1 Visualising spatial data", " 17.1 Visualising spatial data There are many powerful tools for plotting spatial data and some of these are mentioned in this section. Many of these are also able to handle some of the special considerations neded for spatial data. The co-ordinate reference system (CRS) is one of those. Latitude and longitude are standard co-ordinates but data can often be spatially indexed in other ways, for example through Easting and Northing in a local reference system. Another issue is the use of shapefiles which contain the boundaries of regions of interest and other information. The sf package provides support for the simple features standard for representing spatial information. 17.1.1 The maps package This package provides some basic tools for drawing countries and regions on standard plots. In the chapter on flexible models, data on SO\\(_2\\) across Europe was used as an example. The code below uses the map function to plot the locations of othe active monitoring stations in January 1990. Notice the use of :: because the function map is present in both the maps package and the tidyverse. library(rpanel) library(tidyverse) library(maps) SO2_1990 &lt;- SO2 %&gt;% filter(year == 1990 &amp; month == 1) with(SO2, { maps::map(xlim = range(longitude), ylim = range(latitude)) points(longitude, latitude, col = &quot;red&quot;, pch = 16) }) 17.1.2 leaflet and mapview: interactive maps One of the very good features of R is that it can connect to many other computing systems, making these environments available without leaving R. One example is the leaflet Javascript library which provides many useful tools for the creation of interactive maps. Here is a simple example. When the code is executed in R it creates a map which can be scrolled and zoomed. The addTiles function uses Open Street Map by default. Note the use of the ~ symbol in front of variable names. The popup argument allows us to library(leaflet) leaflet(SO2_1990) %&gt;% addTiles() %&gt;% addCircles(~longitude, ~latitude, col = &#39;red&#39;, popup = ~site) The mapview package offers another means of creating interactive maps and it can use leaflet as its ‘engine’. 17.1.3 ggplot and ggmap The ggplot2 visualisation system, part of the tidyverse can handle ‘sf’ information through a geom_sf geometry. An example of this is given in the section on areal data below. 17.1.4 tmap This is a popular package which fits well with the tidyverse format of coding, but it will not be covered here. 17.1.5 spatstat for spatial point patterns This package has a variety of useful tools for plotting spatial point patterns. 17.1.6 Areal data There is a often a wish to investigate regional differences in a map, using well defined boundaries to define the regions of interest. This is a good way to visualise the Scottish Referendum data. A shape file, which provides the co-ordinates of the boundary of each region, has been downloaded from the ONS Open Geography Portal. This has been read into R as an object, using the ‘sf’ package, and then saved to the file UKLA.Rda. loading this file placess the object councils into the workspace. The councils object contains details of all the local authorities in the UK so the filter function from the dplyr package is used to select out those identifiers which begin with ‘S’ for Scotland. library(tidyverse) library(sf) path &lt;- rp.datalink(&quot;scottish_referendum&quot;) ref &lt;- read.table(path, header = TRUE) # path &lt;- &quot;~/iCloud/teaching/book/data_more/UKLA.Rda&quot; # load(path) path &lt;- &quot;~/iCloud/teaching/book/data_more/Local_Authority_Districts_December_2023_Boundaries_UK_BUC_-1729940581044841319/LAD_DEC_2023_UK_BUC.shp&quot; # dest &lt;- tempfile() # download.file(path, dest) # dest1 &lt;- tempfile() # unzip(dest, exdir = dest1) # councils &lt;- read_sf(dest1) councils &lt;- read_sf(path) councils &lt;- councils[substr(councils$LAD23CD, 1, 1) == &quot;S&quot;, ] We then need to merge these two dataframes, councils and ref. The Council name is the obvious way to link the two but we need to check that the same names have been used. The code below checks whether each name in ref can be found in councils. This throws up some exceptions, so themutatefunction fromdplyris used to recode these names incouncilsto match. Theleft_joinfunction fromdplyrcan then be used to merge therefdataframe intocouncils`. ind &lt;- ref$Scottish.Council %in% councils$LAD23NM ref$Scottish.Council[!ind] ## [1] &quot;Aberdeen&quot; &quot;Dundee&quot; &quot;Edinburgh&quot; &quot;Eilean Siar&quot; &quot;Glasgow&quot; councils &lt;- mutate(councils, Scottish.Council = recode(LAD23NM, `Na h-Eileanan Siar` = &quot;Eilean Siar&quot;, `Aberdeen City` = &quot;Aberdeen&quot;, `Glasgow City` = &quot;Glasgow&quot;, `Dundee City` = &quot;Dundee&quot;, `City of Edinburgh` = &quot;Edinburgh&quot;)) councils &lt;- left_join(councils, ref, by = &quot;Scottish.Council&quot;) Now we can plot the council regions as a map, using the geom_sf function in ggplot2 to handle all the drawing for us, and using a variable of interest to define the fill colour. Here we plot the proportion in each region voting ‘no’, plus a rescaling to show which regions have a majority for ‘yes’ or no’, and the level of turnout. ggplot(councils, aes(fill = Voted.no)) + geom_sf() 17.1.7 Image data Not all datasets can be structured easily into simple row and column format. Even for those which can, the dataset may be so large that more efficient and compact forms of storage are required. For example, the netCDF (network Common Data Form) is commonly used to store data which has an array structure, often corresponding to dense spatial and temporal measurements, but involving multiple variables. These datasets may be large in size. Satellite data is one example. The netCDF standard is widely used to achieve efficient storage. The ncdf4 package in R provides tools for reading, handling and plotting this form of data. An article on a heatwave in the sea around the UK in 2023 used satellite data from the National Oceanic and Atmospheric Administration (NOAA) in the USA. Sea surface temperature is one of the key climate change indicators. The data for June 18, 2023 can be read from the NOAA repository using the file download mechanism discussed above. The ncdf4 package can then be used to extract the data we need. Use of the functions within the package require careful study of the help files and other documentation. The code below illustrates what can be done. library(rpanel) library(ncdf4) path &lt;- rp.datalink(&quot;sea_surface_temperature&quot;) nc_data &lt;- nc_open(path) # print(nc_data) lon &lt;- ncvar_get(nc_data, &quot;lon&quot;) lat &lt;- ncvar_get(nc_data, &quot;lat&quot;, verbose = FALSE) anom &lt;- ncvar_get(nc_data, &quot;anom&quot;) fillvalue &lt;- ncatt_get(nc_data, &quot;anom&quot;, &quot;_FillValue&quot;) anom[anom == fillvalue$value] &lt;- NA nc_close(nc_data) This opens the datafile and extracts the latitude and longitude information as vectors which index the rows and columns of the matrix of anomaly (deviations from a long term average) measurements. The print(nc_data) instruction displays a lot of information on the content and source of the dataset but has been commented out above to save space. Missing data are also given the usual R representation. To plot the data efficiently, the raster package is first used to put the data in raster format. library(raster) r &lt;- raster(t(anom), xmn = min(lon), xmx = max(lon), ymn = min(lat), ymx = max(lat), crs = CRS(&quot;+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0&quot;)) r &lt;- flip(r, direction = &#39;y&#39;) plot(r) To explore the high temperatures near the UK in more detail, indicators are used to create a smaller array which focusses on the appropriate latitude and longitudes. ind.lon &lt;- which(lon &gt; 345 &amp; lon &lt; 360) ind.lat &lt;- which(lat &gt; 48 &amp; lat &lt; 60) anom_UK &lt;- anom[ind.lon, ind.lat] r &lt;- raster(t(anom_UK), xmn=min(lon[ind.lon]), xmx=max(lon[ind.lon]), ymn=min(lat[ind.lat]), ymx=max(lat[ind.lat]), crs=CRS(&quot;+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0&quot;)) r &lt;- flip(r, direction=&#39;y&#39;) plot(r) 17.1.8 Spatiotemporal animation The rpanel package includes an rp.plot4d function which uses animation to provide an extra dimension to displays. The rp.spacetime version of this function is intended particularly for data over space and time. library(rpanel) Month &lt;- SO2$month + (SO2$year - 1990) * 12 Year &lt;- SO2$year + (SO2$month - 0.5) / 12 location &lt;- cbind(SO2$longitude, SO2$latitude) mapxy &lt;- maps::map(&#39;world&#39;, plot = FALSE, xlim = range(SO2$longitude), ylim = range(SO2$latitude)) back &lt;- function() maps::map(mapxy, add = TRUE) rp.spacetime(location, Year, SO2$logSO2, col.palette = rev(heat.colors(12)), background.plot = back) "],["models-for-point-pattern-data.html", "17.2 Models for point pattern data", " 17.2 Models for point pattern data The lcancer dataframe in the sm package records the spatial positions of cases of laryngeal cancer in the North-West of England between 1974 and 1983, together with the positions of a number of lung cancer patients who were used as controls. The data have been adjusted to preserve anonymity. data(lcancer, package = &#39;sm&#39;) ggplot(lcancer, aes(Easting, Northing)) + geom_point() + annotate(&#39;point&#39;, x = 354500, y = 413000, col = &#39;red&#39;, size = 3) + facet_wrap(~ Cancer) When the information in spatial data is solely the location at which they sit, density estimation can be a useful tool. ggplot(lcancer, aes(Easting, Northing)) + geom_density2d_filled() + annotate(&#39;point&#39;, x = 354500, y = 413000, col = &#39;red&#39;, size = 3) + facet_wrap(~ Cancer) "],["geostatistical-models.html", "17.3 Geostatistical models", " 17.3 Geostatistical models 17.3.1 Gaussian random fields The concept of a Gaussian random field is an important one in spatial modelling. To gain some intuition on this, an interactive exploration of Gaussian fields can be launched by calling the function rp.geosim in the rpanel package. The smgrid argument controls the resolution of the images. If your computer does not have enough computational power for smgrid to be set to 100, try reducing this or accepting the default value of 40. rp.geosim(smgrid = 100) The plots below illustrate three simulations of a Gaussian random field, using the same parameters settings. There are common features features such as the smoothness of the surface and the size of the variations but the detailed patterns change randomly. At any location in the field, the value observed as a normal random variable with mean 0 and variance controlled by the partial sill parameter (pSill). The correlation between the values at difference locations diminishes with their distance apart. Values at locations which are close look similar while values which are far apart tend to look different. This is controlled by the Range parameter. Values of the field distance at pairs of locations which are this distance apart are virtually independent of one another. If a field is isotropic then it has the same characteristics in all directions. The correlation between values of the field is controlled by their distance apart, not their direction. There are sliders to control the degree of anisotropy and its direction. You may like to experiment with this too. 17.3.2 Geostatistical models library(tidyverse) library(dsm) data(mackerel, package = &#39;sm&#39;) km &lt;- latlong2km(mackerel$mack.long, mackerel$mack.lat) mackerel$mack.long &lt;- km$km.e mackerel$mack.lat &lt;- km$km.n mackerel &lt;- mackerel %&gt;% mutate(long = -mack.long, log.depth = log(mack.depth)) %&gt;% rename(lat = mack.lat) library(spaMM) model.spamm &lt;- fitme(log(Density) ~ poly(log.depth, 2) + Temperature + Matern(1 | long + lat), data = mackerel) summary(model.spamm) ## formula: log(Density) ~ poly(log.depth, 2) + Temperature + Matern(1 | ## long + lat) ## ML: Estimation of corrPars, lambda and phi by ML. ## Estimation of fixed effects by ML. ## Estimation of lambda and phi by &#39;outer&#39; ML, maximizing logL. ## family: gaussian( link = identity ) ## ------------ Fixed effects (beta) ------------ ## Estimate Cond. SE t-value ## (Intercept) 2.59146 0.234630 11.045 ## poly(log.depth, 2)1 11.89888 1.852051 6.425 ## poly(log.depth, 2)2 -9.91492 1.422122 -6.972 ## Temperature 0.02202 0.008943 2.462 ## --------------- Random effects --------------- ## Family: gaussian( link = identity ) ## --- Correlation parameters: ## 1.nu 1.rho ## 1.10634721 0.01976482 ## --- Variance parameters (&#39;lambda&#39;): ## lambda = var(u) for u ~ Gaussian; ## long + lat : 0.7714 ## # of obs: 279; # of groups: long + lat, 275 ## -------------- Residual variance ------------ ## phi estimate was 0.561597 ## ------------- Likelihood values ------------- ## logLik ## logL (p_v(h)): -383.7337 plot_effects(model.spamm, &quot;log.depth&quot;) plot_effects(model.spamm, &quot;Temperature&quot;) Set up a spatial area over which to predict. newdat.space &lt;- expand.grid(log.depth = mean(mackerel$log.depth), Temperature = mean(mackerel$Temperature), long = seq(min(mackerel$long), max(mackerel$long), length = 50), lat = seq(min(mackerel$lat), max(mackerel$lat), length = 50)) library(spatstat) dst &lt;- crossdist(newdat.space$long, newdat.space$lat, mackerel$long, mackerel$lat) dst &lt;- apply(dst, 1, min) newdat.space &lt;- newdat.space[dst &lt; 50, ] The fitted spatial effect. pred.spamm &lt;- c(predict(model.spamm, newdat.space)) ggplot() + geom_raster(aes(long, lat, fill = pred.spamm), data = newdat.space) + scale_fill_viridis_c() + geom_point(aes(long, lat), data = mackerel) A generalised additive model is an alternative approach. This kind of model is discussed in an earlier chapter. library(mgcv) model.gam &lt;- gam(log(Density) ~ s(log.depth) + s(Temperature) + s(long, lat), data = mackerel) summary(model.gam) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## log(Density) ~ s(log.depth) + s(Temperature) + s(long, lat) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.22409 0.05364 60.11 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(log.depth) 2.597 3.298 12.101 2.98e-07 *** ## s(Temperature) 2.922 3.663 4.720 0.00172 ** ## s(long,lat) 25.057 27.996 6.043 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.698 Deviance explained = 73.1% ## GCV = 0.90515 Scale est. = 0.8027 n = 279 plot(model.gam) The estimated effects are very similar, despite the different perspectives of the modelling approaches. The spatial effects in particular are compared below. pred.gam &lt;- c(predict(model.gam, newdat.space)) ggplot() + geom_raster(aes(long, lat, fill = pred.gam), data = newdat.space) + scale_fill_viridis_c() # geom_point(aes(long, lat), data = mackerel) ggplot() + geom_raster(aes(long, lat, fill = pred.spamm), data = newdat.space) + scale_fill_viridis_c() # geom_point(aes(long, lat), data = mackerel) There are an increasing number of options for the modelling of spatial data, including: geoR when the focus is primarily on the spatial effects, without additional covariates; inla for a Bayesian modelling approach; mgcv for an additive modelling approach. prevmap glmmTMB See spatial example at https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html and https://rpubs.com/Rebekko/922320 nlme is also an option: https://stats.oarc.ucla.edu/r/faq/how-do-i-model-a-spatially-autocorrelated-outcome/ Gómez-Rubio (2020), with on-line version at https://becarioprecario.bitbucket.io/inla-gitbook/ McElreath (2018) Krainski et al. (2018), with on-line version at https://becarioprecario.bitbucket.io/spde-gitbook/index.html 17.3.3 SO2: a model based on smoothing An interactive display of this spatiotemporal dataset can be launched by executing the code below. SO2 &lt;- mutate(SO2, year = year + (month - 0.5) / 12) mapxy &lt;- maps::map(&#39;world&#39;, plot = FALSE, xlim = range(SO2$longitude), ylim = range(SO2$latitude)) back &lt;- function() maps::map(mapxy, add = TRUE) with(SO2, rp.plot4d(cbind(longitude, latitude), year, logSO2, col.palette = rev(heat.colors(12)), background.plot = back)) A gam model for these data was considered earlier. This can be built and viewed by executing the code below. with(SO2, { location &lt;- cbind(longitude, latitude) model &lt;- mgcv::gam(logSO2 ~ s(longitude, latitude, year)) loc1 &lt;- seq(min(longitude), max(longitude), length = 30) loc2 &lt;- seq(min(latitude), max(latitude), length = 30) yr &lt;- seq(min(year), max(year), length = 30) newdata &lt;- expand.grid(loc1, loc2, yr) names(newdata) &lt;- c(&quot;longitude&quot;, &quot;latitude&quot;, &quot;year&quot;) model &lt;- predict(model, newdata) model &lt;- list(x = cbind(loc1, loc2), z = yr, y = array(model, dim = rep(30, 3))) rp.plot4d(location, year, logSO2, model, col.palette = rev(heat.colors(20)), foreground.plot = back) }) References Gómez-Rubio, Virgilio. 2020. Bayesian Inference with INLA. Chapman; Hall/CRC. Krainski, Elias, Virgilio Gómez-Rubio, Haakon Bakka, Amanda Lenzi, Daniela Castro-Camilo, Daniel Simpson, Finn Lindgren, and Håvard Rue. 2018. Advanced Spatial Modeling with Stochastic Partial Differential Equations Using r and INLA. Chapman; Hall/CRC. McElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC. "],["models-for-areal-data.html", "17.4 Models for areal data", " 17.4 Models for areal data An example of areal data was given in the visualisation section above. This referred to the voting patterns in the 2014 Scottish Referendum. Visual displays are very informative but when the question of interest concerns the influence of potential covariates a model is then needed to give insight into what is happening. The CARBayes package (Lee, 2013) provides numerous powerful tools to help. The example in this section constructs a model of house prices in Glasgow. The code used follows closely the example provided in the CARBayes package vignette. The text in this section gives only a sketch of the analysis. The tools required to organise, plot and model the data are spread across several packages. R is a very collaborative environment! library(CARBayes) library(CARBayesdata) library(sf) library(tidyverse) library(GGally) library(mapview) library(RColorBrewer) library(spdep) library(coda) The variables available for the model are shown below. data(pricedata) pricedata &lt;- pricedata %&gt;% mutate(logprice = log(price)) head(pricedata) ## IZ price crime rooms sales driveshop type logprice ## 1 S02000260 112.250 390 3 68 1.2 flat 4.720729 ## 2 S02000261 156.875 116 5 26 2.0 semi 5.055449 ## 3 S02000262 178.111 196 5 34 1.7 semi 5.182407 ## 4 S02000263 249.725 146 5 80 1.5 detached 5.520360 ## 5 S02000264 174.500 288 4 60 0.8 semi 5.161925 ## 6 S02000265 163.521 342 4 24 2.5 semi 5.096941 ggpairs(data = pricedata, columns = c(8, 3:7)) We need to collate this with the spatial information for the region, ensuring that the co-ordinate system is moved to longitude and latitude. data(GGHB.IZ) pricedata.sf &lt;- merge(GGHB.IZ, pricedata, by = &quot;IZ&quot;, all.x = FALSE) pricedata.sf &lt;- st_transform(x=pricedata.sf, crs=&#39;+proj=longlat +datum=WGS84 +no_defs&#39;) We can now plot price information on a map using mapview. map1 &lt;- mapview(pricedata.sf, zcol = &quot;price&quot;, col.regions = brewer.pal(9, &quot;YlOrRd&quot;), alpha.regions = 0.6, layer.name = &quot;Price&quot;, lwd = 0.5, col = &quot;grey90&quot;, homebutton = FALSE) removeMapJunk(map1, junk = c(&quot;zoomControl&quot;, &quot;layersControl&quot;)) When the residuals from a standard (non-spatial) linear model are examined, there is clear evidence of spatial correlation. In preparation for a spatial model, the neighbourhood structure of the different areas is set up. W.nb &lt;- poly2nb(pricedata.sf, row.names = pricedata.sf$IZ) ## Warning in poly2nb(pricedata.sf, row.names = pricedata.sf$IZ): neighbour object has 2 sub-graphs; ## if this sub-graph count seems unexpected, try increasing the snap argument. W &lt;- nb2mat(W.nb, style=&quot;B&quot;) This allows us to fit a model which includes all the covariates of interest but also incorporates spatial correlation. We can check that the MCMC has stabilised. ## form &lt;- logprice~crime+rooms+sales+factor(type) + driveshop ## chain &lt;- S.CARleroux(form, data = pricedata.sf, family = &quot;gaussian&quot;, ## W = W, burnin = 100000, n.sample = 300000, thin = 100, ## n.chains = 3, n.cores = 3) plot(chain$sample$beta[ ,2:4]) Now we are in a position to interpret the influence of the covariates. print(chain) ## ## ################# ## #### Model fitted ## ################# ## Likelihood model - Gaussian (identity link function) ## Random effects model - Leroux CAR ## Regression equation - logprice ~ crime + rooms + sales + factor(type) + driveshop ## ## ## ################# ## #### MCMC details ## ################# ## Total number of post burnin and thinned MCMC samples generated - 6000 ## Number of MCMC chains used - 3 ## Length of the burnin period used for each chain - 1e+05 ## Amount of thinning used - 100 ## ## ############ ## #### Results ## ############ ## Posterior quantities and DIC ## ## Mean 2.5% 97.5% n.effective PSRF (upper 95% CI) ## (Intercept) 4.1368 3.8750 4.3951 6215.8 1 ## crime -0.0001 -0.0002 -0.0001 6000.0 1 ## rooms 0.2334 0.1860 0.2814 6251.4 1 ## sales 0.0023 0.0017 0.0029 6000.0 1 ## factor(type)flat -0.2947 -0.4034 -0.1882 6000.0 1 ## factor(type)semi -0.1728 -0.2709 -0.0767 6000.0 1 ## factor(type)terrace -0.3247 -0.4491 -0.2031 6426.3 1 ## driveshop 0.0036 -0.0302 0.0380 6000.0 1 ## nu2 0.0224 0.0118 0.0328 5677.7 1 ## tau2 0.0537 0.0246 0.0962 5979.6 1 ## rho 0.9094 0.7212 0.9904 5771.8 1 ## ## DIC = -162.7665 p.d = 104.7579 LMPL = 60.36 "],["further-reading-7.html", "17.5 Further reading", " 17.5 Further reading van Lieshout book for theory. “Analysing US Census Data” book. “Spatial Data Science” book. “Geocomputation with R” book. Baddeley, A., Rubak, E. and Turner, R. (2015) Spatial Point Patterns: Methodology and Applications with R. Chapman and Hall/CRC Press. "],["case-studies-of-statistical-modelling.html", "18 Case studies of statistical modelling ", " 18 Case studies of statistical modelling "],["covid-19-initial-surveillance.html", "18.1 Covid-19 initial surveillance", " 18.1 Covid-19 initial surveillance In January 2020, as concern about a new virus appearing in China was growing, the COVID-19 Response Team set up at Imperial College London published a report whose opening paragraphs are given below. On the 31st December 2019, the World Health Organization (WHO) China Country Office was informed of cases of pneumonia of unknown aetiology in Wuhan City, Hubei Province, China. A novel Coronavirus (2019-nCoV) related to the Middle Eastern Respiratory Syndrome virus and the Severe Acute Respiratory Syndrome virus has since been implicated. As of 16th January 2020, 41 cases (including two deaths) have been confirmed in Wuhan City with three confirmed cases in travellers detected in Thailand (2 cases) and Japan (1 case). Is it possible to use the number of cases detected outside China at this stage, to estimate the number of clinically comparable cases within Wuhan City that may have occurred thus far? "],["childrens-services-reform-in-scotland.html", "18.2 Children’s services reform in Scotland", " 18.2 Children’s services reform in Scotland In 2021, the Scottish Government committed to building a National Care Service to be responsible for adults’ social work and social care support, as well as taking responsibility for the planning and commissioning of community health services. This led to the question of whether children’s services should also be included. In 2022, CELCIS, a centre for excellence for children’s care and protection in Scotland, was asked by the Scottish Government to carry out a research study with the aim of addressing the question: ‘What is needed to ensure that children, young people and families get the help they need, when they need it?’. Detailed information on the background to the project is available. Strand 3 of the research was entitled Mapping Integration and Outcomes Across Scotland: A Statistical Analysis and it aim to address the specific question: ‘Is the level of structural integration of children’s health and social care services associated with changes in outcomes for children, young people, families, and the workforce?’ To help in answering this question, the researchers looked at the approaches taken towards structural integration following the development of Health and Social Care Partnerships over the last decade in Scotland and 25 indicators associated with the welfare and wellbeing of Scotland’s children, support for families, and the workforce who support them. A zip file containing the data is available through rp.datalink('children_services'). This consists of a folder Data and Code CSRR Strand 3 - August 2023 Update cotaining individual files for one of the indicators (Example_Indicator_CSR.csv), mid-year population estimates (Mid-year-pops-0-17.csv) and contextutal factors (contextual_factors.csv). How should the data be analysed to address this question? Analysis The results were published as a report on statistical analysis and the data and code are also available. This section discusses the analysis which was undertaken. The code used here reformulates some of the code provided with the report above to use the tidyverse form, in order to be consistent with earlier material. As ever, the first step is to read and organise the data. This has been retrieved from different sources and made available as three separate files, in a folder which has been compressed into a ‘zip’ file. The unz function enables us to read the individual files. library(rpanel) library(tidyverse) library(lme4) zfile &lt;- rp.datalink(&quot;children_services&quot;) dirname &lt;- &quot;Data and Code CSRR Strand 3 - August 2023 Update&quot; # Read indicator data path &lt;- unz(zfile, paste(dirname, &quot;Example_Indicator_CSR.csv&quot;, sep = &quot;/&quot;)) indicator &lt;- read_csv(path, skip = 6, col_names = c(&quot;LA&quot;, 2011:2021)) %&gt;% pivot_longer(cols = !LA, names_to = &quot;Year&quot;, values_to = &quot;Offenders&quot;) # Read mid-year population estimates to convert indicator to rates per 10,000 - using 0-17 population. path &lt;- unz(zfile, paste(dirname, &quot;Mid-year-pops-0-17.csv&quot;, sep = &quot;/&quot;)) mid_year_pops &lt;- read_csv(path, skip = 3) %&gt;% rename(&#39;LA&#39; = 1) %&gt;% filter(LA != &#39;Scotland&#39;) %&gt;% dplyr::select(LA, `2011`:`2021`) %&gt;% pivot_longer(cols = !LA, names_to = &#39;Year&#39;, values_to = &#39;young_pop&#39;) # Read data on contextual factors # The pivot_longer instruction took a very long time to puzzle out! path &lt;- unz(zfile, paste(dirname, &quot;contextual_factors.csv&quot;, sep = &quot;/&quot;)) context &lt;- read_csv(path, skip = 3, n_max = 32) %&gt;% mutate(HSCP_start = as.numeric(format(as.Date(HSCP_start, format = &quot;%d-%b-%y&quot;), &quot;%Y&quot;))) %&gt;% pivot_longer(cols = SIMD_2011:PopDens_2021, names_to = c(&quot;.value&quot;, &quot;Year&quot;), names_sep = &quot;_&quot;) %&gt;% mutate(covid = as.numeric(Year == 2021), Int_level = recode(Int_level, &quot;No structural integration&quot; = &quot;none&quot;, &quot;Partial structural integration&quot; = &quot;partial&quot;, &quot;Full structural integration&quot; = &quot;full&quot;)) Create one dataset containing all data. anti_join(indicator, context) ## # A tibble: 0 × 3 ## # ℹ 3 variables: LA &lt;chr&gt;, Year &lt;chr&gt;, Offenders &lt;dbl&gt; anti_join(indicator, mid_year_pops) ## # A tibble: 0 × 3 ## # ℹ 3 variables: LA &lt;chr&gt;, Year &lt;chr&gt;, Offenders &lt;dbl&gt; d &lt;- full_join(indicator, context) %&gt;% full_join(mid_year_pops) %&gt;% mutate(Int_level = fct(Int_level, levels=c(&quot;none&quot;, &quot;partial&quot;, &quot;full&quot;)), Coterminous = fct(Coterminous), LA = fct(LA), Year = as.numeric(Year), Year2 = c(scale(Year)), pre_or_post = fct_recode(fct(as.character(Year - HSCP_start &gt; 0)), &quot;Pre_Int&quot; = &quot;FALSE&quot;, &quot;Post_Int&quot; = &quot;TRUE&quot;), PopDens = c(scale(PopDens)), SIMD = c(scale(SIMD)), rate = Offenders * 10000 / young_pop, ln_pops = log(young_pop / 10000)) str(d) ## tibble [352 × 14] (S3: tbl_df/tbl/data.frame) ## $ LA : Factor w/ 32 levels &quot;Aberdeen City&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Year : num [1:352] 2011 2012 2013 2014 2015 ... ## $ Offenders : num [1:352] 243 138 111 88 73 73 90 81 96 114 ... ## $ Int_level : Factor w/ 3 levels &quot;none&quot;,&quot;partial&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ HSCP_start : num [1:352] 2016 2016 2016 2016 2016 ... ## $ Coterminous: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ SIMD : num [1:352] -0.229 -0.229 -0.229 -0.229 -0.229 ... ## $ PopDens : num [1:352] 0.918 0.935 0.95 0.963 0.973 ... ## $ covid : num [1:352] 0 0 0 0 0 0 0 0 0 0 ... ## $ young_pop : num [1:352] 36561 37060 37182 37519 37775 ... ## $ Year2 : num [1:352] -1.579 -1.263 -0.947 -0.632 -0.316 ... ## $ pre_or_post: Factor w/ 2 levels &quot;Pre_Int&quot;,&quot;Post_Int&quot;: 1 1 1 1 1 1 2 2 2 2 ... ## $ rate : num [1:352] 66.5 37.2 29.9 23.5 19.3 ... ## $ ln_pops : num [1:352] 1.3 1.31 1.31 1.32 1.33 ... Plot data by integration level (ignores correlated data structure so not definitive, but provides an idea of trend.) ggplot(d, aes(Year, rate, colour = LA)) + geom_line() + facet_wrap(~ Int_level) + labs(y = &quot;Offender rate per 10,000&quot;) + theme(legend.position = &quot;none&quot;) ggplot(d, aes(Year, rate)) + geom_smooth(aes()) + facet_wrap(~ Int_level) + labs(y = &quot;Offender rate per 10,000&quot;) + geom_vline(xintercept = 2016, colour = I(&quot;red&quot;), linetype = 2) Negative binomial models. Note - Fit is almost identical with nAGQ=0 but much faster, used to decrease computing time for PBmodcomp. MODEL A0 - Two slopes model with covariates added and NO integration MODEL A1 - Integration as additive effect (same pre- vs post-) MODEL A2 - Integration as interaction effect (different pre- vs post-) modelA0 &lt;- glmer.nb(Offenders ~ offset(ln_pops) + Year2 * pre_or_post + covid + Coterminous + SIMD + PopDens + (1|LA), data = d, na.action=na.exclude, nAGQ=0) # summary(modelA0) modelA1 &lt;- glmer.nb(Offenders ~ offset(ln_pops) + Int_level + Year2 * pre_or_post + covid + Coterminous + SIMD + PopDens + (1|LA), data = d, na.action = na.exclude, nAGQ = 0) # summary(modelA1) modelA2 &lt;- glmer.nb(Offenders ~ offset(ln_pops) + Int_level * pre_or_post + Year2 * pre_or_post + covid + Coterminous + SIMD + PopDens + (1|LA), data = d, na.action = na.exclude, nAGQ = 0) # summary(modelA2) Set up matrix to plot model estimates for the pre- and post-integration trends, using 2016 as the cutoff. predict_pre &lt;- expand.grid(Int_level = unique(d$Int_level), pre_or_post = &quot;Pre_Int&quot;, Year2 = unique(d$Year2[d$Year &lt;= 2016]), SIMD = 0, covid = 0, PopDens = 0, Coterminous = &quot;No&quot;, ln_pops = mean(d$ln_pops)) %&gt;% mutate(Year = rep(2011:2016, each = 3)) predict_post &lt;- expand.grid(Int_level = unique(d$Int_level), pre_or_post = &quot;Post_Int&quot;, Year2 = unique(d$Year2[d$Year &gt;= 2016]), SIMD = 0, covid = 0, PopDens = 0, Coterminous = &quot;No&quot;, ln_pops = mean(d$ln_pops)) %&gt;% mutate(Year = rep(2016:2021, each = 3), covid = case_when(Year == 2021 ~ 1, Year != 2021 ~ 0)) prefits_A0 &lt;- predict(modelA0, newdata = predict_pre, re.form= ~0, type = &quot;response&quot;) / exp(mean(d$ln_pops)) postfits_A0 &lt;- predict(modelA0, newdata = predict_post, re.form= ~0, type = &quot;response&quot;) / exp(mean(d$ln_pops)) prefits_A1 &lt;- predict(modelA1, newdata=predict_pre, re.form = ~0, type = &quot;response&quot;) / exp(mean(d$ln_pops)) postfits_A1 &lt;- predict(modelA1, newdata=predict_post, re.form = ~0, type = &quot;response&quot;) / exp(mean(d$ln_pops)) prefits_A2 &lt;- predict(modelA2, newdata = predict_pre, re.form = ~0, type = &quot;response&quot;) / exp(mean(d$ln_pops)) postfits_A2 &lt;- predict(modelA2, newdata = predict_post, re.form = ~0, type = &quot;response&quot;) / exp(mean(d$ln_pops)) Create the basic plot then add the specifics for each model This is much neater than repeating code p &lt;- ggplot(d, aes(Year, rate, colour=Int_level)) + geom_line(aes(group = LA), alpha = 0.5, linetype = 2) + geom_vline(xintercept = 2016, linetype = 2) + labs(y = &quot;Rate per 10,000 children&quot;, colour = &quot;Integration Level&quot;) + # scale_colour_manual(values=colours_vec[c(1,3,4)]) + scale_x_continuous(breaks = 2011:2021) p + geom_line(data = predict_post, aes(Year, postfits_A0), linewidth = 1, colour = &quot;black&quot;) + geom_line(data = predict_pre, aes(Year, prefits_A0), linewidth = 1, colour = &quot;black&quot;) + labs(title = &quot;Model A0: No group effect&quot;) p + geom_line(data = predict_post, aes(Year, postfits_A1), linewidth = 1, linetype=rep(c(1, 1, 2), each = 6)) + geom_line(data = predict_pre, aes(Year, prefits_A1), linewidth = 1, linetype=rep(c(1, 1, 2), each = 6)) + labs(title = &quot;Model A1: Group effect constant over time&quot;) p + geom_line(data = predict_post, aes(Year, postfits_A2), linewidth = 1, linetype=rep(c(1, 1, 1), each = 6)) + geom_line(data = predict_pre, aes(Year, prefits_A2), linewidth = 1, linetype=rep(c(1, 2, 1), each = 6)) + labs(title = &quot;Model A2: Group effect can differ pre- vs post-integration&quot;) Model comparison: set seed so simulations are reproducible. Note the use of the :: syntax to avoid loading the full package. Results: stat df p.value LRT 0.06218459 2 0.9693861 PBtest 0.06218459 NA 0.9750250 stat df p.value LRT 4.231913 2 0.1205180 PBtest 4.231913 NA 0.1474704 Plot of predicted values vs observed values - Good. plot(d$Offenders, predict(modelA2, type=&quot;response&quot;)) abline(0,1) ## Check dispersion for best model (or full model) - only applicable to poisson/neg binom # overdisp_fun(modelA2) # ratio=0.9, so okay ### Check for multicollinearity in the explanatory variables ##Check for correlation of SIMD and POP dens plot(context$PopDens, context$SIMD) cor(context$PopDens, context$SIMD) ##r=.547 ## [1] 0.5472664 ## Confirm levels of correlation are not problematic through inspection of VIF (variance inflation factors) car::vif(modelA2) ## All non-interaction terms under 3, indicating levels of correlation are not problematic. ## GVIF Df GVIF^(1/(2*Df)) ## Int_level 1.401894 2 1.088125 ## pre_or_post 5.823416 1 2.413176 ## Year2 7.261942 1 2.694799 ## covid 1.910853 1 1.382336 ## Coterminous 1.209348 1 1.099704 ## SIMD 1.739639 1 1.318954 ## PopDens 1.450657 1 1.204432 ## Int_level:pre_or_post 2.688646 2 1.280511 ## pre_or_post:Year2 6.447831 1 2.539258 ## Check for normality of random effects r_int&lt;- ranef(modelA2)$LA qqnorm(unlist(r_int)) qqline(unlist(r_int)) hist(unlist(r_int)) shapiro.test(unlist(r_int)) ## ## Shapiro-Wilk normality test ## ## data: unlist(r_int) ## W = 0.95697, p-value = 0.2265 ## Assumption okay. Shapiro test p-value &gt; 0.05 (p=.21), implying that the distribution of the random effects are ## not significantly different from normal distribution. ## Check for any residual autocorrelation acf(residuals(modelA2, retype=&quot;normalized&quot;)) #Fine pacf(residuals(modelA2, retype=&quot;normalized&quot;)) #Fine See Benjamini (1995) paper for FDR. "],["more-on-r.html", "19 More on R ", " 19 More on R "],["repetition.html", "19.1 Repetition", " 19.1 Repetition Although R has many functions which can perform high-level operations from a single call, it is also a full-blooded programming environment. All the standard programming concepts and control mechanisms are available and these are introduced in this chapter. For those who may be interested, R also has the tools required for functional and object-oriented programming, although these are not covered here. Those who are experienced programmers in other languages may find that R is at the more permissive end of the spectrum, for example in the nature of the parameters which are passed to functions. This does give substantial flexibility but some may find that this runs counter to their training in more formal programming languages. 19.1.1 Standard loops As a motivating example, suppose we are interested in extremes such as the highest sea level or wind speed in a month or a year. Some progress can be made on this by studying the statistical theory. However, we can also use simulation to help us understand what is going on. Suppose we are observing a phenomenon where individual daily measurements might reasonably be expected to follow a normal distribution with mean 10 and standard deviation 2. What will the maximum measurement over week, month or year look like? Let’s start with a week. The rnorm function generates random values from a normal distribution. y &lt;- rnorm(7, mean = 10, sd = 2) hist(y) With only 7 observations the histogram is highly variable. Let’s move to a month. y &lt;- rnorm(30, mean = 10, sd = 2) hist(y) The max function makes the largest value easy to find. y &lt;- rnorm(30, mean = 10, sd = 2) max(y) ## [1] 17.0445 The concept of a for loop makes releated simulations easy to produce. Here the dummy variable i runs through the range from 1 to nsim and for each value of i a set of data is simulated and the maximum found. nsim &lt;- 100 mx &lt;- rep(0, nsim) for (i in 1:nsim) mx[i] &lt;- max(rnorm(30, mean = 10, sd = 2)) hist(mx) We can obtain a clearer picture of the distribution by increasing the number of simulations. nsim &lt;- 1000 mx &lt;- rep(0, nsim) for (i in 1:nsim) mx[i] &lt;- max(rnorm(30, mean = 10, sd = 2)) hist(mx) The for construction provides a clear and simple method of looping but it can be very inefficient. Much more efficient methds are available in the apply family of functions. 19.1.2 The ‘apply’ family A for loop is a standard means of repeating an operation, but in R this can be a very inefficient way of proceeding. This is because R is an interpreted language and so there are overheads associated with repeated operations. A good solution is to make use of R functions which are constructed to deal with repeated operations more efficiently. The apply family of functions offers this. The example below shows how this works. We first generate the entire set of simulated data, covering all nsim sets of 30 observations. This is stored in a matrix with 30 columns. When the apply function is called, it executes the nominated function (max) on each row (specified as dimension 1) of the matrix (x). This creates nsim maxima and these are stored in the object mx. The difference in execution time in this example is small but in more complex problems the time saved can be considerable. This sometimes has to be balanced against the storage required to set up the object to which apply will be applied. There are various other forms of apply which are designed to handle more complex objects such as lists or data indexed by a factor. See the help function on apply (and its friends) for more details. nsim &lt;- 100 x &lt;- rnorm(nsim * 30, mean = 10, sd = 2) mat &lt;- matrix(x, ncol = 30) mx &lt;- apply(mat, 1, max) hist(mx) "],["writing-your-own-functions.html", "19.2 Writing your own functions", " 19.2 Writing your own functions Once we have a useful block of code, which we may wish to use repeatedly and in different circumstances, it can be very useful to turn this into a new function. This is done simply by assigning a name to a function which consists of this block of code. The value of the last instruction in the function is returned as the value of the function. The invisible function is useful to avoid this being printed out. sim.max &lt;- function() { nsim &lt;- 100 x &lt;- rnorm(nsim * 30, mean = 10, sd = 2) mat &lt;- matrix(x, ncol = 30) mx &lt;- apply(mat, 1, max) hist(mx) invisible(mx) } The function can then be activated (or called) simply by typing its name in the usual way. sim.max() To provide detailed control of the function we can add arguments. These have stated default values but we can override these by specifying (or passing) different values. sim.max &lt;- function(n = 30, mn = 10, s = 2, nsim = 1000) { x &lt;- rnorm(nsim * n, mean = mn, sd = s) mat &lt;- matrix(x, ncol = n) mx &lt;- apply(mat, 1, max) hist(mx) invisible(mx) } sim.max(50, 20, 2) "],["conditional-statements-and-while-loops.html", "19.3 Conditional statements and while loops", " 19.3 Conditional statements and while loops There is often a need to control the execution of statements through logical expression. For example, suppose in the wind speed context we want to limit the upper wind speeds we consider because we are modelling the power output of wind turbines and at very high speeds these have to be switched off to prevent damage. Let’s suppose the threshold for this is \\(50\\) mph. Let’s also consider a lognormal model for wind speed, so that the log speed is normal with mean \\(2\\) and standard deviation \\(1\\). How should we amend our code to implement this? Here is a possibility. First we amend the arguments of the sim.max function to signal that the mean and standard deviation refer to the log scale. We also add the threshold argument as it is likely that we will want the option to change this. Now we have to change the way the data are simulated. Here we have done this one observation at a time, inspecting whether each one is below the threshold or not. Notice that samp is set up as an empty vector by numeric(0). The while construction means that we generate more data only is the samp vector does not yet have enough observations. If that’s the case then we simulate a value from a lognormal distribution (see the help file for rlnorm) and add it to samp using the c function if it lies below the threshold. (The if construction also has the possibility of a following else statement but that’s not needed here.) The rest of the function follows as before. There are usually many ways of coding any task. The example below is not the most elegant or efficient. It’s primary aim is to illustrate the while and if constructions. sim.max &lt;- function(n = 30, mn.log = 2, s.log = 1, threshold = 50, nsim = 1000) { mx &lt;- rep(0, nsim) for (i in 1:nsim) { samp &lt;- numeric(0) while (length(samp) &lt; n) { value &lt;- rlnorm(1, mn.log, s.log) if (value &lt; threshold) samp &lt;- c(samp, value) } mx[i] &lt;- max(samp) } hist(mx) invisible(mx) } sim.max() "],["further-reading-8.html", "19.4 Further reading", " 19.4 Further reading Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. R Markdown allows Markdown documents to be created with embedded R code which runs when the document is compiled. R Studio makes the creation of R Markdown documents very straightforward. This is a very powerful way of working. In particular it promotes reproducible research thrugh the creation of a single document which embodies and reports on a statistical analysis. For more details on using R Markdown see http://rmarkdown.rstudio.com. A simple way to begin experimenting with the system is to use the menu item File &gt; New File &gt; R Markdown to place a template R Markdown file in the editor window. This can be compiled by clicking on the Knit button. You might like to go back to the very first example of the session, to create a document which weaves a commentary around the R code, to create an integrated report. The Markdown Quick Reference material available from the Help menu in RStudio provides a very helpful summary of the key instructions. Packages provide a very efficient means of extending the facilities of R and making these extensions available to others. Writing a package can be a rewarding enterprise, although it does take some effort to do well. Wickham and Bryan (2023) provide an excellent guide on the details of doing this. References Wickham, Hadley, and Jennifer Bryan. 2023. R Packages. \" O’Reilly Media, Inc.\". "],["exercises-12.html", "19.5 Exercises", " 19.5 Exercises 19.5.1 A simple function With a sample of data on a continuous scale, it can be useful to plot a histogram and then superimpose a normal density function to assess the suitability of the normal model. For the purpose of the exercise, we can simply simulate some random data using the rnorm function. x &lt;- rnorm(50) In plotting the histogram, it will help to use the additional probability argument in the hist function so that the histogram is scaled to have area 1. To superimpose the normal density, we can define a grid of values along the horizontal axis and then compute the values of the normal density function there, using dnorm. The normal density curve can then be drawn simply by connecting these positions with lines, using the lines function. The code below does this. xgrid &lt;- seq(min(x), max(x), length = 100) ygrid &lt;- dnorm(xgrid) hist(x, probability = TRUE) lines(xgrid, ygrid, lwd = 2, col = &quot;blue&quot;) Turn the code into a function, with a name of your choosing, with a single argument x, the data to be plotted. As the code above assumes the mean and standard deviation of the normal density to be 0 and 1 respectively, amend the code to draw a density function whose mean and standard deviation match the sample mean and sample standard deviation of the data. [The mean and sd functions will be helpful here, as will the mean and sd arguments of the dnorm function.] Test the function out with data of your choice (or simulated data). Add arguments to the function to allow the user to control the colour and line width of the plotted normal density function. Amend the code of the histogram and normal density example above to ensure that (a) the density curve is never truncated because its peak is higher than the highest histogram bar and (b) that the normal density always stretches out to at least +/-3 sd’s. [*This task is a little trickier. Notice that if you set the argument plot = FALSE in an initial call to hist and store the result then you can get information on the height and spread of the bars, among other things.] See Benjamini (1995) paper for FDR. See Benjamini (1995) paper for FDR. 19.5.2 A simple function: thresholds Write a function which accepts a vector and a single numeric value as input and which then calculates how many elements of the vector exceed the threshold set by the given numerical value. Apply this function to demographic data across the world to calculate the number of countries whose population exceeds 50 million. Code to read and collate UN data is available at the end of Section 4.1. If you prefer, a simpler option is to use the gapminder data, available in the gapminder package. Plot this pattern over time. See Benjamini (1995) paper for FDR. 19.5.3 Improving the sim.max function Consider how you might improve the coding of the sim.max function. Here are two possibilities. Generate a full set n observations from the lognormal distribution. Now remove those observations which do not lie below the threshold. This can be used as a good starting point for the samp vector, rather than starting from nothing. Amend the code within the while loop to generate as many observations from the lognormal function which are needed, again removing those which do not lie above the threshold. This should greatly reduce the number of loops required. See Benjamini (1995) paper for FDR. "],["references.html", "References", " References Aitchison, John. 1982. “The Statistical Analysis of Compositional Data.” Journal of the Royal Statistical Society: Series B (Methodological) 44 (2): 139–60. Åstrand, Per-Olof. 1952. “Experimental Studies of Physical Working Capacity in Relation to Sex and Age.” PhD thesis, Munksgaard Forlag. Belenky, Gregory, Nancy J Wesensten, David R Thorne, Maria L Thomas, Helen C Sing, Daniel P Redmond, Michael B Russo, and Thomas J Balkin. 2003. “Patterns of Performance Degradation and Restoration During Sleep Restriction and Subsequent Recovery: A Sleep Dose-Response Study.” Journal of Sleep Research 12 (1): 1–12. Bell, Alexander Graham. 1918. The Duration of Life and Conditions Associated with Longevity. Genealogical record office. Bhatt, Arun. 2010. “Evolution of Clinical Research: A History Before and Beyond James Lind.” Perspectives in Clinical Research 1 (1): 6. Bowman, A. W. 1994. “Teaching by Design.” Teaching Statistics 16: 2–4. https://doi.org/10.1111/j.1467-9639.1994.tb00670.x. Breiman, Leo. 2001. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” Statistical Science 16 (3): 199–231. Chambers, John M, William S Cleveland, Beat Kleiner, and Paul A Tukey. 2018. Graphical Methods for Data Analysis. Chapman; Hall/CRC. Collaborators, MRC Crash Trial et al. 2008. “Predicting Outcome After Traumatic Brain Injury: Practical Prognostic Models Based on Large Cohort of International Patients.” British Medical Journal 336 (7641): 425–29. Diggle, Peter J, and Amanda Chetwynd. 2011. Statistics and Scientific Method: An Introduction for Students and Researchers. Oxford University Press. Finney, DJ. 1947. “The Estimation from Individual Records of the Relationship Between Dose and Quantal Response.” Biometrika 34 (3/4): 320–34. Fisher, Ronald Aylmer et al. 1925. “Applications of ‘Student’s’ Distribution.” Metron 5 (3): 90–104. Friendly, M. n.d. “Visualising Categorical Data; SAS Institute: Carry, NC, USA, 2000.” Google Scholar. Friendly, Michael, and Howard Wainer. 2021. A History of Data Visualization and Graphic Communication. Harvard University Press. Gelman, Andrew, Cristian Pasarica, and Rahul Dodhia. 2002. “Let’s Practice What We Preach: Turning Tables into Graphs.” The American Statistician 56 (2): 121–30. Gilliatt, Roger W. 1948. “Vaso-Constriction in the Finger After Deep Inspiration.” The Journal of Physiology 107 (1): 76–88. Gómez-Rubio, Virgilio. 2020. Bayesian Inference with INLA. Chapman; Hall/CRC. Gordon, Ian, and Sue Finch. 2015. “Statistician Heal Thyself: Have We Lost the Plot?” Journal of Computational and Graphical Statistics 24 (4): 1210–29. Grant, Robert. 2018. Data Visualization: Charts, Maps, and Interactive Graphics. Crc Press. Hand, David. 2019. “What Is the Purpose of Statistical Modelling.” Harvard Data Science Review 1 (1): 1–6. https://hdsr.mitpress.mit.edu/pub/9qsbf3hz/release/7. Hawkins, Ed, Stephen Burt, Philip Brohan, Michael Lockwood, Harriett Richardson, Marjory Roy, and Simon Thomas. 2019. “Hourly Weather Observations from the Scottish Highlands (1883–1904) Rescued by Volunteer Citizen Scientists.” Geoscience Data Journal 6 (2): 160–73. https://doi.org/10.1002/gdj3.79. Hintze, Jerry L, and Ray D Nelson. 1998. “Violin Plots: A Box Plot-Density Trace Synergism.” American Statistician 52 (2): 181–84. Hubbard, Raymond, Brian D Haig, and Rahul A Parsa. 2019. “The Limited Role of Formal Statistical Inference in Scientific Inference.” The American Statistician 73 (sup1): 91–98. Ihaka, Ross, and Robert Gentleman. 1996. “R: A Language for Data Analysis and Graphics.” Journal of Computational and Graphical Statistics 5 (3): 299–314. https://doi.org/10.1080/10618600.1996.10474713. Jane’s. 1978. Jane’s Encyclopedia of Aviation. London: Jane’s. Krainski, Elias, Virgilio Gómez-Rubio, Haakon Bakka, Amanda Lenzi, Daniela Castro-Camilo, Daniel Simpson, Finn Lindgren, and Håvard Rue. 2018. Advanced Spatial Modeling with Stochastic Partial Differential Equations Using r and INLA. Chapman; Hall/CRC. Langhammer, Penny F, Joseph W Bull, Jake E Bicknell, Joseph L Oakley, Mary H Brown, Michael W Bruford, Stuart HM Butchart, et al. 2024. “The Positive Impact of Conservation Action.” Science 384 (6694): 453–58. M., Diez D., Barr C. D., and Çetinkaya-Rundel M. 2019. OpenIntro Statistics. 4th edition. openintro.org/os. MacKay, R Jock, and R Wayne Oldford. 2000. “Scientific Method, Statistical Method and the Speed of Light.” Statistical Science, 254–78. https://www.jstor.org/stable/2676665. Marshall, Geoffrey, J. W. S. Blacklock, C. Cameron, N. B. Capon, R. Cruickshank andJ. H. Gaddum, F. R. G. Heaf, A. Bradford Hill, et al. 1948. “Streptomycin Treatment of Pulmonary Tuberculosis.” British Medical Journal, 769–82. McElreath, Richard. 2018. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. Chapman; Hall/CRC. Pickering, JF. 1985. “Giving in the Church of England: An Econometric Analysis.” Applied Economics 17 (4): 619–32. Potthoff, Richard F, and Samarendra N Roy. 1964. “A Generalized Multivariate Analysis of Variance Model Useful Especially for Growth Curve Problems.” Biometrika 51 (3-4): 313–26. Rahlf, Thomas. 2019. Data Visualisation with r: 111 Examples. Springer Nature. Rosling, Hans. 2018. Factfulness. London: Hodder &amp; Stoughton. Saviotti, P P, and A W Bowman. 1984. “Indicators of Output of Technology.” In Proceedings of the ICSSR/SSRC Workshop on Science and Technology in the 1980’s, edited by M Gibbons et al., 117–47. Brighton: Harvester Press. Strand, August Leroy. 1930. “Measuring the Toxicity of Insect Fumigants.” Industrial &amp; Engineering Chemistry Analytical Edition 2 (1): 4–8. Teasdale, Graham, and Bryan Jennett. 1974. “Assessment of Coma and Impaired Consciousness: A Practical Scale.” The Lancet 304 (7872): 81–84. Teasdale, Graham, Andrew Maas, Fiona Lecky, Geoffrey Manley, Nino Stocchetti, and Gordon Murray. 2014. “The Glasgow Coma Scale at 40 Years: Standing the Test of Time.” The Lancet Neurology 13 (8): 844–54. Unwin, Antony. 2018. Graphical Data Analysis with r. Chapman; Hall/CRC. Van den Boogaart, K Gerald, and Raimon Tolosana-Delgado. 2013. Analyzing Compositional Data with r. Vol. 122. Springer. Welch, Bernard L. 1947. “The Generalization of ‘STUDENT’s’problem When Several Different Population Varlances Are Involved.” Biometrika 34 (1-2): 28–35. Wickham, Hadley. 2009. Ggplot2: Elegant Graphics for Data Analysis. New York: Springer-Verlag. https://r-pkgs.org. Wickham, Hadley, and Jennifer Bryan. 2023. R Packages. \" O’Reilly Media, Inc.\". Wickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\". Wilke, Claus O. 2023. Ggridges: Ridgeline Plots in ’Ggplot2’. https://CRAN.R-project.org/package=ggridges. Young, Forrest W, Pedro M Valero-Mora, and Michael Friendly. 2011. Visual Statistics: Seeing Data with Dynamic Interactive Graphics. John Wiley &amp; Sons. Zeileis, Achim, Kurt Hornik, and Paul Murrell. 2009. “Escaping RGBland: Selecting Colors for Statistical Graphics.” Computational Statistics &amp; Data Analysis 53 (9): 3259–70. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
