[["index.html", "Statistical Modelling A conceptual, visual and practical introduction Preface: what’s the problem?", " Statistical Modelling A conceptual, visual and practical introduction Adrian Bowman The University of Glasgow adrian.bowman@glasgow.ac.uk 29 May, 2025 Preface: what’s the problem? The world abounds with data - but scientific and investigative work does not begin with data. It is true that data which is already available may give ideas and suggest hypotheses, but a serious project will start by thinking carefully about well defined objectives. In other words, scientific work begins with a clearly stated problem. Statistical modelling refers to the process by which we collect and use data to gain insight into the problem we have defined and to lead us towards a conclusion. In an interesting paper on scientific and statistical methods, MacKay and Oldford (2000) structured the process in the following broad steps, referred to by the acronym PPDAC. Some of the key questions which are likely to arise in each step have been highlighted. Problem What are the key questions we would like to address? What is the context in which these questions are framed? Plan How should our experiment be designed? What data should be collected and how much? Data How should the data be checked for validity and consistency? What methods of visual exploration should we use? Analysis How should an appropriate model be constructed? What does analysis of our model tell us about our problem? Conclusion How should we report to others on our conclusions? What limitations and caveats should we highlight? This broad view of statistical modelling sets the agenda for the book. There are many textbooks and resources available which discuss statistical modelling - why another one? This book aims takes a particular approach. The focus is on conceptual understanding of the main ideas behind statistical thinking and modelling. Some technical details are provided for those who are interested but engagement with this material is optional. There is a strong theme of visual communication of both data and the concepts behind statistical models. The approach is also practical with extensive reference to the widely used statistical computing environment R as a means of engaging with the concepts and implementing the methods discussed. In particular, there is extensive use of real datasets. The aim is to discuss interesting and scientifically important questions, where possible with the data used in published papers. As data become increasingly ‘open’, datsets are read from publicly available sources wherever possible. The target audience is those who need statistical methods to understand data. A good example is PhD students who are well motivated to analyse their experimental data. The scientific contexts of the examples come from a wide range of application areas, including the life sciences, the social sciences and topics of general interest. The aim is to provide examples which are both interesting and accessible, without the need for detailed technical knowledge of a particular area. Throughout the book there are frequent references to the widely used statistical computing system R. It is possible to read this book without using R but it is primarily intended that the reader will use this powerful system to engage with the examples and exercises and with the whole process of statistical modelling. A description of how to install R, and the popular system RStudio which helpfully manages some aspects of the environment, is available immediately after this preface. A gentle introduction to R is provided in Chapter 2. Acknowledgements Please note that this is a work in progress. Please forgive some rough edges in the presentation here and there. References MacKay, R Jock, and R Wayne Oldford. 2000. “Scientific Method, Statistical Method and the Speed of Light.” Statistical Science, 254–78. https://www.jstor.org/stable/2676665. "],["getting-started-with-r.html", "Getting started with R", " Getting started with R The R system can be downloaded for all major computing platforms at: https://cran.r-project.org RStudio has become very popular as a ‘front-end’ for R which helpfully manages some aspects of the environment. It is recommended that you use this. It is freely available at: https://posit.co/download/rstudio-desktop/ A note to Mac users If you are using R on an Apple computer which is running the macOS operating system, you need the XQuartz system to be installed. If this is not present, it can be obtained from https://www.xquartz.org. A note to Windows users Ther are many packages available to extend the range of tools providing in R. Installation of packages from the CRAN archive (https://cran.r-project.org) is very straightforward and quite a few packages will be used throughout this book. Occasionally the need arises to install a package from ‘source’ rather than from the convenient form provided by CRAN. For computers running the Microsoft Windows operating system, some additional tools, referred to as Rtools, may be needed to allow that. Details on how to install these tools are at https://cran.r-project.org/bin/windows/Rtools/. Installing the rpanel package One of the features of R is the availability of a large number of add-on packages which considerably extend the tools available. One of these, rpanel, is particularly associated with this book. It provides some of the datasets used as examples and it has some tools intended to help in communicating the statistical concepts to be covered. It would therefore be helpful to install this package at the very beginning. It would also be helpful to install the development version so that the contents are up-to-date. This can be done by typing the following instructions into the console window. R may respond with some messages as it tracks progress in obeying these instructions. install.packages(&quot;devtools&quot;) devtools::install_github(&quot;adrian-bowman/rpanel/rpanel&quot;) Learning R A gentle introduction to R is provided in Chapter 2. Following this, in the early chapters of the book there are quite a few comments in the text on the R code used to analyse the data. In later chapters, very detailed comments are sometimes inserted as comments in the displayed code, in order not to distract attention from the narrative of the data and its analysis. The material in this book is intended to introduce you to R and what it can do. However, if you are new to R then, as you become more comfortable with the system, it is worth knowing that there is a great deal of useful information at the R project web pages at https://www.r-project.org. In particular, this includes: links to an enormous list of contributed packages at https://cran.r-project.org/web/packages/index.html; manuals at https://cran.r-project.org/manuals.html; an introduction to the syntax of R at https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf. Some reassurance This book provides code which shows how each dataset can be analysed. It is quite likely that you will at times see this and wonder how on earth you could possibly have thought of this yourself! Please be reassured. Code is very often the product of a lot of trial and effort. The final code presented here was often preceded by considerable a number of false starts and substantial editing. So you will often find yourself going through the same process as you experiment with the examples, attempt the exercises or address your own datasets. We all do! "],["where-data.html", "1 Where do data come from?", " 1 Where do data come from? Data can take many forms. Sometimes we are dealing with simple numbers, such as blood pressure measurements or age. Some data may consist of categories, such as country of origin or presence/absence of a characteristic of interest. Modern data can be much more sophisticated, arising in the form of images, temperature curves, three-dimensional surfaces or networks. This chapter focuses on relatively simple data forms but uses these to discuss some important principles of how data are collected and what they are able to tell us. "],["examples-data.html", "1.1 Some examples of data", " 1.1 Some examples of data 1.1.1 CO2 and global warming One of the world’s most pressing issue is climate change as a result of global warming. The levels of CO2 in the atmosphere are clearly implicated in this as a result of the ‘greenhouse’ effect. Accurate monitoring of CO2 began in the late 1950’s through the pioneering work of Charles David Keeling at Mauna Loa Observatory in Hawaii and measurements have been made continuously since then, providing an invaluable record of change. This ‘time series’ is displayed at monthly resolution in the left hand panel below. This time series makes it clear that there has been a steady rise in atmospheric CO2 over the entire period of monitoring. The implications of this become startling when we compare the series to data which indicate the levels of CO2 which correspond to earlier historical times. This can be done by measuring the CO2 content of air trapped in ice cores. Careful analysis of the ice layers allow the identification of the time scale, which stretches back over hundreds of thousands of years. The CO2 measurements from ice cores has been superimposed on the plot of the Mauna Loa data in the right hand panel above. This makes it clear that the modern measurements of CO2 are ‘off the scale’ of the historical levels. In order to assuage any concerns about the comparability between the modern atmospheric and historical ice core CO2 measurements, the plots below focus on the period where these overlap. The left hand plot highlights the sudden dramatic rise in CO2 as the industrial revolution gathered pace. The right hand panel zooms in on the years when both types of measurement are available, indicating the very strong level of agreement between the two. We will look at corresponding temperature changes later, but these data already paint a stark picture of the nature and size of the challenge we face in addressing climate change. The Intergovernmental Panel on Climate Change synthesises our scientific understanding of the process and continues to report on the current situation, with urgent calls to action. This example highlights that, in some settings, there is a need for an appropriate control to enable informative comparisons. Controls can sometimes be difficult to identify. In this case, considerable effort has been undertaken to ensure the validity of comparisons with the control data. The example also highlights the relevance of our scientific understanding of the context in which the experiment takes place. The physics of greenhouse gasses is well understood and this allows us to strengthen the interpretation of what we see in the data. 1.1.2 The first tuberculosis trial One of the very earliest systematic evaluations of medical treatment, marking a significant step in what we now term ‘clinical trials’, was a study on the effects of Streptomycin on pulmonary tuberculosis by Marshall et al. (1948). The statistician Austin Bradford Hill, whose picture is below, introduced very important methodology in this study. In his review of the development of clinical trials, Bhatt (2010) writes “This trial was a model of meticulousness in design and implementation, with systematic enrolment criteria and data collection compared with the ad hoc nature of other contemporary research. A key advantage of Dr Hill’s randomization scheme over alternation procedure was “allocation concealment” at the time patients were enrolled in the trial. Another significant feature of the trial was the use of objective measures such as interpretation of x-rays by experts who were blinded to the patient’s treatment assignment.” Figure 1.1: Sir Austin Bradford Hill, Wellcome Collection, CC BY. The headline results reported in the scientific paper are shown in the table below. Radiological assessment Streptomycin group Control group Considerable improvement 28 4 Moderate or slight improvement 10 13 No material change 2 3 Moderate or slight deterioration 5 12 Considerable deterioration 6 6 Deaths 4 14 Total 55 52 The beneficial effects of the treatment are clear and we can have confidence in the conclusion because of the careful conduct of the trial. Key features of the design include the presence of a control group and the use of randomisation. 1.1.3 The birds and the bees: how to tell the sex of a herring gull Herring gulls are found across the coastal regions of North-Western Europe. When studying the behaviour of these birds, it is useful to be able to identify sex. With this species, this is not possible by visual examination of the obvious anatomical features as the appropriate organs are internal. It would therefore be very useful to be able to identify the sex of a bird by taking simple measurements of some kind, on the assumption that the sexes are lijkely to differn in size, as happens with many animal species. The correct identification of the sex of a herring gull has to be carried out by dissection. The most suitable source of data for this purpose is therefore birds which have been found dead or have been culled for other reasons. Measurements from a sample of 100 male and 100 female birds, kindly provided by Prof. Pat Monaghan from thte University of Glasgow, are available for investigation. Length measurements, based on the distance between two of the yellow landmarks in the picture above, could be useful in distinguishing between the sexes. as males and females tend to have different sizes in many species. There is an interactive application in R which can help in thinking this through. A gentle introduction to R is provided in Chapter 2 but, if you have followed the guidance at the start of the book on installing R and the add-on package rpanel, then type in the following instructions into the console window to launch the app. library(rpanel) rp.gulls() Consider which pairs of landmarks might provide a suitable length measurement. Suitable criteria are: reproducible, by yourself and others; valid, in what they aim to measure; informative, as they are likely to be different for male and females; well calibrated, as they target a feature of interest; practical, as measurements can be made reasonably easily. Click on your selected pairs of landmarks and some feedback will be given. If you are able to identify some suitable measurements, checkboxes and buttons will appear to allow you to see some plots of the data, separated out by sex. 1.1.4 Tracking the covid pandemic When the covid19 pandemic began there was immediate and urgent effort to track its progress. That happened in many ways, most immediately in the numbers of deaths and hospital admissions. In the UK, when tests became available, the number of positive results was also regularly reported. The symptoms data collected by the app from the ZOE Health Study provided another source of information. Although these sources provided useful information, the most reliable estimates of covid infection levels came from a large survey conducted by the Office of National Statistics (ONS), in partnership with the University of Oxford, the University of Manchester, the UK Health Security Agency (UKHSA) and the Wellcome Trust. Swab and blood samples were provided by thousands of people from across the UK who had been selected and random and who had agreed to participate by providing repeated samples over an extended time period. Statistical modeling was also undertaken to ensure that the results represented the population as a whole. The ONS provided extensive information about the data, including details of the methods and study design. Scientific publication of the methods appeared in a Lancet paper. This is an example where very considerable effort was made to ensure that what we observe is an accurate representation of what is happening. The plot above uses the survey data to track the pandemic in terms of the percentage of people in the UK who tested positive. The virulence of the infection changed over time so deaths and hospital admissions show rather different patterns. We will look at data on those later. An account of the covid-19 tracking project is given in an article published by the Royal Statistical Society in its Statistics Under Pressure series. The hugely valuable nature of the information provided by the ONS survey is discussed in a Conversation article. 1.1.5 More complex data objects The examples above mostly involve measurements of a single quantity of interest, such as CO\\(_2\\), the level of health improvement or the proportion of people with covid19. The herring gulls example might involve several different measurements on each bird. However, data can be much more complex. Some examples are: high-resolution images captured from a video camera monitoring wildlife movement; free-form text entered into the search box of a web browser; an extensive set of responses recorded from an individual in a survey, including later questions which are conditional on the responses to earlier questions; a network describing the interactions of one individual with others in a group; the life history of a hospital patient. As the complexity of data increases, so the models required for analysis may also need to increase in sophistication. Nonetheless, some basic principles and concepts still apply and it is the aim of this book to discuss these. While the focus is on relatively straightforward types of data, the ideas will provide helpful building blocks for more complex situations. References Bhatt, Arun. 2010. “Evolution of Clinical Research: A History Before and Beyond James Lind.” Perspectives in Clinical Research 1 (1): 6. Marshall, Geoffrey, J. W. S. Blacklock, C. Cameron, N. B. Capon, R. Cruickshank andJ. H. Gaddum, F. R. G. Heaf, A. Bradford Hill, et al. 1948. “Streptomycin Treatment of Pulmonary Tuberculosis.” British Medical Journal, 769–82. "],["some-broad-issues.html", "1.2 Some broad issues", " 1.2 Some broad issues It is worthwhile reflecting on some of the broad issues which have already arisen in our review of these examples of data. To structure our reflections it will be helpful to recall the PPDAC framework discussed in the Preface, displayed below in graphical form. In this section we will focus on the Problem, Plan and Data stages. 1.2.1 What’s the problem? It is very important to define the objectives of an experiment clearly. Failure to do this will make subsequent analysis and interpretation very difficult. Pilot studies are a perfectly valid preliminary step but a clear objective is still needed. ‘Fishing expeditions’ are of rather limited use. Our objectives will be informed by previous work so a review of the scientific literature is a wise start. Scientific knowledge of the subject domain will be very helpful not only in deciding what problem to tackle but also in informing later decisions in the planning process and in due course in interpreting the results of our analysis. There are some general aspects of this process which are worth highlighting. The first is the distinction between an observational study where we simply observe and record variables of interest. We do not intervene or influence the situation in any way other by recording what we see. In an observational study we can only identify association between variables. A simple of example is the association between social or economic characteristics and political voting patterns. In contrast, a designed experiment involves the specification of `treatments’ which are assigned by the experimenter. Rather than simply observe, we intervene. We are then interested in the way the treatments affect the outcome. If a designed experiment is conducted well it allows us to identify causal relationships between variables. A simple example is a clinical trial, such as the tuberculosis trial described in Section 1.1, where we compare the effects of different treatments on the recovery of patients suffering from a particular medical condition. A second general issue is whether our aim is to understand the processes at work in our scientific context or whether we simply want to predict an outcome or design a system which will classify future observations into different groups. The classification of herring gulls into male and female groups discussed in Section 1.1 is an example of the latter. This is a case where we may not be primarily interested in which variables are involved in our model, simply in how successful our model is in prediction or classification. This may change our attitude to building a model, when the time comes, but it is also helpful to be aware of the distinction between these two aims from the start of the planning process. 1.2.2 What’s the plan? Acquiring data which are appropriate, informative and unbiase requires careful thought. The ONS covid-19 survey was described in Section 1. Take a look at the methodology guide for this survey. It is very extensive, covering all the major issues which had to be considered. One of the exercises at the end of this chapter ask you to consider how you might design a simple experiment to investigate the operation of short term memory, known to psychologists as “working memory”. How is a list of items recalled from memory? This is an experiment which can be caried out in a classroom or small group setting. It might surprise you just how extensive the list of detailed arrangements needs to be. One of the important tasks is to identify which measurements should be taken. The list should include not only those which are mentioned in the definition of the problem being tackled but also those which we know or suspect from previous work or our scientific understanding are likely to influence the process we are studying. This will help us to consider an appropriate experimental design - a topic we will revisit in a later chapter. 1.2.3 Where do the data come from? The diagram at the start of this section includes two headings, “Real world” and “Model world”. The question our experiment aims to tackle is about what is going on “out there in the wild”, even when the “wild” refers to a laboratory setting. If we consider all the observations we might ever make then we can view this as the population we are studying. The process of collecting a particular dataset then delivers a sample. Our modelling process will use this sample to try to understand what is going on in the population. It is then crucially important to obtain a sample which properly represents the population and does not suffer from serious bias. If we make serious mistakes at this stage, it is unlikely that we will be able to retrieve the situation. A good mechanism for avoiding bias is to use randomisation. This applies in two ways. The first is to the process of identifying which items from the population are recruited into our sample, ideally ensuring that all potential items have an equal chance of appearing. The second applies to the allocation of any treatments to sampled items. The tuberculosis trial outlined in Section 1.1 is one of the first occasions where this was used. It is now a standard component of clinical trials worldwide. 1.2.4 Who do you trust? We live in a world where ‘fake news’ has become a commonly used term. Not everyone is careful in the way data are collected, analysed and interpreted. Sadly, data are sometimes used selectively to support a conclusion already adopted. It is very important that this is countered and that the analysis of data is conducted in an honest and professional manner. The UK Statistics Authority is the body which oversees the production of official statistics in the UK. Its code of practice is based on the principles of: trustworthiness, confidence in the people and organisations that produce statistics and data; quality, data and methods that produce assured statistics; value, statistics that support society’s needs for information. The full code is well worth reading. Although it is couched in terms of official statistics, providing information for the public and for government, the principles it describes are very important. A Declaration on Professional Ethics is also provided by the International Statistical Institute. In addition to the obligation on all who collect and analyse data to act with professionalism and integrity, specific ethical considerations arise in the planning and design of experiments, particularly those involving humans. Indeed, the protocol for any experiment involving humans must be approved by an appropriate ethical committee before it can be put into practice. A simple example is in a clinical trial to compare two treatments where the size of the sample must be considered very carefully. The sample must be large enough to enable a clear conclusion about the treatments to be reached, but if it is too large then some patients may end up being given a treatment which might clearly have been shown by a smaller trial to be inferior. That would not be ethical. We will often find ourselves using data from other sources as part of our investigations. Indeed, many different sources are used in this book. The issue of trustworthiness, raised in the UK Statistics Authority code of conduct, again arises. What sources can we trust as reliable? Some guidance comes from the source organisations stated aims and code of conduct. Reputation also matters. Open documentation is important, so that the details of the data collection process can be reviewed. Accountability matters too, so that there is a mechanism for query and complaint if the need arises. The RSS has provided a document, Sound or suspicious? Ten tips to be statistically savvy which offers advice on how to assess claims that are made. "],["what-could-possibly-go-wrong.html", "1.3 What could possibly go wrong?", " 1.3 What could possibly go wrong? It is very instructive to think about studies which went wrong. 1.3.1 US presidential elections, 1936 and 1948 The Literary Digest was a magazine which surveyed 10 million people, beginning with its own readers, who they planned to vote for in the 1936 US presidential election. A massive 2.4 million people responded, leading to the prediction of a clear win for the candidate Alf Landon. In fact, Franklin Delano Roosevelt had a landslide victory. What went wrong? For another occasion when things went wrong in the prediction of the outcome of a US presidential election, see the article in the Los Angeles Times about the 1948 election. "],["further-reading.html", "1.4 Further reading", " 1.4 Further reading The Royal Statistical Society has a very helpful Guide to UK official statistics on climate change. The CEDA Archive, maintained by the National Environmental Research Council (NERC) in the UK, contains a very large collection of environmental data from atmospheric and earth observation research. If data are to be collected through a survey, the nature and construction of the questions are very important, to avoid bias or leading the respondent. The Pew Research Centre, a high profile independent organisation, provides a helpful discussion of writing survey questions. Copernicus: European environmental data. https://surfobs.climate.copernicus.eu/dataaccess/index.php Diggle and Chetwynd (2011) Rosling (2018) The Tiger That Isn’t. Andrew Dilnot &amp; Michael Blastland. How to lie with Statistics. Darrell Huff. Damned Lies and Statistics. Joel BEst. More Damned Lies and Statistics. Joel Best. Innumeracy. John Allen Paulos. Reckoning with Risk. Gerd Gegerenzer. (Some people object to technical errors?) Dicing with Death. Stephen Senn. Risk. John Adams. Britain in Numbers. Simon Briscoe. Why Do Buses Come in Threes. Rob Eastaway. How Long is a Piece of String. Rob Eastaway. How to Take a Penalty. Rob Eastaway. References Diggle, Peter J, and Amanda Chetwynd. 2011. Statistics and Scientific Method: An Introduction for Students and Researchers. Oxford University Press. Rosling, Hans. 2018. Factfulness. London: Hodder &amp; Stoughton. "],["exercises.html", "1.5 Exercises", " 1.5 Exercises 1.5.1 An investigation of short-term memory Consider how you might design a simple experiment to investigate the operation of short term memory, known to psychologists as “working memory”? How is a list of items recalled from memory? This is an experiment which can be caried out in a classroom or small group setting. It might surprise you just how extensive the list of detailed arrangements needs to be. Once you have spent some time considering this, you may like tyo consult Bowman (1994) which describes some of the issues which arose in a classroom setting. 1.5.2 A survey of dental health Imagine you have been commissioned to conduct a survey of the dental health of five year old children in England. Write down some of the things on which you will need to make decisions and sketch out some possible answers. This should include how a suitable sample of children will be selected, what measurements will be made, how this will be done, and another other issues which you think are relevant. In fact, a survey of exactly this type is regularly conducted under the National Dental Epidemiology Programme (NDEP) for England. Once you have spent some time considering the issues, you can see the detail of what was done in the documents available here for the 2016-2017 survey. The protocol document describes the planning of the survey in considerable detail. The results of a further survey in 2022, using the same protocol, are also available. 1.5.3 Hearings aid and dementia A Lancet paper studied the association between hearing loss and dementia, in particular examining the role of hearing aids. The interpretation of the findings were: In people with hearing loss, hearing aid use is associated with a risk of dementia of a similar level to that of people without hearing loss. With the postulation that up to 8% of dementia cases could be prevented with proper hearing loss management, our findings highlight the urgent need to take measures to address hearing loss to improve cognitive decline. Are there other possible interpretations? You may wish to look at this article published by the British Geriatrics Society which discusses the issue. The article also provides a link to a further scientific paper for the technical detail. References Bowman, A. W. 1994. “Teaching by Design.” Teaching Statistics 16: 2–4. https://doi.org/10.1111/j.1467-9639.1994.tb00670.x. "],["R-introduction.html", "2 A gentle introduction to R", " 2 A gentle introduction to R This chapter assumes no previous experience of R. Those who do have some exposure may still find the material useful in rehearsing some of the key concepts, as these are a little different from some other computing environments. If you haven’t yet installed R, guidance for this is given in the section entitled Getting started with R at the very beginning of the book. "],["r-fuss.html", "2.1 R: what’s all the fuss about?", " 2.1 R: what’s all the fuss about? 2.1.1 History In the 1970’s, a research project led by John Chambers in Bell Labs produced an experimental statistical computing environment called S. This was designed around high level structures for handling data. It became very popular and began to be used in teaching, although its origins as a research project meant that the standard methods users expect needed to be added. A commercial version was created to support this and to provide the facilities that package users expect. In the 1990’s, Ross Ihaka and Rob Gentleman from the University of Auckland started to create a system which was ‘not unlike’ S in syntax but which was available as open source software. Over the years this became so popular that it has become the standard computational environment in Statistics and is widely used in many other areas. Why did this happen? There are several reason and these are outlined in the subsection below. The power of the system, its cost (nil), access to state-of-the art modelling tools and a very well organised mechanism for sharing add-on contributed packages are some of the reasons. R has become a research sharing mechanism, where methods described in scientific papers can quickly be made available for others to use. To give one example from genetics, the popular Bioconductor project provides tools for the analysis and comprehension of high-throughput genomic data. The articles below indicate how big an impact R has made. Vance, Ashlee (2009). “Data Analysts Are Mesmerized by the Power of Program R: [Business/Financial Desk]”. The New York Times. https://www.nytimes.com/2009/01/07/technology/business-computing/07program.html Vance, Ashlee (2009). “R You Ready for R?”. The New York Times. http://bits.blogs.nytimes.com/2009/01/08/r-you-ready-for-r/ Muenchen, Robert (2017). “The Popularity of Data Science Software”. http://r4stats.com/articles/popularity/ 2.1.2 Why should I be interested? There are many reasons why R may be very helpful for you. Access to very powerful statistical and visualisation tools. Easy access to an enormous library of user-contributed packages which provide state-of-the-art specialist tools for a wide variety of statistical topics and application areas. Relatively simple production of publication quality graphics. A full and very flexible programming environment in which to develop your own functions and tools. Good facilities for connecting with other computing languages (C++ etc.) and to other software systems (Matlab, OpenGL, etc.). Powerful tools to assist in producing reproducible research, including the creation of documents which integrate code and text into a self-generating report. It is freely available under a GNU licence. It operates in a consistent manner across all major computing platforms (Windows, Mac, Unix, Linux). It is supported by a very large community of users. The simplest form of operation is based on scripts containing written instructions. This approach has many advantages. It allows detailed, fine control of exactly what happens. It promotes logical thinking about the data analysis process as a whole. It allows iteration and refinement of the modelling process as scripts are improved by editing. It creates a record of the analysis which can be revisited at any time, without starting again from scratch. The script defines the analysis in a reproducible manner. 2.1.3 What’s the down side? For the reasons outlined above, R is not a ‘point-and-click’ system. However, this necessarily means that the syntax has to be learned by the user. All systems have an element of syntax but R requires more effort to learn than most. This is a consequence of its power and flexibility. So there is a learning curve to climb. If your statistical analysis requires only simple techniques then R may not be for you. "],["taste-of-r.html", "2.2 A taste of R", " 2.2 A taste of R Example: Mass and speed in quadrupedal rodents In an investigation of the relationship between mass (kg) and speed (km/hr) in mammals, Garland (1983) collected information from published articles on these two variables for a large number of different species. These measurements were recorded for a variety of four-footed rodents. (The common names of the species are taken from Corbet &amp; Hill, 1986.) Notice that the measurements are not all recorded to the same level of accuracy since the results have been collated from the work of a number of different scientists. Is it true that the bigger you are the faster you can run? We will use this simple example to explore how R operates. This will include the idea of a script where ‘R’ instructions can be typed and, when required, copied into the Console window for ‘R’ to execute. The majority of datasets in this book are read directly from external sources and Section 2.4 below shows how this can be done conveniently. Some datasets, such as the rodent one described above, are not available externally and these are provided within the add-on package rpanel. If you haven’t yet installed this package, follow the instructions in the Getting started with R page which immediately precedes Chapter 1. The first thing to do is to set up a new script. If you are using RStudio, this can be done from the menu: File &gt; New file &gt; R Script. (The detailed wording of the menu items may change with the particular computing system you are using.) Now type the following text into this script window. data(rodent, package = &#39;rpanel&#39;) It is strongly recommended that all R instructions are placed in a script, to build up a record of your analysis which can be edited and re-executed as needed. As discussed in Section 2.1, this provides a very convenient and reproducible workflow. Instructions can easily be copied and pasted from the script window into the console window. (This can be done efficiently by placing the cursor on the line of interest, or highlighting a block of code, and then using a keystroke combination such as Ctrl + Enter or command + Enter, although this may depend on your particular computer system.) Copy and paste the data instruction now. In R, data is a function designed to make a dataset available, in this case the rodent dataset from the rpanel package. It is helpful to think of R as a collection of functions. A function is something which accepts input of some kind and creates an output or action of some kind. The inputs are specified through the arguments of the function. Here the first argument is the name of the dataset. Functions often have many arguments. In this case, the second argument has been ‘named’ to identify that it refers to the ‘package’ argument, out of all the other arguments available. (If the package argument had been the second one in the definition of the function, we could have omitted naming it.) There is no automatic response from R but the rodent dataset is now available in the object rodent. This has rows for each rodent and columns for each variable. It is referred to as a dataframe. We can see what is inside any object by typing its name in the console. (Alternatively, the object can be inspected by clicking on its name in the RStudio Environment window.) rodent ## Mass Speed ## North American Porcupine 9.000 3.2 ## Woodchuck 4.000 16.0 ## Long-clawed ground squirrel 0.600 36.0 ## Long-tailed souslik 0.600 20.0 ## Eastern grey squirrel 0.550 27.0 ## European souslik 0.500 18.0 ## European red squirrel and Persian squirrel 0.400 20.0 ## Belding&#39;s ground squirrel 0.300 13.0 ## Rat 0.250 9.7 ## American red squirrel 0.220 15.0 ## Golden hamster 0.110 9.0 ## Eastern American chipmunk 0.100 17.0 ## Chisel-toothed kangaroo rat 0.056 21.0 ## Meadow vole 0.050 11.0 ## Least chipmunk 0.045 16.0 ## Merriman&#39;s kangaroo rat 0.035 32.0 ## Fawn hopping mouse 0.035 14.0 ## Pine mouse 0.030 6.8 ## Deer mouse 0.030 9.1 ## White footed mouse 0.025 11.0 ## Woodland jumping mouse 0.025 8.6 ## North American meadow jumping mouse 0.018 8.9 ## House mouse 0.016 13.0 To explore the data, plots always provide a good starting point. The plot function is very powerful and adaptable. Here we give it two variables and a scatterplot is produced. Notice that we refer to variables inside the dataset by &lt;dataframe name&gt;$&lt;variable name&gt;. What does this plot tell us? plot(rodent$Mass, rodent$Speed) A noticeable feature is that the data are ‘bunched up’ at one end and stretched out at the other, making any underlying pattern difficult to see. It would be worthwhile considering a change of scale by applying the log transformation to both axes. The log transformation is often effective in dealing with data which are skewed in this manner but there is also a more reasoned justification in this example. Mass is proportional to volume, which is the product of three dimensions - length, breadth and height (when adopting a rather drastic simplification of rodent shape!). On the other hand speed refers to the distance travelled in unit time and so it is one-dimensional. When the log transformation is applied to a product it creates quantities which add together on the log scale. With some further thought on the mathematical relationships involved, there is a strong aregument for the appropritaeness of the log transformation. (You are invited to consider this further in one of the Exercises at the end of the chapter.) The plot below shows this to be very effective at putting the data onto scales where the variation is more consistent. Notice that we do not need to create new variables containing the log transformed data. We can simply apply the log function inside our instruction. Note that this refers to logarithm to the base \\(e\\), the mathematical constant with value 2.7182…, and not to the base 10 (for which the function is log10). plot(log(rodent$Mass), log(rodent$Speed)) model1 &lt;- lm(log(Speed) ~ log(Mass), data = rodent) abline(model1, col = &#39;red&#39;) The instructions also fit a simple linear trend to the data. This is an example of a linear model which we will discuss in detail in Chapter ??. The lm function accepts a formula of the form &lt;response variable&gt; ~ &lt;explanatory variable&gt;. We can also conveniently specify the dataframe we are working with in the data argument. The syntax of this instruction also makes an assignment by placing the result of the lm function into a new object with a name of our own choosing. The backwards arrow &lt;- is how we make assignments in R. Objects can contain numbers, text, vectors of data, matrices, dataframes and many other things, some of them rather sophisticated. This is one of the more sophisticated cases as model1 now contains a fitted linear model. The fitted model has been passed to the abline which is designed to draw straight lines. What should it do? This illustrates that R is an object-oriented system. The abline function recognises that the object which has been passed to it is a linear model with a single ‘explanatory’ variable. There is a method for that kind of object which locates the slope and intercept of the fitted line and draws that on the plot, with the colour set by the col argument. The red line shows the effect of fitting a simple linear model. This is almost entirely flat and so provides no evidence of any relationship between Mass and Speed. Are you content that this model provides a good description of the data? Inspection of the plot raises concern about the observation in the lower right corner. This has the largest mass but the slowest speed. What is this rodent? It is the porcupine, whose spines provide an excellent defence mechanism as an alternative to running fast to escape a predator. We should not throw away data because they appear to be inconvenient. However, where there is a good biological motivation, as here, we can investigate the effect of fitting models without particular observations. The subset argument of the lm function allows us to do that easily. The porcupine is the first observation in the dataframe and the minus sign indicates that it should omitted. The green line shows the effect of omitting the procupine and this does suggest an underlying relationship between mass and speed for the other rodents. plot(log(rodent$Mass), log(rodent$Speed)) model2 &lt;- lm(log(Speed) ~ log(Mass), data = rodent, subset = -1) abline(model2, col = &#39;green&#39;) This simple example has proceeded slowly to enable detailed comments on the syntax and construction of the R code. The sequence of steps we have taken also illustate that building a suitable model involves careful thought, critical inspection, and the use of our understanding of the context. At this stage, we should save our script in a file. This defines our analysis. It allows us to reload the script when we need to revisit or refine the analysis. It is also easy to send the file to others to show precisely what we have done. This provides an excellent basis for reproducible research. It is worthwhile spending a few moments to add comments so that we can remind ourselves about the thought process we have gone through. Lines whose first non-space character is # will be ognored when the R code is executed. # Make the rodent data available from the rpanel package data(rodent, package = &#39;rpanel&#39;) # Plot Speed against Mass plot(rodent$Mass, rodent$Speed) # Use the log transformation to provide more effective scales plot(log(rodent$Mass), log(rodent$Speed)) # Fit a simple straight line model and display this on the plot. model1 &lt;- lm(log(Speed) ~ log(Mass), data = rodent) abline(model1, col = &#39;red&#39;) # Fit the model without the porcupine which is biologically different plot(log(rodent$Mass), log(rodent$Speed)) model2 &lt;- lm(log(Speed) ~ log(Mass), data = rodent, subset = -1) abline(model2, col = &#39;green&#39;) This can be taken further with R markdown which is a very powerful system for combining text and R code in a single document. When this document is compiled the R code runs to perform the analysis and generate graphs and other forms of output. "],["a-summary-of-key-concepts-in-r.html", "2.3 A summary of key concepts in R", " 2.3 A summary of key concepts in R It is useful to take stock of some key ideas. Everything in R is an object. To expand on this, examples of objects include: single values; the type could be integer, numeric, character, logical or date; a set of values, all of the same type, referred to as a vector; a vector can be created directly by using the c function to collect values together, for example as c(1, 5, 3, 8, 6); a matrix of values, all of the same type, arranged in row and column format; an array of values, all of the same type, arranged with multiple dimensions; a dataset, called in R a dataframe; a list which brings together different types of object into a single object; a dataframe is an example of a list object; the individual components can be accessed by using the name of the list object and the name of the component, separated by a $ sign; a function. R can be thought of as a large collection of functions which operate on objects and produce some kind of output, perhaps a plot or a new object. The standard form of a statement in R is result &lt;- &lt;functionname&gt;(&lt;arguments&gt;). (Here the &lt;...&gt; is a placeholder for an actual function name or a set of arguments.) The round brackets are an essential part of the syntax as they delimit the input arguments. Here are some other things which it is useful to know when writing R instructions. Help! The number of functions available even in base R can be rather overwhelming but you can achieve a lot by becoming familiar with a relatively small set. If you know the name of the function you would like to use, say lm, then the instruction ?lm will show a help file with all the details of how the function can be used. Examples are also usually given. You may find that the help information is sometimes complex. This is the downside of the power and flexibility of the functions available, as the help files have to document all the different uses to which they may be put, and all the different arguments available to control the details of the operation. More help! A full-blooded introduction to R as an environment and a programming language is available from the RStudio menu Help &gt; R Help. This brings up in the Help pane links to a large collection of resources, including An Introduction to R. Cheatsheets. A large collection of helpful summaries of different topics are available from the RStudio menu: Help &gt; Cheatsheets &gt; Browse Cheatsheets .... Many refer to the functions available in particular R packages. If you scroll down to the Contributed section you will find one on Base R. Specifying function arguments. Functions often have multiple arguments and each of these has a name given in the definition of the function. When the function is called, the arguments can be named. When they are not named, they can be identified by their position in the list of arguments. For example, the code for the rodent example used the lm function in the form: lm(log(Speed) ~ log(Mass), data = rodent, subset = -1) The help file for lm shows that the first three arguments are formula, data and subset. The call above names the data and subset arguments but the formula has been left unnamed. It is the first argument, so its meaning is clear. We could have omitted the names of the other two arguments because, as it happens, these are the second and third arguments in the function definition. However, the statement is much more readable if we give the names. If we wish to set one of the arguments further down the list then we need to use the name, otherwise there is ambiguity over which argument we intend to use. Square bracket notation. It is very helpful to be able to specify a subset of an object. Square brackets provide a convenient mechanism to do this. For example, the speed the porcupine can be expressed as rodent$Speed[1]. The speeds of all the other rodents are available as rodent$Speed[-1] where a negative sign indicates that this case should be removed. For dataframes, square brackets need row and column information so, for example, rodent[1, ] gives the data for the porcupine while rodent[-1, ] provides a dataframe with the porcupine removed. Logical expressions. These are very useful, particularly in specifying a subset of a vector or dataframe. For example, we wished to exclude the porcupine when we fitted our second linear model to the rodent data. It was convenient to use the subset argument of the lm function but we could instead have defined an indicator vector as: ind &lt;- (rownames(rodent) != &#39;North American Porcupine&#39;) (The brackets around the logical expression make it clear that the order of operations is to perform the test first and then assign the result to ind.) A test for equality is performed by == while != corresponds to ‘not equal’. If you inspect the contents of ind you will see a vector of logical values, one for each rodent, which are TRUE in every case except for the porcupine, where it is FALSE. We could now fit the linear model by using our logical variable in conjunction with the square bracket notation as lm(log(Speed)[ind] ~ log(Mass)[ind], data = rodent) or as lm(log(Speed) ~ log(Mass), data = rodent[ind, ]). A further possibility in this case would be to use the subset argument as lm(log(Speed) ~ log(Mass), data = rodent, subset = ind). One advantage of this over the use of the indicator -1 is that the code is more readable - in the definition of ind we can see what’s happening. Finally, although we haven’t needed it in this example, logical variables and expressions can be combined using the &amp; (‘and’), &amp;&amp; (‘conditional and’, where the second expression is examined only if the first is TRUE), | (‘or’) and ! (‘not’) operators. Again, for vectors this is done element by element. Packages. The ability to access the additional facilities provided by the very large number of packages available for R is one of its strengths. The rpanel package has already appeared but many more will be used throughout this book. Remember that when the library function is used to load a package this assumes that the package has already been installed. You may need to do that first by calling the install.packages function. The intention of this chapter is simply to provide a head start in learning R in order that it may be used to engage seriously with the business of statistical modelling. The concepts discussed here should allow us to do many useful things. Many other facilities and ways of doing things will appear along the way. Learning by doing is a good way of proceeding. For those who would prefer a more systematic and thorough coverage of R, the resources mentioned under Help! above and the further resources listed near the end of this chapter will prove very helpful. "],["reading-data.html", "2.4 Reading data into R", " 2.4 Reading data into R 2.4.1 Accessing the data used in this book There are many examples of datasets throughout this book and, wherever possible, these are read from their original source. This is a good principle as it avoids the possibility of mistakes by copying and redistributing the data but, more importantly, it encourages the data to be seen in their original context. Also, some of the datasets are regularly updated so accessing these from source ensures that the most recent data are used. Many of the datasets are therefore accessed through web addresses. Some of these are rather long and some need to be downloaded locally before they can be read into R. The rp.datalink function is therefore provided in the rpanel package to help. library(rpanel) rp.datalink() ## name type ## 1 Ben_Nevis .csv ## 2 centenarians .txt ## 3 children_services .zip .... A list of the datasets available is provided by calling the function with no arguments. The output above shows the first few lines of the result. The remote file addresses are truncated but these can be viewed in full by assigning the output of the rp.datalink function to an object and inspecting it. When an individual dataset is needed, it can be accessed by calling the function with the name of the dataset as an argument. path &lt;- rp.datalink(&#39;scottish_referendum&#39;) ref &lt;- read.table(path, header = TRUE) The object which the rp.datalink function creates is simply a piece of text which gives the location of the dataset. As some R functions cannot read directly from remote locations, in this case the dataset is downloaded to a local temporary file and it is this file name which is returned. It is often convenient to be able to work with datasets offline so an option to download datasets to a convenient local directory is also provided. The second ‘action’ argument of the function allows a local directory to be specified and files to be downloaded there. For the download action, the name 'all' can be given, in which case all the datasets will be downloaded. rp.datalink(&#39;~/Desktop/temp&#39;, &#39;set local directory&#39;) rp.datalink(&#39;scottish_referendum&#39;, &#39;download&#39;) 2.4.2 Functions for reading data One of the first things to learn is how to read your own data into R. Some useful functions from base R are listed below. read.table: this takes a file with plain text information and reads it in row/column format into a dataframe. If the first line of the file contains the names of the variables in the different columns, then use the additional argument header = TRUE. There are many other arguments to control details. This function can handle a filename which is a remote web address. read.csv: this is very convenient when data have been exported from Excel or similar systems in ‘comma separated’ format. Fine control of separation characters and other features is available, if required. load: this is a very convenient function for placing into your R workspace objects of any type which have previously been saved, by yourself or others, using the save function. This offers considerable flexibility because it can handle lists and other objects which are not in standard row/column format. Similar functions are available for exporting data and information from R. The analogues of some of the functions above are write.table and write.csv. Several packages which provide additional facilities for reading data are available. For example, the readr package provides read_table, read_csv and several other functions, while the readxl package provides the read_excel function to read Excel files directly. A helpful summary is provided in a cheatsheet. Sometimes data are made available in the form of a ‘zip’ file, which may contain one or more individual files. The code below gives an example of reading data on the UK parliamentary boundaries of 2024, available on the ONS web site. The unzip function unpacks the files into a specified folder from which the individual files can be read. (This particular dataset will be used in a later case study.) path &lt;- rp.datalink(&quot;UK_parliamentary_boundaries_2024&quot;) unzip(path, exdir = &#39;~/Desktop/temp&#39;) An example of reading an Excel file is given in Section 2.4.3 below. The CRAN website provides extensive details on import/export issues, including facilities for reading data from other software systems. The rio package also provides a helpful interface to various packages for reading different kinds of data files. 2.4.3 An example: reading an Excel spreadsheet The ONS holds information on marriage, cohabitation and cohort analysis. The code below uses the rp.datalink function to provide the path to the Excel file holding historical data and then the read_excel function from the readxl package to read the data. The further arguments of read_excel locate the particular sheet of interest, skip some lines with header information, stop at the appropriate row and rename the columns as we wish. The read_excel functions actually delivers the data in a form known as a tibble. This is a slightly enhanced form of dataframe whose useful properties we will encounter later. library(readxl) path &lt;- rp.datalink(&#39;married_men&#39;) married_men &lt;- read_excel(path, sheet = &quot;Table 2&quot;, skip = 9, n_max = 96, col_names = c(&quot;Year&quot;, as.character(17:50))) This dataset essentially describes a surface over year and age so it is interesting to plot it in this form. The rgl package provides powerful facilities for three-dimensional plotting by giving access to the OpenGL system. In the code below, the grid points on the x and y axes and the matrix of values of surface heights are passed as the principal arguments to the persp3d function. The heights are given by omitting the first column of the dataframe d and scaling down by 1000 to create proportions. library(rgl) persp3d(married_men$Year, 17:50, as.matrix(married_men[ , -1]) / 1000, col = &quot;lightgreen&quot;, xlab = &quot;Year&quot;, ylab = &quot;Age&quot;, zlab = &quot;Proportion married&quot;) The dramatic post-war change in marriage patterns is very clear. With the web version of this book, you should be able to rotate the plot using the mouse controls. "],["a-further-example-the-scottish-independence-referendum.html", "2.5 A further example: the Scottish independence referendum", " 2.5 A further example: the Scottish independence referendum Example: The Scottish Referendum on Independence The results of the Scottish referendum on independence were of enormous interest to the UK. A research paper from the House of Commons Library reported the voting patterns separately by Council area, together with a variety of social and demographic characteristics. The data are available in the Scottish_referendum dataframe in the rpanel package. The information in this file is: Council the Scottish council name Voted.no the percentage of people who voted no Turnout the percentage of people who voted Population the number of people eligible to vote Unemployment.rate the percentage unemployed Scottish.identity.only the percentage who identify themselves as Scottish only Aged.16 the percentage who are 16 years of age Aged.over.50 the percentage who are over 50 years of age Aged.over.65 the percentage who are over 65 years of age We can use this information to explore the demographic and geographic patterns of voting. The first thing we need to do is to read the data. The rp.datalink function from the rpanel package provides a convenient way of locating the dataset, as described in Section 2.4 above. Here we also use the head function to inspect the first few rows. head(Scottish_referendum) ## Council Yes No Rejected Total Electorate ## 1 Aberdeen City 59390 84094 180 143664 175751 ## 2 Aberdeenshire 71337 108606 102 180045 206490 ## 3 Angus 35044 45192 66 80302 93656 ## 4 Argyll and Bute 26324 37143 49 63516 72014 ## 5 Clackmannanshire 16350 19036 24 35410 39974 ## 6 Dumfries and Galloway 36614 70039 122 106775 122052 ## Scottish_only_identity Born_in_Scotland Unemployed Age_65_or_over ## 1 54.7 75.0 7.6 14.8 ## 2 61.3 80.5 5.6 17.2 ## 3 66.8 85.9 9.4 21.5 ## 4 57.4 76.1 10.3 23.4 ## 5 67.0 86.4 14.9 17.6 ## 6 59.6 77.1 11.4 23.3 R can act as a simple calculator. For example, consider the data recording the proportion of people voting ‘no’ and the proportion of people who voted. We can identify the number of people who voted ‘no’ in each region, and then confirm the percentage of those who voted ‘no’ across the country, by Scottish_referendum$Vote &lt;- Scottish_referendum$Yes + Scottish_referendum$No + Scottish_referendum$Rejected Scottish_referendum$Turnout &lt;- 100 * Scottish_referendum$Vote / Scottish_referendum$Electorate Scottish_referendum$Yes_percent &lt;- 100 * Scottish_referendum$Yes / Scottish_referendum$Vote 100 * sum(Scottish_referendum$No) / sum(Scottish_referendum$Vote) ## [1] 55.25078 The first instruction multiplies each population by the corresponding turnout proportion to find the number of people who voted in each region. Notice that the operation is performed for each element of ref$Population and the corresponding element of ref$Turnout. The resulting vector of numbers is stored in a new component of the ref dataframe, with the variable name Vote. The second instruction calculates a weighted average of the percentages voting ‘no’, using the number of people voting in each region as the weights. This creates the overall percentage who voted ‘no’. It would be interesting to produce a scatterplot to explore the relationship between the percentage of people who voted ‘no’ and the unemployment rate of the Council regions. The plot function can do this for us. There seems to be quite a strong relationship here. plot(Scottish_referendum$Unemployed, Scottish_referendum$Yes_percent) It might be more helpful to plot the Council names instead of simple points. Here we add the argument type = \"n\" to the plot function to stop any points being plotted and then use the text function to plot the Council names instead. plot(Scottish_referendum$Unemployed, Scottish_referendum$Yes_percent, type = &quot;n&quot;) text(Scottish_referendum$Unemployed, Scottish_referendum$Yes_percent, Scottish_referendum$Council) "],["further-reading-1.html", "2.6 Further reading", " 2.6 Further reading Data Science for Psychologists. https://bookdown.org/hneth/ds4psy/ The first appearance of R in the scientific literature: Ihaka and Gentleman (1996). References Ihaka, Ross, and Robert Gentleman. 1996. “R: A Language for Data Analysis and Graphics.” Journal of Computational and Graphical Statistics 5 (3): 299–314. https://doi.org/10.1080/10618600.1996.10474713. "],["exercises-1.html", "2.7 Exercises", " 2.7 Exercises These exercises are intended to reinforce the introductory material on R functions and expressions. They require relatively simple R code, although examination of the help files for relevant functions may be required in order to identify useful arguments and their options. 2.7.1 Scottish Referendum data Here are some further things to explore. It would be interesting to relate the turnout to the unemployment rate. This can be done simply by replacing Voted.no with Turnout in the code. Try this. What do you see? Remember that you can use plot and text together to see the Council names. There are several other variables whose relationships with turnout and with the ‘no’ vote it would be interesting to explore. Two of these are Scottish.identity.only and Aged.over.50. Spend some time creating plots with these two variables on the horizontal axis and with Voted.no and Turnout on the vertical axis. Any comments? 2.7.2 Aircraft designs In a study of how aircraft technology develop during the 20th century, Saviotti and Bowman (1984) collated some simple physical characteristics of each design. These included: total engine power (kW); wing span (m); length (m); maximum take-off weight (kg); maximum speed (km/h); range (km). The year (`Yr) of the first appearance of each design is also provided. Produce some plots to investigate the patterns of change in these individual characteristics over time. The data are available in the aircraft dataframe in the sm package. (Remember that if this package is not already installed then the install.packages instruction needs to be given to do that.) 2.7.3 Centenarians What are the population age trends in Scotland? Data from the National Records of Scotland are available in the dataset centenarians, which can be access in the usual way through the rp.datalink function. The variables are: Region the administrative region of Scotland Males the total number of males of all ages in 2014 Males90 the number of males over 90 years of age in 2014 Males100 the number of males over 100 years of age in 2014 Females the total number of females of all ages in 2014 Females90 the number of females over 90 years of age in 2014 Females100 the number of females over 100 years of age in 2014 Males90y2004 the number of males over 90 years of age in 2004 Females90y2004 the number of females over 90 years of age in 2004 Explore the data in any way you see fit, in order to highlight recent trends. Here are some questions which might guide you. Is there a consistent pattern in the numbers of males and females over 90 years of age in 2014? Is the ratio of males to females similar across the regions? Repeat this for those over 100 years of age. Repeat this for those over 90 years of age using the data from 2004. 2.7.4 Justification of the log transformation for the rodent data This exercise is for those who are comfortable in handling mathematical formulae. In the discussion of the rodent dataset, a log transformation was used to very good effect. If you are comfortable in handling mathematical formulae, consider a simple model which relates speed (S) to length (L), breadth (B) and height (H) through the relationship which assumes speed to be proportional to a power of volume: \\[ S = \\alpha (L B H)^\\beta \\] If \\(\\beta = 1\\) then we have simple proportionality but using the parameter \\(\\beta\\) allows other relationships to be described. What happens when you apply a log transformation? Consider in particular what happens in the realistic scenario when length, breadth and height all increase by the same proportion, becoming \\(\\gamma L\\), \\(\\gamma B\\) and \\(\\gamma H\\), for example when \\(\\gamma = 1.1\\) for a 10% increase. What happens to S when the log transformation is used? References Saviotti, P P, and A W Bowman. 1984. “Indicators of Output of Technology.” In Proceedings of the ICSSR/SSRC Workshop on Science and Technology in the 1980’s, edited by M Gibbons et al., 117–47. Brighton: Harvester Press. "],["references.html", "References", " References Bhatt, Arun. 2010. “Evolution of Clinical Research: A History Before and Beyond James Lind.” Perspectives in Clinical Research 1 (1): 6. Bowman, A. W. 1994. “Teaching by Design.” Teaching Statistics 16: 2–4. https://doi.org/10.1111/j.1467-9639.1994.tb00670.x. Diggle, Peter J, and Amanda Chetwynd. 2011. Statistics and Scientific Method: An Introduction for Students and Researchers. Oxford University Press. Ihaka, Ross, and Robert Gentleman. 1996. “R: A Language for Data Analysis and Graphics.” Journal of Computational and Graphical Statistics 5 (3): 299–314. https://doi.org/10.1080/10618600.1996.10474713. MacKay, R Jock, and R Wayne Oldford. 2000. “Scientific Method, Statistical Method and the Speed of Light.” Statistical Science, 254–78. https://www.jstor.org/stable/2676665. Marshall, Geoffrey, J. W. S. Blacklock, C. Cameron, N. B. Capon, R. Cruickshank andJ. H. Gaddum, F. R. G. Heaf, A. Bradford Hill, et al. 1948. “Streptomycin Treatment of Pulmonary Tuberculosis.” British Medical Journal, 769–82. Rosling, Hans. 2018. Factfulness. London: Hodder &amp; Stoughton. Saviotti, P P, and A W Bowman. 1984. “Indicators of Output of Technology.” In Proceedings of the ICSSR/SSRC Workshop on Science and Technology in the 1980’s, edited by M Gibbons et al., 117–47. Brighton: Harvester Press. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
